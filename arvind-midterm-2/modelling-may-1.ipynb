{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab75135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Dict, Tuple, Any, Type, Union\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4981d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import election_project as ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "250af039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5342bded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9270,  9271,  9272, ..., 12357, 12358, 12359], shape=(3090,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.where(df['year'] == 2020)[0]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e9d158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>gisjoin</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>median_household_income</th>\n",
       "      <th>per_capita_income</th>\n",
       "      <th>per_capita_area</th>\n",
       "      <th>per_capita_households_income_under_10k</th>\n",
       "      <th>per_capita_households_income_10k_15k</th>\n",
       "      <th>per_capita_households_income_15k_25k</th>\n",
       "      <th>...</th>\n",
       "      <th>P(labor_force_employed|C)</th>\n",
       "      <th>P(labor_force_unemployed|C)</th>\n",
       "      <th>P(not_in_labor_force|C)</th>\n",
       "      <th>P(persons_hispanic|C)</th>\n",
       "      <th>P(persons_below_poverty|C)</th>\n",
       "      <th>P(18plus|C)</th>\n",
       "      <th>P(democrat|C)</th>\n",
       "      <th>P(other|C)</th>\n",
       "      <th>P(republican|C)</th>\n",
       "      <th>P(non_voter|C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>2020</td>\n",
       "      <td>G0100010</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>68315</td>\n",
       "      <td>35332</td>\n",
       "      <td>0.026639</td>\n",
       "      <td>0.016848</td>\n",
       "      <td>0.011164</td>\n",
       "      <td>0.040180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449465</td>\n",
       "      <td>0.012798</td>\n",
       "      <td>0.324024</td>\n",
       "      <td>0.031722</td>\n",
       "      <td>0.112830</td>\n",
       "      <td>0.765729</td>\n",
       "      <td>0.127687</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.337605</td>\n",
       "      <td>0.293137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>2020</td>\n",
       "      <td>G0100030</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>71039</td>\n",
       "      <td>38907</td>\n",
       "      <td>0.022494</td>\n",
       "      <td>0.017351</td>\n",
       "      <td>0.016370</td>\n",
       "      <td>0.027281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459489</td>\n",
       "      <td>0.016387</td>\n",
       "      <td>0.333318</td>\n",
       "      <td>0.048025</td>\n",
       "      <td>0.100441</td>\n",
       "      <td>0.788381</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.006670</td>\n",
       "      <td>0.357913</td>\n",
       "      <td>0.318503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9272</th>\n",
       "      <td>2020</td>\n",
       "      <td>G0100050</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>39712</td>\n",
       "      <td>23378</td>\n",
       "      <td>0.094171</td>\n",
       "      <td>0.036861</td>\n",
       "      <td>0.029787</td>\n",
       "      <td>0.059010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339752</td>\n",
       "      <td>0.020742</td>\n",
       "      <td>0.459501</td>\n",
       "      <td>0.048318</td>\n",
       "      <td>0.212244</td>\n",
       "      <td>0.792941</td>\n",
       "      <td>0.193592</td>\n",
       "      <td>0.003216</td>\n",
       "      <td>0.225992</td>\n",
       "      <td>0.370141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>2020</td>\n",
       "      <td>G0100070</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>50669</td>\n",
       "      <td>24966</td>\n",
       "      <td>0.072885</td>\n",
       "      <td>0.024673</td>\n",
       "      <td>0.029662</td>\n",
       "      <td>0.035459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370410</td>\n",
       "      <td>0.035324</td>\n",
       "      <td>0.405285</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.193115</td>\n",
       "      <td>0.788099</td>\n",
       "      <td>0.089254</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.338187</td>\n",
       "      <td>0.356883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9274</th>\n",
       "      <td>2020</td>\n",
       "      <td>G0100090</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>57440</td>\n",
       "      <td>29053</td>\n",
       "      <td>0.028524</td>\n",
       "      <td>0.024460</td>\n",
       "      <td>0.018992</td>\n",
       "      <td>0.030943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420739</td>\n",
       "      <td>0.026711</td>\n",
       "      <td>0.352117</td>\n",
       "      <td>0.096840</td>\n",
       "      <td>0.140105</td>\n",
       "      <td>0.770401</td>\n",
       "      <td>0.044687</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.418285</td>\n",
       "      <td>0.303418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12355</th>\n",
       "      <td>2020</td>\n",
       "      <td>G5600370</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Sweetwater County</td>\n",
       "      <td>79375</td>\n",
       "      <td>40268</td>\n",
       "      <td>0.645738</td>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.011003</td>\n",
       "      <td>0.025904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497873</td>\n",
       "      <td>0.030158</td>\n",
       "      <td>0.245063</td>\n",
       "      <td>0.162076</td>\n",
       "      <td>0.114855</td>\n",
       "      <td>0.745146</td>\n",
       "      <td>0.090853</td>\n",
       "      <td>0.015352</td>\n",
       "      <td>0.290620</td>\n",
       "      <td>0.348321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12356</th>\n",
       "      <td>2020</td>\n",
       "      <td>G5600390</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Teton County</td>\n",
       "      <td>108279</td>\n",
       "      <td>76296</td>\n",
       "      <td>0.467860</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>0.008652</td>\n",
       "      <td>0.024372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649190</td>\n",
       "      <td>0.013878</td>\n",
       "      <td>0.180245</td>\n",
       "      <td>0.151975</td>\n",
       "      <td>0.068791</td>\n",
       "      <td>0.823739</td>\n",
       "      <td>0.421828</td>\n",
       "      <td>0.025615</td>\n",
       "      <td>0.185942</td>\n",
       "      <td>0.190354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12357</th>\n",
       "      <td>2020</td>\n",
       "      <td>G5600410</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Uinta County</td>\n",
       "      <td>78164</td>\n",
       "      <td>32955</td>\n",
       "      <td>0.263227</td>\n",
       "      <td>0.007495</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>0.019420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465395</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.271294</td>\n",
       "      <td>0.098559</td>\n",
       "      <td>0.064003</td>\n",
       "      <td>0.720724</td>\n",
       "      <td>0.077436</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.364840</td>\n",
       "      <td>0.260343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12358</th>\n",
       "      <td>2020</td>\n",
       "      <td>G5600430</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Washakie County</td>\n",
       "      <td>61875</td>\n",
       "      <td>32979</td>\n",
       "      <td>0.751918</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.014239</td>\n",
       "      <td>0.047249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512751</td>\n",
       "      <td>0.009320</td>\n",
       "      <td>0.293204</td>\n",
       "      <td>0.142783</td>\n",
       "      <td>0.065113</td>\n",
       "      <td>0.781230</td>\n",
       "      <td>0.084272</td>\n",
       "      <td>0.017605</td>\n",
       "      <td>0.420065</td>\n",
       "      <td>0.259288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12359</th>\n",
       "      <td>2020</td>\n",
       "      <td>G5600450</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston County</td>\n",
       "      <td>71800</td>\n",
       "      <td>37067</td>\n",
       "      <td>0.904808</td>\n",
       "      <td>0.030131</td>\n",
       "      <td>0.021834</td>\n",
       "      <td>0.020524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441776</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>0.370597</td>\n",
       "      <td>0.035371</td>\n",
       "      <td>0.120815</td>\n",
       "      <td>0.795779</td>\n",
       "      <td>0.052402</td>\n",
       "      <td>0.013537</td>\n",
       "      <td>0.452256</td>\n",
       "      <td>0.277584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3090 rows Ã— 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year   gisjoin    state             county  median_household_income  \\\n",
       "9270   2020  G0100010  Alabama     Autauga County                    68315   \n",
       "9271   2020  G0100030  Alabama     Baldwin County                    71039   \n",
       "9272   2020  G0100050  Alabama     Barbour County                    39712   \n",
       "9273   2020  G0100070  Alabama        Bibb County                    50669   \n",
       "9274   2020  G0100090  Alabama      Blount County                    57440   \n",
       "...     ...       ...      ...                ...                      ...   \n",
       "12355  2020  G5600370  Wyoming  Sweetwater County                    79375   \n",
       "12356  2020  G5600390  Wyoming       Teton County                   108279   \n",
       "12357  2020  G5600410  Wyoming       Uinta County                    78164   \n",
       "12358  2020  G5600430  Wyoming    Washakie County                    61875   \n",
       "12359  2020  G5600450  Wyoming      Weston County                    71800   \n",
       "\n",
       "       per_capita_income  per_capita_area  \\\n",
       "9270               35332         0.026639   \n",
       "9271               38907         0.022494   \n",
       "9272               23378         0.094171   \n",
       "9273               24966         0.072885   \n",
       "9274               29053         0.028524   \n",
       "...                  ...              ...   \n",
       "12355              40268         0.645738   \n",
       "12356              76296         0.467860   \n",
       "12357              32955         0.263227   \n",
       "12358              32979         0.751918   \n",
       "12359              37067         0.904808   \n",
       "\n",
       "       per_capita_households_income_under_10k  \\\n",
       "9270                                 0.016848   \n",
       "9271                                 0.017351   \n",
       "9272                                 0.036861   \n",
       "9273                                 0.024673   \n",
       "9274                                 0.024460   \n",
       "...                                       ...   \n",
       "12355                                0.020248   \n",
       "12356                                0.007753   \n",
       "12357                                0.007495   \n",
       "12358                                0.016570   \n",
       "12359                                0.030131   \n",
       "\n",
       "       per_capita_households_income_10k_15k  \\\n",
       "9270                               0.011164   \n",
       "9271                               0.016370   \n",
       "9272                               0.029787   \n",
       "9273                               0.029662   \n",
       "9274                               0.018992   \n",
       "...                                     ...   \n",
       "12355                              0.011003   \n",
       "12356                              0.008652   \n",
       "12357                              0.012216   \n",
       "12358                              0.014239   \n",
       "12359                              0.021834   \n",
       "\n",
       "       per_capita_households_income_15k_25k  ...  P(labor_force_employed|C)  \\\n",
       "9270                               0.040180  ...                   0.449465   \n",
       "9271                               0.027281  ...                   0.459489   \n",
       "9272                               0.059010  ...                   0.339752   \n",
       "9273                               0.035459  ...                   0.370410   \n",
       "9274                               0.030943  ...                   0.420739   \n",
       "...                                     ...  ...                        ...   \n",
       "12355                              0.025904  ...                   0.497873   \n",
       "12356                              0.024372  ...                   0.649190   \n",
       "12357                              0.019420  ...                   0.465395   \n",
       "12358                              0.047249  ...                   0.512751   \n",
       "12359                              0.020524  ...                   0.441776   \n",
       "\n",
       "       P(labor_force_unemployed|C)  P(not_in_labor_force|C)  \\\n",
       "9270                      0.012798                 0.324024   \n",
       "9271                      0.016387                 0.333318   \n",
       "9272                      0.020742                 0.459501   \n",
       "9273                      0.035324                 0.405285   \n",
       "9274                      0.026711                 0.352117   \n",
       "...                            ...                      ...   \n",
       "12355                     0.030158                 0.245063   \n",
       "12356                     0.013878                 0.180245   \n",
       "12357                     0.016500                 0.271294   \n",
       "12358                     0.009320                 0.293204   \n",
       "12359                     0.016448                 0.370597   \n",
       "\n",
       "       P(persons_hispanic|C)  P(persons_below_poverty|C)  P(18plus|C)  \\\n",
       "9270                0.031722                    0.112830     0.765729   \n",
       "9271                0.048025                    0.100441     0.788381   \n",
       "9272                0.048318                    0.212244     0.792941   \n",
       "9273                0.029212                    0.193115     0.788099   \n",
       "9274                0.096840                    0.140105     0.770401   \n",
       "...                      ...                         ...          ...   \n",
       "12355               0.162076                    0.114855     0.745146   \n",
       "12356               0.151975                    0.068791     0.823739   \n",
       "12357               0.098559                    0.064003     0.720724   \n",
       "12358               0.142783                    0.065113     0.781230   \n",
       "12359               0.035371                    0.120815     0.795779   \n",
       "\n",
       "       P(democrat|C)  P(other|C)  P(republican|C)  P(non_voter|C)  \n",
       "9270        0.127687    0.007301         0.337605        0.293137  \n",
       "9271        0.105295    0.006670         0.357913        0.318503  \n",
       "9272        0.193592    0.003216         0.225992        0.370141  \n",
       "9273        0.089254    0.003775         0.338187        0.356883  \n",
       "9274        0.044687    0.004012         0.418285        0.303418  \n",
       "...              ...         ...              ...             ...  \n",
       "12355       0.090853    0.015352         0.290620        0.348321  \n",
       "12356       0.421828    0.025615         0.185942        0.190354  \n",
       "12357       0.077436    0.018106         0.364840        0.260343  \n",
       "12358       0.084272    0.017605         0.420065        0.259288  \n",
       "12359       0.052402    0.013537         0.452256        0.277584  \n",
       "\n",
       "[3090 rows x 123 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['year']==2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "099d3dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9270,\n",
       " 9271,\n",
       " 9272,\n",
       " 9273,\n",
       " 9274,\n",
       " 9275,\n",
       " 9276,\n",
       " 9277,\n",
       " 9278,\n",
       " 9279,\n",
       " 9280,\n",
       " 9281,\n",
       " 9282,\n",
       " 9283,\n",
       " 9284,\n",
       " 9285,\n",
       " 9286,\n",
       " 9287,\n",
       " 9288,\n",
       " 9289,\n",
       " 9290,\n",
       " 9291,\n",
       " 9292,\n",
       " 9293,\n",
       " 9294,\n",
       " 9295,\n",
       " 9296,\n",
       " 9297,\n",
       " 9298,\n",
       " 9299,\n",
       " 9300,\n",
       " 9301,\n",
       " 9302,\n",
       " 9303,\n",
       " 9304,\n",
       " 9305,\n",
       " 9306,\n",
       " 9307,\n",
       " 9308,\n",
       " 9309,\n",
       " 9310,\n",
       " 9311,\n",
       " 9312,\n",
       " 9313,\n",
       " 9314,\n",
       " 9315,\n",
       " 9316,\n",
       " 9317,\n",
       " 9318,\n",
       " 9319,\n",
       " 9320,\n",
       " 9321,\n",
       " 9322,\n",
       " 9323,\n",
       " 9324,\n",
       " 9325,\n",
       " 9326,\n",
       " 9327,\n",
       " 9328,\n",
       " 9329,\n",
       " 9330,\n",
       " 9331,\n",
       " 9332,\n",
       " 9333,\n",
       " 9334,\n",
       " 9335,\n",
       " 9336,\n",
       " 9337,\n",
       " 9338,\n",
       " 9339,\n",
       " 9340,\n",
       " 9341,\n",
       " 9342,\n",
       " 9343,\n",
       " 9344,\n",
       " 9345,\n",
       " 9346,\n",
       " 9347,\n",
       " 9348,\n",
       " 9349,\n",
       " 9350,\n",
       " 9351,\n",
       " 9352,\n",
       " 9353,\n",
       " 9354,\n",
       " 9355,\n",
       " 9356,\n",
       " 9357,\n",
       " 9358,\n",
       " 9359,\n",
       " 9360,\n",
       " 9361,\n",
       " 9362,\n",
       " 9363,\n",
       " 9364,\n",
       " 9365,\n",
       " 9366,\n",
       " 9367,\n",
       " 9368,\n",
       " 9369,\n",
       " 9370,\n",
       " 9371,\n",
       " 9372,\n",
       " 9373,\n",
       " 9374,\n",
       " 9375,\n",
       " 9376,\n",
       " 9377,\n",
       " 9378,\n",
       " 9379,\n",
       " 9380,\n",
       " 9381,\n",
       " 9382,\n",
       " 9383,\n",
       " 9384,\n",
       " 9385,\n",
       " 9386,\n",
       " 9387,\n",
       " 9388,\n",
       " 9389,\n",
       " 9390,\n",
       " 9391,\n",
       " 9392,\n",
       " 9393,\n",
       " 9394,\n",
       " 9395,\n",
       " 9396,\n",
       " 9397,\n",
       " 9398,\n",
       " 9399,\n",
       " 9400,\n",
       " 9401,\n",
       " 9402,\n",
       " 9403,\n",
       " 9404,\n",
       " 9405,\n",
       " 9406,\n",
       " 9407,\n",
       " 9408,\n",
       " 9409,\n",
       " 9410,\n",
       " 9411,\n",
       " 9412,\n",
       " 9413,\n",
       " 9414,\n",
       " 9415,\n",
       " 9416,\n",
       " 9417,\n",
       " 9418,\n",
       " 9419,\n",
       " 9420,\n",
       " 9421,\n",
       " 9422,\n",
       " 9423,\n",
       " 9424,\n",
       " 9425,\n",
       " 9426,\n",
       " 9427,\n",
       " 9428,\n",
       " 9429,\n",
       " 9430,\n",
       " 9431,\n",
       " 9432,\n",
       " 9433,\n",
       " 9434,\n",
       " 9435,\n",
       " 9436,\n",
       " 9437,\n",
       " 9438,\n",
       " 9439,\n",
       " 9440,\n",
       " 9441,\n",
       " 9442,\n",
       " 9443,\n",
       " 9444,\n",
       " 9445,\n",
       " 9446,\n",
       " 9447,\n",
       " 9448,\n",
       " 9449,\n",
       " 9450,\n",
       " 9451,\n",
       " 9452,\n",
       " 9453,\n",
       " 9454,\n",
       " 9455,\n",
       " 9456,\n",
       " 9457,\n",
       " 9458,\n",
       " 9459,\n",
       " 9460,\n",
       " 9461,\n",
       " 9462,\n",
       " 9463,\n",
       " 9464,\n",
       " 9465,\n",
       " 9466,\n",
       " 9467,\n",
       " 9468,\n",
       " 9469,\n",
       " 9470,\n",
       " 9471,\n",
       " 9472,\n",
       " 9473,\n",
       " 9474,\n",
       " 9475,\n",
       " 9476,\n",
       " 9477,\n",
       " 9478,\n",
       " 9479,\n",
       " 9480,\n",
       " 9481,\n",
       " 9482,\n",
       " 9483,\n",
       " 9484,\n",
       " 9485,\n",
       " 9486,\n",
       " 9487,\n",
       " 9488,\n",
       " 9489,\n",
       " 9490,\n",
       " 9491,\n",
       " 9492,\n",
       " 9493,\n",
       " 9494,\n",
       " 9495,\n",
       " 9496,\n",
       " 9497,\n",
       " 9498,\n",
       " 9499,\n",
       " 9500,\n",
       " 9501,\n",
       " 9502,\n",
       " 9503,\n",
       " 9504,\n",
       " 9505,\n",
       " 9506,\n",
       " 9507,\n",
       " 9508,\n",
       " 9509,\n",
       " 9510,\n",
       " 9511,\n",
       " 9512,\n",
       " 9513,\n",
       " 9514,\n",
       " 9515,\n",
       " 9516,\n",
       " 9517,\n",
       " 9518,\n",
       " 9519,\n",
       " 9520,\n",
       " 9521,\n",
       " 9522,\n",
       " 9523,\n",
       " 9524,\n",
       " 9525,\n",
       " 9526,\n",
       " 9527,\n",
       " 9528,\n",
       " 9529,\n",
       " 9530,\n",
       " 9531,\n",
       " 9532,\n",
       " 9533,\n",
       " 9534,\n",
       " 9535,\n",
       " 9536,\n",
       " 9537,\n",
       " 9538,\n",
       " 9539,\n",
       " 9540,\n",
       " 9541,\n",
       " 9542,\n",
       " 9543,\n",
       " 9544,\n",
       " 9545,\n",
       " 9546,\n",
       " 9547,\n",
       " 9548,\n",
       " 9549,\n",
       " 9550,\n",
       " 9551,\n",
       " 9552,\n",
       " 9553,\n",
       " 9554,\n",
       " 9555,\n",
       " 9556,\n",
       " 9557,\n",
       " 9558,\n",
       " 9559,\n",
       " 9560,\n",
       " 9561,\n",
       " 9562,\n",
       " 9563,\n",
       " 9564,\n",
       " 9565,\n",
       " 9566,\n",
       " 9567,\n",
       " 9568,\n",
       " 9569,\n",
       " 9570,\n",
       " 9571,\n",
       " 9572,\n",
       " 9573,\n",
       " 9574,\n",
       " 9575,\n",
       " 9576,\n",
       " 9577,\n",
       " 9578,\n",
       " 9579,\n",
       " 9580,\n",
       " 9581,\n",
       " 9582,\n",
       " 9583,\n",
       " 9584,\n",
       " 9585,\n",
       " 9586,\n",
       " 9587,\n",
       " 9588,\n",
       " 9589,\n",
       " 9590,\n",
       " 9591,\n",
       " 9592,\n",
       " 9593,\n",
       " 9594,\n",
       " 9595,\n",
       " 9596,\n",
       " 9597,\n",
       " 9598,\n",
       " 9599,\n",
       " 9600,\n",
       " 9601,\n",
       " 9602,\n",
       " 9603,\n",
       " 9604,\n",
       " 9605,\n",
       " 9606,\n",
       " 9607,\n",
       " 9608,\n",
       " 9609,\n",
       " 9610,\n",
       " 9611,\n",
       " 9612,\n",
       " 9613,\n",
       " 9614,\n",
       " 9615,\n",
       " 9616,\n",
       " 9617,\n",
       " 9618,\n",
       " 9619,\n",
       " 9620,\n",
       " 9621,\n",
       " 9622,\n",
       " 9623,\n",
       " 9624,\n",
       " 9625,\n",
       " 9626,\n",
       " 9627,\n",
       " 9628,\n",
       " 9629,\n",
       " 9630,\n",
       " 9631,\n",
       " 9632,\n",
       " 9633,\n",
       " 9634,\n",
       " 9635,\n",
       " 9636,\n",
       " 9637,\n",
       " 9638,\n",
       " 9639,\n",
       " 9640,\n",
       " 9641,\n",
       " 9642,\n",
       " 9643,\n",
       " 9644,\n",
       " 9645,\n",
       " 9646,\n",
       " 9647,\n",
       " 9648,\n",
       " 9649,\n",
       " 9650,\n",
       " 9651,\n",
       " 9652,\n",
       " 9653,\n",
       " 9654,\n",
       " 9655,\n",
       " 9656,\n",
       " 9657,\n",
       " 9658,\n",
       " 9659,\n",
       " 9660,\n",
       " 9661,\n",
       " 9662,\n",
       " 9663,\n",
       " 9664,\n",
       " 9665,\n",
       " 9666,\n",
       " 9667,\n",
       " 9668,\n",
       " 9669,\n",
       " 9670,\n",
       " 9671,\n",
       " 9672,\n",
       " 9673,\n",
       " 9674,\n",
       " 9675,\n",
       " 9676,\n",
       " 9677,\n",
       " 9678,\n",
       " 9679,\n",
       " 9680,\n",
       " 9681,\n",
       " 9682,\n",
       " 9683,\n",
       " 9684,\n",
       " 9685,\n",
       " 9686,\n",
       " 9687,\n",
       " 9688,\n",
       " 9689,\n",
       " 9690,\n",
       " 9691,\n",
       " 9692,\n",
       " 9693,\n",
       " 9694,\n",
       " 9695,\n",
       " 9696,\n",
       " 9697,\n",
       " 9698,\n",
       " 9699,\n",
       " 9700,\n",
       " 9701,\n",
       " 9702,\n",
       " 9703,\n",
       " 9704,\n",
       " 9705,\n",
       " 9706,\n",
       " 9707,\n",
       " 9708,\n",
       " 9709,\n",
       " 9710,\n",
       " 9711,\n",
       " 9712,\n",
       " 9713,\n",
       " 9714,\n",
       " 9715,\n",
       " 9716,\n",
       " 9717,\n",
       " 9718,\n",
       " 9719,\n",
       " 9720,\n",
       " 9721,\n",
       " 9722,\n",
       " 9723,\n",
       " 9724,\n",
       " 9725,\n",
       " 9726,\n",
       " 9727,\n",
       " 9728,\n",
       " 9729,\n",
       " 9730,\n",
       " 9731,\n",
       " 9732,\n",
       " 9733,\n",
       " 9734,\n",
       " 9735,\n",
       " 9736,\n",
       " 9737,\n",
       " 9738,\n",
       " 9739,\n",
       " 9740,\n",
       " 9741,\n",
       " 9742,\n",
       " 9743,\n",
       " 9744,\n",
       " 9745,\n",
       " 9746,\n",
       " 9747,\n",
       " 9748,\n",
       " 9749,\n",
       " 9750,\n",
       " 9751,\n",
       " 9752,\n",
       " 9753,\n",
       " 9754,\n",
       " 9755,\n",
       " 9756,\n",
       " 9757,\n",
       " 9758,\n",
       " 9759,\n",
       " 9760,\n",
       " 9761,\n",
       " 9762,\n",
       " 9763,\n",
       " 9764,\n",
       " 9765,\n",
       " 9766,\n",
       " 9767,\n",
       " 9768,\n",
       " 9769,\n",
       " 9770,\n",
       " 9771,\n",
       " 9772,\n",
       " 9773,\n",
       " 9774,\n",
       " 9775,\n",
       " 9776,\n",
       " 9777,\n",
       " 9778,\n",
       " 9779,\n",
       " 9780,\n",
       " 9781,\n",
       " 9782,\n",
       " 9783,\n",
       " 9784,\n",
       " 9785,\n",
       " 9786,\n",
       " 9787,\n",
       " 9788,\n",
       " 9789,\n",
       " 9790,\n",
       " 9791,\n",
       " 9792,\n",
       " 9793,\n",
       " 9794,\n",
       " 9795,\n",
       " 9796,\n",
       " 9797,\n",
       " 9798,\n",
       " 9799,\n",
       " 9800,\n",
       " 9801,\n",
       " 9802,\n",
       " 9803,\n",
       " 9804,\n",
       " 9805,\n",
       " 9806,\n",
       " 9807,\n",
       " 9808,\n",
       " 9809,\n",
       " 9810,\n",
       " 9811,\n",
       " 9812,\n",
       " 9813,\n",
       " 9814,\n",
       " 9815,\n",
       " 9816,\n",
       " 9817,\n",
       " 9818,\n",
       " 9819,\n",
       " 9820,\n",
       " 9821,\n",
       " 9822,\n",
       " 9823,\n",
       " 9824,\n",
       " 9825,\n",
       " 9826,\n",
       " 9827,\n",
       " 9828,\n",
       " 9829,\n",
       " 9830,\n",
       " 9831,\n",
       " 9832,\n",
       " 9833,\n",
       " 9834,\n",
       " 9835,\n",
       " 9836,\n",
       " 9837,\n",
       " 9838,\n",
       " 9839,\n",
       " 9840,\n",
       " 9841,\n",
       " 9842,\n",
       " 9843,\n",
       " 9844,\n",
       " 9845,\n",
       " 9846,\n",
       " 9847,\n",
       " 9848,\n",
       " 9849,\n",
       " 9850,\n",
       " 9851,\n",
       " 9852,\n",
       " 9853,\n",
       " 9854,\n",
       " 9855,\n",
       " 9856,\n",
       " 9857,\n",
       " 9858,\n",
       " 9859,\n",
       " 9860,\n",
       " 9861,\n",
       " 9862,\n",
       " 9863,\n",
       " 9864,\n",
       " 9865,\n",
       " 9866,\n",
       " 9867,\n",
       " 9868,\n",
       " 9869,\n",
       " 9870,\n",
       " 9871,\n",
       " 9872,\n",
       " 9873,\n",
       " 9874,\n",
       " 9875,\n",
       " 9876,\n",
       " 9877,\n",
       " 9878,\n",
       " 9879,\n",
       " 9880,\n",
       " 9881,\n",
       " 9882,\n",
       " 9883,\n",
       " 9884,\n",
       " 9885,\n",
       " 9886,\n",
       " 9887,\n",
       " 9888,\n",
       " 9889,\n",
       " 9890,\n",
       " 9891,\n",
       " 9892,\n",
       " 9893,\n",
       " 9894,\n",
       " 9895,\n",
       " 9896,\n",
       " 9897,\n",
       " 9898,\n",
       " 9899,\n",
       " 9900,\n",
       " 9901,\n",
       " 9902,\n",
       " 9903,\n",
       " 9904,\n",
       " 9905,\n",
       " 9906,\n",
       " 9907,\n",
       " 9908,\n",
       " 9909,\n",
       " 9910,\n",
       " 9911,\n",
       " 9912,\n",
       " 9913,\n",
       " 9914,\n",
       " 9915,\n",
       " 9916,\n",
       " 9917,\n",
       " 9918,\n",
       " 9919,\n",
       " 9920,\n",
       " 9921,\n",
       " 9922,\n",
       " 9923,\n",
       " 9924,\n",
       " 9925,\n",
       " 9926,\n",
       " 9927,\n",
       " 9928,\n",
       " 9929,\n",
       " 9930,\n",
       " 9931,\n",
       " 9932,\n",
       " 9933,\n",
       " 9934,\n",
       " 9935,\n",
       " 9936,\n",
       " 9937,\n",
       " 9938,\n",
       " 9939,\n",
       " 9940,\n",
       " 9941,\n",
       " 9942,\n",
       " 9943,\n",
       " 9944,\n",
       " 9945,\n",
       " 9946,\n",
       " 9947,\n",
       " 9948,\n",
       " 9949,\n",
       " 9950,\n",
       " 9951,\n",
       " 9952,\n",
       " 9953,\n",
       " 9954,\n",
       " 9955,\n",
       " 9956,\n",
       " 9957,\n",
       " 9958,\n",
       " 9959,\n",
       " 9960,\n",
       " 9961,\n",
       " 9962,\n",
       " 9963,\n",
       " 9964,\n",
       " 9965,\n",
       " 9966,\n",
       " 9967,\n",
       " 9968,\n",
       " 9969,\n",
       " 9970,\n",
       " 9971,\n",
       " 9972,\n",
       " 9973,\n",
       " 9974,\n",
       " 9975,\n",
       " 9976,\n",
       " 9977,\n",
       " 9978,\n",
       " 9979,\n",
       " 9980,\n",
       " 9981,\n",
       " 9982,\n",
       " 9983,\n",
       " 9984,\n",
       " 9985,\n",
       " 9986,\n",
       " 9987,\n",
       " 9988,\n",
       " 9989,\n",
       " 9990,\n",
       " 9991,\n",
       " 9992,\n",
       " 9993,\n",
       " 9994,\n",
       " 9995,\n",
       " 9996,\n",
       " 9997,\n",
       " 9998,\n",
       " 9999,\n",
       " 10000,\n",
       " 10001,\n",
       " 10002,\n",
       " 10003,\n",
       " 10004,\n",
       " 10005,\n",
       " 10006,\n",
       " 10007,\n",
       " 10008,\n",
       " 10009,\n",
       " 10010,\n",
       " 10011,\n",
       " 10012,\n",
       " 10013,\n",
       " 10014,\n",
       " 10015,\n",
       " 10016,\n",
       " 10017,\n",
       " 10018,\n",
       " 10019,\n",
       " 10020,\n",
       " 10021,\n",
       " 10022,\n",
       " 10023,\n",
       " 10024,\n",
       " 10025,\n",
       " 10026,\n",
       " 10027,\n",
       " 10028,\n",
       " 10029,\n",
       " 10030,\n",
       " 10031,\n",
       " 10032,\n",
       " 10033,\n",
       " 10034,\n",
       " 10035,\n",
       " 10036,\n",
       " 10037,\n",
       " 10038,\n",
       " 10039,\n",
       " 10040,\n",
       " 10041,\n",
       " 10042,\n",
       " 10043,\n",
       " 10044,\n",
       " 10045,\n",
       " 10046,\n",
       " 10047,\n",
       " 10048,\n",
       " 10049,\n",
       " 10050,\n",
       " 10051,\n",
       " 10052,\n",
       " 10053,\n",
       " 10054,\n",
       " 10055,\n",
       " 10056,\n",
       " 10057,\n",
       " 10058,\n",
       " 10059,\n",
       " 10060,\n",
       " 10061,\n",
       " 10062,\n",
       " 10063,\n",
       " 10064,\n",
       " 10065,\n",
       " 10066,\n",
       " 10067,\n",
       " 10068,\n",
       " 10069,\n",
       " 10070,\n",
       " 10071,\n",
       " 10072,\n",
       " 10073,\n",
       " 10074,\n",
       " 10075,\n",
       " 10076,\n",
       " 10077,\n",
       " 10078,\n",
       " 10079,\n",
       " 10080,\n",
       " 10081,\n",
       " 10082,\n",
       " 10083,\n",
       " 10084,\n",
       " 10085,\n",
       " 10086,\n",
       " 10087,\n",
       " 10088,\n",
       " 10089,\n",
       " 10090,\n",
       " 10091,\n",
       " 10092,\n",
       " 10093,\n",
       " 10094,\n",
       " 10095,\n",
       " 10096,\n",
       " 10097,\n",
       " 10098,\n",
       " 10099,\n",
       " 10100,\n",
       " 10101,\n",
       " 10102,\n",
       " 10103,\n",
       " 10104,\n",
       " 10105,\n",
       " 10106,\n",
       " 10107,\n",
       " 10108,\n",
       " 10109,\n",
       " 10110,\n",
       " 10111,\n",
       " 10112,\n",
       " 10113,\n",
       " 10114,\n",
       " 10115,\n",
       " 10116,\n",
       " 10117,\n",
       " 10118,\n",
       " 10119,\n",
       " 10120,\n",
       " 10121,\n",
       " 10122,\n",
       " 10123,\n",
       " 10124,\n",
       " 10125,\n",
       " 10126,\n",
       " 10127,\n",
       " 10128,\n",
       " 10129,\n",
       " 10130,\n",
       " 10131,\n",
       " 10132,\n",
       " 10133,\n",
       " 10134,\n",
       " 10135,\n",
       " 10136,\n",
       " 10137,\n",
       " 10138,\n",
       " 10139,\n",
       " 10140,\n",
       " 10141,\n",
       " 10142,\n",
       " 10143,\n",
       " 10144,\n",
       " 10145,\n",
       " 10146,\n",
       " 10147,\n",
       " 10148,\n",
       " 10149,\n",
       " 10150,\n",
       " 10151,\n",
       " 10152,\n",
       " 10153,\n",
       " 10154,\n",
       " 10155,\n",
       " 10156,\n",
       " 10157,\n",
       " 10158,\n",
       " 10159,\n",
       " 10160,\n",
       " 10161,\n",
       " 10162,\n",
       " 10163,\n",
       " 10164,\n",
       " 10165,\n",
       " 10166,\n",
       " 10167,\n",
       " 10168,\n",
       " 10169,\n",
       " 10170,\n",
       " 10171,\n",
       " 10172,\n",
       " 10173,\n",
       " 10174,\n",
       " 10175,\n",
       " 10176,\n",
       " 10177,\n",
       " 10178,\n",
       " 10179,\n",
       " 10180,\n",
       " 10181,\n",
       " 10182,\n",
       " 10183,\n",
       " 10184,\n",
       " 10185,\n",
       " 10186,\n",
       " 10187,\n",
       " 10188,\n",
       " 10189,\n",
       " 10190,\n",
       " 10191,\n",
       " 10192,\n",
       " 10193,\n",
       " 10194,\n",
       " 10195,\n",
       " 10196,\n",
       " 10197,\n",
       " 10198,\n",
       " 10199,\n",
       " 10200,\n",
       " 10201,\n",
       " 10202,\n",
       " 10203,\n",
       " 10204,\n",
       " 10205,\n",
       " 10206,\n",
       " 10207,\n",
       " 10208,\n",
       " 10209,\n",
       " 10210,\n",
       " 10211,\n",
       " 10212,\n",
       " 10213,\n",
       " 10214,\n",
       " 10215,\n",
       " 10216,\n",
       " 10217,\n",
       " 10218,\n",
       " 10219,\n",
       " 10220,\n",
       " 10221,\n",
       " 10222,\n",
       " 10223,\n",
       " 10224,\n",
       " 10225,\n",
       " 10226,\n",
       " 10227,\n",
       " 10228,\n",
       " 10229,\n",
       " 10230,\n",
       " 10231,\n",
       " 10232,\n",
       " 10233,\n",
       " 10234,\n",
       " 10235,\n",
       " 10236,\n",
       " 10237,\n",
       " 10238,\n",
       " 10239,\n",
       " 10240,\n",
       " 10241,\n",
       " 10242,\n",
       " 10243,\n",
       " 10244,\n",
       " 10245,\n",
       " 10246,\n",
       " 10247,\n",
       " 10248,\n",
       " 10249,\n",
       " 10250,\n",
       " 10251,\n",
       " 10252,\n",
       " 10253,\n",
       " 10254,\n",
       " 10255,\n",
       " 10256,\n",
       " 10257,\n",
       " 10258,\n",
       " 10259,\n",
       " 10260,\n",
       " 10261,\n",
       " 10262,\n",
       " 10263,\n",
       " 10264,\n",
       " 10265,\n",
       " 10266,\n",
       " 10267,\n",
       " 10268,\n",
       " 10269,\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = (df[df['year'] == 2020]).index.tolist()\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f408846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Default Hyperparameter Grids for CV ---\n",
    "RIDGE_PARAM_GRID = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "MLP1_PARAM_GRID = {\n",
    "    'n_hidden': [16, 64, 128],\n",
    "    'dropout_rate': [0.1, 0.3, 0.5],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "    # Note: weight_decay could be added here too if desired\n",
    "}\n",
    "MLP2_PARAM_GRID = {\n",
    "    'shared_hidden_size': [16, 32, 64],\n",
    "    'dropout_rate': [0.1, 0.3, 0.5],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "    # Note: weight_decay could be added here too if desired\n",
    "}\n",
    "\n",
    "# --- XGBoost Hyperparameter Grid and Constants ---\n",
    "XGB_PARAM_GRID = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],     # Step size shrinkage (eta)\n",
    "    'max_depth': [5, 7],                # Max depth of a tree\n",
    "    'subsample': [0.8, 1.0],         # Fraction of samples used per tree\n",
    "    'colsample_bytree': [0.8, 1.0],  # Fraction of features used per tree\n",
    "    'gamma': [0.1, 0.2],                # Min loss reduction for split (min_split_loss)\n",
    "    'reg_alpha': [0, 0.1, 1.0],            # L1 regularization\n",
    "    'reg_lambda': [0, 0.1, 1.0],           # L2 regularization\n",
    "    # Fixed parameters for consistency\n",
    "    'objective': ['reg:squarederror'], # Regression objective for each target\n",
    "    'n_estimators': [200],             # High initial value, CV uses early stopping\n",
    "    'random_state': [42]               # For reproducibility\n",
    "}\n",
    "\n",
    "XGB_EARLY_STOPPING_ROUNDS = 20 # Early stopping rounds for CV fits\n",
    "\n",
    "RUNG_EPOCHS = [25, 50, 75, 100, 125, 150, 175, 200] # Rung epochs for MLP models\n",
    "RUNG_PATIENCE = [15, 20, 25, 30, 35, 40, 45, 50] # Rung patience for MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497e3789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized - Using 114 features - Test year: 2020\n",
      "Updated cross-validation DataLoaders with batch size 64.\n",
      "Updated final training DataLoader with batch size 64.\n"
     ]
    }
   ],
   "source": [
    "#set a manual seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "dh = ep.DataHandler()\n",
    "dh.update_cv_dataloaders(batch_size=BATCH_SIZE)\n",
    "dh.update_final_dataloader(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a35132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNModel initialized for 'mlp1'. CV results: ./results/mlp1_cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "MLP1_PARAM_GRID = {\n",
    "    'hidden_layers': [[16], [32], [64], [128]],       # Width of the single hidden layer\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],        # Optimizer learning rate\n",
    "    'dropout_rate': [0.0, 0.1, 0.2],           # Dropout regularization\n",
    "    'weight_decay': [0, 1e-5, 1e-3]             # L2 regularization (AdamW style)\n",
    "}\n",
    "\n",
    "RUNG_EPOCHS = [25, 50, 75, 100, 125, 150, 175, 200] # Rung epochs for MLP models\n",
    "RUNG_PATIENCE = [15, 20, 25, 30, 35, 40, 45, 50] # Rung patience for MLP models\n",
    "MLP1_SCHEDULE = list(zip(RUNG_EPOCHS, RUNG_PATIENCE))\n",
    "\n",
    "#make mlp1 model\n",
    "mlp1_model = ep.NNModel(model_name = 'mlp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e1a282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0,\n",
       " 'hidden_layers': [128],\n",
       " 'learning_rate': np.float64(0.001),\n",
       " 'weight_decay': np.float64(1e-05),\n",
       " 'last_epoch': 153,\n",
       " 'best_epoch': 104,\n",
       " 'train_loss_at_best': np.float64(0.832217),\n",
       " 'best_val_loss': np.float64(0.839444)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1_model._parse_best_params_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28b772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Model Training for MLP1 ---\n",
      "Using best hyperparameters from CV: {'dropout_rate': 0, 'hidden_layers': [128], 'learning_rate': np.float64(0.001), 'weight_decay': np.float64(1e-05), 'last_epoch': 153, 'best_epoch': 104, 'train_loss_at_best': np.float64(0.832217), 'best_val_loss': np.float64(0.839444)}\n",
      "Starting final training for up to 150 epochs (Patience: 50)...\n",
      "  Epoch 10/150 - Loss: 0.841157 (Best Loss: 0.841157, Epochs No Improve: 0)\n",
      "  Epoch 20/150 - Loss: 0.838287 (Best Loss: 0.838287, Epochs No Improve: 0)\n",
      "  Epoch 30/150 - Loss: 0.838319 (Best Loss: 0.837144, Epochs No Improve: 2)\n",
      "  Epoch 40/150 - Loss: 0.838080 (Best Loss: 0.835730, Epochs No Improve: 1)\n",
      "  Epoch 50/150 - Loss: 0.835654 (Best Loss: 0.835585, Epochs No Improve: 1)\n",
      "  Epoch 60/150 - Loss: 0.835354 (Best Loss: 0.835354, Epochs No Improve: 0)\n",
      "  Epoch 70/150 - Loss: 0.835682 (Best Loss: 0.834135, Epochs No Improve: 6)\n",
      "  Epoch 80/150 - Loss: 0.834687 (Best Loss: 0.834135, Epochs No Improve: 16)\n",
      "  Epoch 90/150 - Loss: 0.836347 (Best Loss: 0.834014, Epochs No Improve: 5)\n",
      "  Epoch 100/150 - Loss: 0.835326 (Best Loss: 0.833792, Epochs No Improve: 2)\n",
      "  Epoch 110/150 - Loss: 0.835492 (Best Loss: 0.833792, Epochs No Improve: 12)\n",
      "  Epoch 120/150 - Loss: 0.833735 (Best Loss: 0.833735, Epochs No Improve: 0)\n",
      "  Epoch 130/150 - Loss: 0.834784 (Best Loss: 0.833067, Epochs No Improve: 2)\n",
      "  Epoch 140/150 - Loss: 0.835375 (Best Loss: 0.833067, Epochs No Improve: 12)\n",
      "  Epoch 150/150 - Loss: 0.833415 (Best Loss: 0.833067, Epochs No Improve: 22)\n",
      "Loaded model state from epoch with best loss: 0.833067\n",
      "Saved best model state_dict to: ./models/mlp1_final_state_dict.pth\n",
      "Saved training loss history to: ./results/mlp1_final_training_loss.csv\n",
      "--- Finished Final Model Training for MLP1 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=114, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0, inplace=False)\n",
       "  (3): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (4): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1_model.train_final_model(dh, \n",
    "                             final_train_epochs=FINAL_TRAIN_EPOCHS,\n",
    "                             optimizer_choice=OPTIMIZER_CHOICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92ec281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Predictions for MLP1 on Year 2020 ---\n",
      "County-level raw predictions saved to: ./preds/2020_mlp1_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.15219903, 0.02495648, 0.26912528, 0.31944814],\n",
       "       [0.15105082, 0.03149771, 0.31526598, 0.29056692],\n",
       "       [0.18049634, 0.0103337 , 0.20910275, 0.3930085 ],\n",
       "       ...,\n",
       "       [0.08355194, 0.05605646, 0.28210762, 0.29900822],\n",
       "       [0.09211148, 0.04730706, 0.36937997, 0.27243122],\n",
       "       [0.11738477, 0.03578475, 0.27462345, 0.3679858 ]],\n",
       "      shape=(3090, 4), dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1_model.predict(dh, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faf2c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting SHA Cross-Validation for MLP1 (eta=3) ---\n",
      "-------------------------------------------------------------------------------\n",
      ">>> SHA Rung 1/8 | Target Epochs: 25 | Patience: 15 | Evaluating 108 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     1  |     23     |     15     |  0.838874  |  0.845222  |      27.59s    |\n",
      "|     2  |     22     |     11     |  0.841238  |  0.843598  |      25.90s    |\n",
      "|     3  |     25     |     19     |  0.840484  |  0.845393  |      29.50s    |\n",
      "|     4  |     25     |     21     |  0.838757  |  0.845708  |      29.60s    |\n",
      "|     5  |     25     |     19     |  0.838702  |  0.845632  |      29.88s    |\n",
      "|     6  |     25     |     20     |  0.839764  |  0.846116  |      29.90s    |\n",
      "|     7  |     25     |     24     |  0.844878  |  0.853489  |      30.43s    |\n",
      "|     8  |     25     |     24     |  0.846943  |  0.854509  |      29.99s    |\n",
      "|     9  |     25     |     24     |  0.844672  |  0.853544  |      30.14s    |\n",
      "|    10  |     24     |     19     |  0.838599  |  0.843556  |      29.66s    |\n",
      "|    11  |     23     |     13     |  0.838751  |  0.843209  |      28.71s    |\n",
      "|    12  |     25     |     23     |  0.837997  |  0.841162  |      30.48s    |\n",
      "|    13  |     25     |     20     |   0.83789  |   0.84508  |      29.99s    |\n",
      "|    14  |     23     |     16     |  0.838794  |  0.845754  |      28.24s    |\n",
      "|    15  |     25     |     19     |  0.837428  |  0.844385  |      29.75s    |\n",
      "|    16  |     25     |     24     |  0.843097  |  0.851636  |      30.47s    |\n",
      "|    17  |     25     |     25     |  0.842738  |  0.850566  |      29.79s    |\n",
      "|    18  |     25     |     24     |  0.843656  |   0.85132  |      29.79s    |\n",
      "|    19  |     24     |     16     |   0.83837  |  0.843018  |      30.04s    |\n",
      "|    20  |     25     |     19     |  0.838723  |  0.842526  |      30.03s    |\n",
      "|    21  |     25     |     13     |  0.838963  |  0.843484  |      29.97s    |\n",
      "|    22  |     25     |     18     |  0.837203  |  0.844432  |      35.49s    |\n",
      "|    23  |     25     |     20     |  0.836251  |  0.842632  |      43.90s    |\n",
      "|    24  |     24     |     16     |  0.837372  |  0.843611  |      62.28s    |\n",
      "|    25  |     25     |     23     |  0.842058  |  0.850377  |      40.41s    |\n",
      "|    26  |     25     |     23     |  0.841904  |  0.850034  |      37.65s    |\n",
      "|    27  |     25     |     20     |  0.842891  |  0.850723  |      30.17s    |\n",
      "|    28  |     24     |     11     |  0.838885  |  0.843078  |      29.24s    |\n",
      "|    29  |     24     |     12     |  0.838854  |  0.843479  |      31.17s    |\n",
      "|    30  |     24     |     18     |  0.838448  |  0.842586  |      30.38s    |\n",
      "|    31  |     25     |     22     |  0.835837  |  0.842754  |      31.29s    |\n",
      "|    32  |     25     |     17     |  0.835533  |  0.844147  |      31.03s    |\n",
      "|    33  |     25     |     22     |  0.835633  |   0.84179  |      30.99s    |\n",
      "|    34  |     25     |     22     |  0.839914  |  0.848122  |      30.03s    |\n",
      "|    35  |     25     |     23     |  0.839556  |   0.84896  |      30.30s    |\n",
      "|    36  |     25     |     23     |  0.840255  |  0.847501  |      31.02s    |\n",
      "|    37  |     23     |     16     |  0.844282  |  0.845423  |      31.29s    |\n",
      "|    38  |     24     |     10     |  0.844068  |  0.845573  |      31.14s    |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlp1_model\u001b[38;5;241m.\u001b[39mcross_validate(dh,\n\u001b[1;32m      2\u001b[0m                           param_grid\u001b[38;5;241m=\u001b[39mMLP1_PARAM_GRID,\n\u001b[1;32m      3\u001b[0m                           optimizer_choice\u001b[38;5;241m=\u001b[39mOPTIMIZER_CHOICE,\n\u001b[1;32m      4\u001b[0m                           rung_schedule\u001b[38;5;241m=\u001b[39mMLP1_SCHEDULE\n\u001b[1;32m      5\u001b[0m                           )\n",
      "File \u001b[0;32m~/Documents/MATH-392-Intro-to-neural-networks/arvind-midterm-2/election_project.py:737\u001b[0m, in \u001b[0;36mNNModel.cross_validate\u001b[0;34m(self, dh, param_grid, optimizer_choice, rung_schedule, reduction_factor, min_finalists, max_finalists, save)\u001b[0m\n\u001b[1;32m    735\u001b[0m config\u001b[38;5;241m=\u001b[39mconfig_dict[config_id]\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# Call helper to train/evaluate this config for the current rung\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m updated_config_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_one_config(\n\u001b[1;32m    738\u001b[0m                                 config,\n\u001b[1;32m    739\u001b[0m                                 config_id,\n\u001b[1;32m    740\u001b[0m                                 config_history,\n\u001b[1;32m    741\u001b[0m                                 dh,\n\u001b[1;32m    742\u001b[0m                                 rung_epochs,\n\u001b[1;32m    743\u001b[0m                                 rung_patience,\n\u001b[1;32m    744\u001b[0m                                 optimizer_choice\n\u001b[1;32m    745\u001b[0m                             )\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Update the main history dictionary with the results\u001b[39;00m\n\u001b[1;32m    748\u001b[0m cv_history_dict[config_id] \u001b[38;5;241m=\u001b[39m updated_config_history\n",
      "File \u001b[0;32m~/Documents/MATH-392-Intro-to-neural-networks/arvind-midterm-2/election_project.py:596\u001b[0m, in \u001b[0;36mNNModel._train_one_config\u001b[0;34m(self, config, config_id, config_history, dh, rung_epochs, rung_patience, optimizer_choice)\u001b[0m\n\u001b[1;32m    594\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[1;32m    595\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweighted_cross_entropy_loss(outputs, targets_batch, weights)\n\u001b[0;32m--> 596\u001b[0m         epoch_val_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    597\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m epoch_val_loss_sum \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# --- Update Cumulative State ---\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp1_model.cross_validate(dh,\n",
    "                          param_grid=MLP1_PARAM_GRID,\n",
    "                          optimizer_choice=OPTIMIZER_CHOICE,\n",
    "                          rung_schedule=MLP1_SCHEDULE\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf1a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNModel initialized for 'Softmax'. CV results: ./results/Softmax_cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "#make softmax model\n",
    "softmax_model = ep.NNModel(model_name = 'Softmax')\n",
    "\n",
    "# param grid and rung schedule for softmax model\n",
    "SOFTMAX_PARAM_GRID = {'learning_rate': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "                      'weight_decay': [0, 1e-5, 1e-4, 1e-3]\n",
    "                    }\n",
    "SOFTMAX_SCHEDULE = [(50, 20), (75, 25), (100, 30), (150, 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb21cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting SHA Cross-Validation for SOFTMAX (eta=3) ---\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ">>> SHA Rung 1/4 | Target Epochs: 50 | Patience: 20 | Evaluating 16 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     1  |     38     |     21     |   1.09341  |   1.05917  |      43.09s    |\n",
      "|     2  |     46     |     31     |  1.083709  |  1.058134  |      49.63s    |\n",
      "|     3  |     22     |      2     |  1.229732  |  1.062608  |      23.81s    |\n",
      "|     4  |     23     |      3     |  1.147415  |  1.058997  |      25.25s    |\n",
      "|     5  |     38     |     18     |  0.841234  |  0.844526  |      41.64s    |\n",
      "|     6  |     39     |     19     |   0.84157  |  0.843677  |      42.12s    |\n",
      "|     7  |     38     |     19     |  0.841895  |  0.844437  |      41.00s    |\n",
      "|     8  |     39     |     22     |  0.841344  |  0.844243  |      42.80s    |\n",
      "|     9  |     44     |     31     |  0.839508  |   0.84601  |      48.24s    |\n",
      "|    10  |     50     |     34     |  0.839185  |  0.846043  |      54.42s    |\n",
      "|    11  |     45     |     28     |  0.839245  |  0.846325  |      48.62s    |\n",
      "|    12  |     50     |     39     |  0.837802  |   0.84611  |      54.03s    |\n",
      "|    13  |     42     |     34     |  0.848848  |  0.853343  |      45.65s    |\n",
      "|    14  |     50     |     49     |  0.842916  |  0.852438  |      53.47s    |\n",
      "|    15  |     50     |     43     |  0.843647  |  0.851518  |      53.47s    |\n",
      "|    16  |     50     |     45     |  0.843585  |  0.851095  |      50.47s    |\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ">>> SHA Rung 2/4 | Target Epochs: 75 | Patience: 25 | Evaluating 5 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     6  |     44     |     19     |   0.84157  |  0.843677  |       5.05s    |\n",
      "|     8  |     63     |     39     |   0.84138  |   0.84348  |      23.33s    |\n",
      "|     7  |     52     |     27     |   0.84146  |  0.843925  |      15.29s    |\n",
      "|     5  |     43     |     18     |  0.841234  |  0.844526  |       5.75s    |\n",
      "|     9  |     59     |     42     |  0.838568  |  0.845971  |      15.95s    |\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ">>> SHA Rung 3/4 | Target Epochs: 100 | Patience: 30 | Evaluating 3 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     8  |     75     |     47     |  0.842698  |  0.843331  |      12.50s    |\n",
      "|     6  |     49     |     19     |   0.84157  |  0.843677  |       5.33s    |\n",
      "|     7  |     57     |     27     |   0.84146  |  0.843925  |       5.42s    |\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ">>> SHA Rung 4/4 | Target Epochs: 150 | Patience: 40 | Evaluating 3 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     8  |     87     |     47     |  0.842698  |  0.843331  |      12.86s    |\n",
      "|     6  |     59     |     19     |   0.84157  |  0.843677  |      10.73s    |\n",
      "|     7  |     92     |     53     |  0.840768  |  0.843501  |      38.29s    |\n",
      "--- Finished SHA Cross-Validation for SOFTMAX (868.21 seconds) ---\n",
      "Final results for 3 surviving configurations saved to: ./results/Softmax_cv_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>last_epoch</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>train_loss_at_best</th>\n",
       "      <th>best_val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>87</td>\n",
       "      <td>47</td>\n",
       "      <td>0.842698</td>\n",
       "      <td>0.843331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>92</td>\n",
       "      <td>53</td>\n",
       "      <td>0.840768</td>\n",
       "      <td>0.843501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>0.841570</td>\n",
       "      <td>0.843677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  weight_decay  last_epoch  best_epoch  train_loss_at_best  \\\n",
       "0           0.01       0.00100          87          47            0.842698   \n",
       "1           0.01       0.00010          92          53            0.840768   \n",
       "2           0.01       0.00001          59          19            0.841570   \n",
       "\n",
       "   best_val_loss  \n",
       "0       0.843331  \n",
       "1       0.843501  \n",
       "2       0.843677  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_model.cross_validate(dh,\n",
    "                             param_grid=SOFTMAX_PARAM_GRID,\n",
    "                             optimizer_choice=OPTIMIZER_CHOICE,\n",
    "                             rung_schedule=SOFTMAX_SCHEDULE\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21d6558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Model Training for SOFTMAX ---\n",
      "Using best hyperparameters from CV: {'learning_rate': 0.01, 'weight_decay': 0.0001, 'last_epoch': 77.66666666666667, 'best_epoch': 37.666666666666664, 'train_loss_at_best': 0.8412514337149682, 'best_val_loss': 0.8426987974822117}\n",
      "Starting final training for up to 150 epochs (Patience: 50)...\n",
      "  Epoch 10/150 - Loss: 0.843121 (Best Loss: 0.843121, Epochs No Improve: 0)\n",
      "  Epoch 20/150 - Loss: 0.841399 (Best Loss: 0.841399, Epochs No Improve: 0)\n",
      "  Epoch 30/150 - Loss: 0.841244 (Best Loss: 0.840897, Epochs No Improve: 3)\n",
      "  Epoch 40/150 - Loss: 0.840943 (Best Loss: 0.840566, Epochs No Improve: 9)\n",
      "  Epoch 50/150 - Loss: 0.843518 (Best Loss: 0.839810, Epochs No Improve: 1)\n",
      "  Epoch 60/150 - Loss: 0.840637 (Best Loss: 0.839810, Epochs No Improve: 11)\n",
      "  Epoch 70/150 - Loss: 0.841734 (Best Loss: 0.839807, Epochs No Improve: 1)\n",
      "  Epoch 80/150 - Loss: 0.840093 (Best Loss: 0.839807, Epochs No Improve: 11)\n",
      "  Epoch 90/150 - Loss: 0.841632 (Best Loss: 0.839559, Epochs No Improve: 6)\n",
      "  Epoch 100/150 - Loss: 0.839543 (Best Loss: 0.838876, Epochs No Improve: 5)\n",
      "  Epoch 110/150 - Loss: 0.839564 (Best Loss: 0.838798, Epochs No Improve: 8)\n",
      "  Epoch 120/150 - Loss: 0.840540 (Best Loss: 0.838798, Epochs No Improve: 18)\n",
      "  Epoch 130/150 - Loss: 0.839607 (Best Loss: 0.838798, Epochs No Improve: 28)\n",
      "  Epoch 140/150 - Loss: 0.840467 (Best Loss: 0.838798, Epochs No Improve: 38)\n",
      "  Epoch 150/150 - Loss: 0.841163 (Best Loss: 0.838798, Epochs No Improve: 48)\n",
      "Loaded model state from epoch with best loss: 0.838798\n",
      "Saved best model state_dict to: ./models/Softmax_final_state_dict.pth\n",
      "Saved training loss history to: ./results/Softmax_final_training_loss.csv\n",
      "--- Finished Final Model Training for SOFTMAX ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=114, out_features=4, bias=True)\n",
       "  (1): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_model.train_final_model(dh,\n",
    "                                final_train_epochs= FINAL_TRAIN_EPOCHS,\n",
    "                                optimizer_choice=OPTIMIZER_CHOICE\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b94ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Predictions for SOFTMAX on Year 2020 ---\n",
      "County-level raw predictions saved to: ./preds/2020_Softmax_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "softmax_preds = softmax_model.predict(dh, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d19ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15207319, 0.02530907, 0.2590983 , 0.32924837],\n",
       "       [0.15103795, 0.06097275, 0.2786021 , 0.29776868],\n",
       "       [0.20952702, 0.0151517 , 0.16357335, 0.4046892 ],\n",
       "       ...,\n",
       "       [0.11637519, 0.05959414, 0.2536456 , 0.2911093 ],\n",
       "       [0.15302789, 0.04937229, 0.29364637, 0.2851832 ],\n",
       "       [0.08838788, 0.12065135, 0.21881565, 0.3679239 ]],\n",
       "      shape=(3090, 4), dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89032675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
