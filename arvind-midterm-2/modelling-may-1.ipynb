{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab75135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Dict, Tuple, Any, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4981d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import election_project as ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a85a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Default Training Hyperparameters ---\n",
    "BATCH_SIZE: int = 64\n",
    "MAX_CV_EPOCHS: int = 30 # Max epochs for CV\n",
    "PATIENCE: int = 10      # Patience for early stopping during CV\n",
    "FINAL_TRAIN_EPOCHS: int = 150 # Fixed epochs for final training\n",
    "OPTIMIZER_CHOICE: Type[optim.Optimizer] = optim.AdamW # Default optimizer\n",
    "\n",
    "# --- Default Hyperparameter Grids for CV ---\n",
    "RIDGE_PARAM_GRID = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "MLP1_PARAM_GRID = {\n",
    "    'n_hidden': [16, 64, 128],\n",
    "    'dropout_rate': [0.1, 0.3, 0.5],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "    # Note: weight_decay could be added here too if desired\n",
    "}\n",
    "MLP2_PARAM_GRID = {\n",
    "    'shared_hidden_size': [16, 32, 64],\n",
    "    'dropout_rate': [0.1, 0.3, 0.5],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "    # Note: weight_decay could be added here too if desired\n",
    "}\n",
    "\n",
    "# --- XGBoost Hyperparameter Grid and Constants ---\n",
    "XGB_PARAM_GRID = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],     # Step size shrinkage (eta)\n",
    "    'max_depth': [5, 7],                # Max depth of a tree\n",
    "    'subsample': [0.8, 1.0],         # Fraction of samples used per tree\n",
    "    'colsample_bytree': [0.8, 1.0],  # Fraction of features used per tree\n",
    "    'gamma': [0.1, 0.2],                # Min loss reduction for split (min_split_loss)\n",
    "    'reg_alpha': [0, 0.1, 1.0],            # L1 regularization\n",
    "    'reg_lambda': [0, 0.1, 1.0],           # L2 regularization\n",
    "    # Fixed parameters for consistency\n",
    "    'objective': ['reg:squarederror'], # Regression objective for each target\n",
    "    'n_estimators': [200],             # High initial value, CV uses early stopping\n",
    "    'random_state': [42]               # For reproducibility\n",
    "}\n",
    "\n",
    "XGB_EARLY_STOPPING_ROUNDS = 20 # Early stopping rounds for CV fits\n",
    "\n",
    "RUNG_EPOCHS = [25, 50, 75, 100, 125, 150, 175, 200] # Rung epochs for MLP models\n",
    "RUNG_PATIENCE = [15, 20, 25, 30, 35, 40, 45, 50] # Rung patience for MLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "497e3789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized - Using 114 features - Test year: 2020\n",
      "Updated cross-validation DataLoaders with batch size 64.\n",
      "Updated final training DataLoader with batch size 64.\n"
     ]
    }
   ],
   "source": [
    "#set a manual seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "dh = ep.DataHandler()\n",
    "dh.update_cv_dataloaders(batch_size=BATCH_SIZE)\n",
    "dh.update_final_dataloader(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a35132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNModel initialized for 'mlp1'. CV results: ./results/mlp1_cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "MLP1_PARAM_GRID = {\n",
    "    'hidden_layers': [[16], [32], [64], [128]],       # Width of the single hidden layer\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],        # Optimizer learning rate\n",
    "    'dropout_rate': [0.0, 0.1, 0.2],           # Dropout regularization\n",
    "    'weight_decay': [0, 1e-5, 1e-3]             # L2 regularization (AdamW style)\n",
    "}\n",
    "\n",
    "RUNG_EPOCHS = [25, 50, 75, 100, 125, 150, 175, 200] # Rung epochs for MLP models\n",
    "RUNG_PATIENCE = [15, 20, 25, 30, 35, 40, 45, 50] # Rung patience for MLP models\n",
    "MLP1_SCHEDULE = list(zip(RUNG_EPOCHS, RUNG_PATIENCE))\n",
    "\n",
    "#make mlp1 model\n",
    "mlp1_model = ep.NNModel(model_name = 'mlp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faf2c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting SHA Cross-Validation for MLP1 (eta=3) ---\n",
      "-------------------------------------------------------------------------------\n",
      ">>> SHA Rung 1/8 | Target Epochs: 25 | Patience: 15 | Evaluating 108 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     1  |     23     |     15     |  0.838874  |  0.845222  |      27.59s    |\n",
      "|     2  |     22     |     11     |  0.841238  |  0.843598  |      25.90s    |\n",
      "|     3  |     25     |     19     |  0.840484  |  0.845393  |      29.50s    |\n",
      "|     4  |     25     |     21     |  0.838757  |  0.845708  |      29.60s    |\n",
      "|     5  |     25     |     19     |  0.838702  |  0.845632  |      29.88s    |\n",
      "|     6  |     25     |     20     |  0.839764  |  0.846116  |      29.90s    |\n",
      "|     7  |     25     |     24     |  0.844878  |  0.853489  |      30.43s    |\n",
      "|     8  |     25     |     24     |  0.846943  |  0.854509  |      29.99s    |\n",
      "|     9  |     25     |     24     |  0.844672  |  0.853544  |      30.14s    |\n",
      "|    10  |     24     |     19     |  0.838599  |  0.843556  |      29.66s    |\n",
      "|    11  |     23     |     13     |  0.838751  |  0.843209  |      28.71s    |\n",
      "|    12  |     25     |     23     |  0.837997  |  0.841162  |      30.48s    |\n",
      "|    13  |     25     |     20     |   0.83789  |   0.84508  |      29.99s    |\n",
      "|    14  |     23     |     16     |  0.838794  |  0.845754  |      28.24s    |\n",
      "|    15  |     25     |     19     |  0.837428  |  0.844385  |      29.75s    |\n",
      "|    16  |     25     |     24     |  0.843097  |  0.851636  |      30.47s    |\n",
      "|    17  |     25     |     25     |  0.842738  |  0.850566  |      29.79s    |\n",
      "|    18  |     25     |     24     |  0.843656  |   0.85132  |      29.79s    |\n",
      "|    19  |     24     |     16     |   0.83837  |  0.843018  |      30.04s    |\n",
      "|    20  |     25     |     19     |  0.838723  |  0.842526  |      30.03s    |\n",
      "|    21  |     25     |     13     |  0.838963  |  0.843484  |      29.97s    |\n",
      "|    22  |     25     |     18     |  0.837203  |  0.844432  |      35.49s    |\n",
      "|    23  |     25     |     20     |  0.836251  |  0.842632  |      43.90s    |\n",
      "|    24  |     24     |     16     |  0.837372  |  0.843611  |      62.28s    |\n",
      "|    25  |     25     |     23     |  0.842058  |  0.850377  |      40.41s    |\n",
      "|    26  |     25     |     23     |  0.841904  |  0.850034  |      37.65s    |\n",
      "|    27  |     25     |     20     |  0.842891  |  0.850723  |      30.17s    |\n",
      "|    28  |     24     |     11     |  0.838885  |  0.843078  |      29.24s    |\n",
      "|    29  |     24     |     12     |  0.838854  |  0.843479  |      31.17s    |\n",
      "|    30  |     24     |     18     |  0.838448  |  0.842586  |      30.38s    |\n",
      "|    31  |     25     |     22     |  0.835837  |  0.842754  |      31.29s    |\n",
      "|    32  |     25     |     17     |  0.835533  |  0.844147  |      31.03s    |\n",
      "|    33  |     25     |     22     |  0.835633  |   0.84179  |      30.99s    |\n",
      "|    34  |     25     |     22     |  0.839914  |  0.848122  |      30.03s    |\n",
      "|    35  |     25     |     23     |  0.839556  |   0.84896  |      30.30s    |\n",
      "|    36  |     25     |     23     |  0.840255  |  0.847501  |      31.02s    |\n",
      "|    37  |     23     |     16     |  0.844282  |  0.845423  |      31.29s    |\n",
      "|    38  |     24     |     10     |  0.844068  |  0.845573  |      31.14s    |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlp1_model\u001b[38;5;241m.\u001b[39mcross_validate(dh,\n\u001b[1;32m      2\u001b[0m                           param_grid\u001b[38;5;241m=\u001b[39mMLP1_PARAM_GRID,\n\u001b[1;32m      3\u001b[0m                           optimizer_choice\u001b[38;5;241m=\u001b[39mOPTIMIZER_CHOICE,\n\u001b[1;32m      4\u001b[0m                           rung_schedule\u001b[38;5;241m=\u001b[39mMLP1_SCHEDULE\n\u001b[1;32m      5\u001b[0m                           )\n",
      "File \u001b[0;32m~/Documents/MATH-392-Intro-to-neural-networks/arvind-midterm-2/election_project.py:737\u001b[0m, in \u001b[0;36mNNModel.cross_validate\u001b[0;34m(self, dh, param_grid, optimizer_choice, rung_schedule, reduction_factor, min_finalists, max_finalists, save)\u001b[0m\n\u001b[1;32m    735\u001b[0m config\u001b[38;5;241m=\u001b[39mconfig_dict[config_id]\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# Call helper to train/evaluate this config for the current rung\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m updated_config_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_one_config(\n\u001b[1;32m    738\u001b[0m                                 config,\n\u001b[1;32m    739\u001b[0m                                 config_id,\n\u001b[1;32m    740\u001b[0m                                 config_history,\n\u001b[1;32m    741\u001b[0m                                 dh,\n\u001b[1;32m    742\u001b[0m                                 rung_epochs,\n\u001b[1;32m    743\u001b[0m                                 rung_patience,\n\u001b[1;32m    744\u001b[0m                                 optimizer_choice\n\u001b[1;32m    745\u001b[0m                             )\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Update the main history dictionary with the results\u001b[39;00m\n\u001b[1;32m    748\u001b[0m cv_history_dict[config_id] \u001b[38;5;241m=\u001b[39m updated_config_history\n",
      "File \u001b[0;32m~/Documents/MATH-392-Intro-to-neural-networks/arvind-midterm-2/election_project.py:596\u001b[0m, in \u001b[0;36mNNModel._train_one_config\u001b[0;34m(self, config, config_id, config_history, dh, rung_epochs, rung_patience, optimizer_choice)\u001b[0m\n\u001b[1;32m    594\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[1;32m    595\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweighted_cross_entropy_loss(outputs, targets_batch, weights)\n\u001b[0;32m--> 596\u001b[0m         epoch_val_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    597\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m epoch_val_loss_sum \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# --- Update Cumulative State ---\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp1_model.cross_validate(dh,\n",
    "                          param_grid=MLP1_PARAM_GRID,\n",
    "                          optimizer_choice=OPTIMIZER_CHOICE,\n",
    "                          rung_schedule=MLP1_SCHEDULE\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf1a366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNModel initialized for 'Softmax'. CV results: ./results/Softmax_cv_results.csv\n"
     ]
    }
   ],
   "source": [
    "#make softmax model\n",
    "softmax_model = ep.NNModel(model_name = 'Softmax')\n",
    "\n",
    "# param grid and rung schedule for softmax model\n",
    "SOFTMAX_PARAM_GRID = {'learning_rate': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "                      'weight_decay': [0, 1e-5, 1e-4, 1e-3]\n",
    "                    }\n",
    "SOFTMAX_SCHEDULE = [(50, 20), (75, 25), (100, 30), (150, 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb21cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting SHA Cross-Validation for SOFTMAX (eta=3) ---\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ">>> SHA Rung 1/4 | Target Epochs: 50 | Patience: 20 | Evaluating 16 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     1  |     38     |     21     |   1.09341  |   1.05917  |      43.09s    |\n",
      "|     2  |     46     |     31     |  1.083709  |  1.058134  |      49.63s    |\n",
      "|     3  |     22     |      2     |  1.229732  |  1.062608  |      23.81s    |\n",
      "|     4  |     23     |      3     |  1.147415  |  1.058997  |      25.25s    |\n",
      "|     5  |     38     |     18     |  0.841234  |  0.844526  |      41.64s    |\n",
      "|     6  |     39     |     19     |   0.84157  |  0.843677  |      42.12s    |\n",
      "|     7  |     38     |     19     |  0.841895  |  0.844437  |      41.00s    |\n",
      "|     8  |     39     |     22     |  0.841344  |  0.844243  |      42.80s    |\n",
      "|     9  |     44     |     31     |  0.839508  |   0.84601  |      48.24s    |\n",
      "|    10  |     50     |     34     |  0.839185  |  0.846043  |      54.42s    |\n",
      "|    11  |     45     |     28     |  0.839245  |  0.846325  |      48.62s    |\n",
      "|    12  |     50     |     39     |  0.837802  |   0.84611  |      54.03s    |\n",
      "|    13  |     42     |     34     |  0.848848  |  0.853343  |      45.65s    |\n",
      "|    14  |     50     |     49     |  0.842916  |  0.852438  |      53.47s    |\n",
      "|    15  |     50     |     43     |  0.843647  |  0.851518  |      53.47s    |\n",
      "|    16  |     50     |     45     |  0.843585  |  0.851095  |      50.47s    |\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ">>> SHA Rung 2/4 | Target Epochs: 75 | Patience: 25 | Evaluating 5 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     6  |     44     |     19     |   0.84157  |  0.843677  |       5.05s    |\n",
      "|     8  |     63     |     39     |   0.84138  |   0.84348  |      23.33s    |\n",
      "|     7  |     52     |     27     |   0.84146  |  0.843925  |      15.29s    |\n",
      "|     5  |     43     |     18     |  0.841234  |  0.844526  |       5.75s    |\n",
      "|     9  |     59     |     42     |  0.838568  |  0.845971  |      15.95s    |\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ">>> SHA Rung 3/4 | Target Epochs: 100 | Patience: 30 | Evaluating 3 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     8  |     75     |     47     |  0.842698  |  0.843331  |      12.50s    |\n",
      "|     6  |     49     |     19     |   0.84157  |  0.843677  |       5.33s    |\n",
      "|     7  |     57     |     27     |   0.84146  |  0.843925  |       5.42s    |\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      ">>> SHA Rung 4/4 | Target Epochs: 150 | Patience: 40 | Evaluating 3 configs <<<\n",
      "-------------------------------------------------------------------------------\n",
      "| Config | Last Epoch | Best Epoch | Train Loss |  Val Loss  | Time (seconds) |\n",
      "|     8  |     87     |     47     |  0.842698  |  0.843331  |      12.86s    |\n",
      "|     6  |     59     |     19     |   0.84157  |  0.843677  |      10.73s    |\n",
      "|     7  |     92     |     53     |  0.840768  |  0.843501  |      38.29s    |\n",
      "--- Finished SHA Cross-Validation for SOFTMAX (868.21 seconds) ---\n",
      "Final results for 3 surviving configurations saved to: ./results/Softmax_cv_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>last_epoch</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>train_loss_at_best</th>\n",
       "      <th>best_val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>87</td>\n",
       "      <td>47</td>\n",
       "      <td>0.842698</td>\n",
       "      <td>0.843331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>92</td>\n",
       "      <td>53</td>\n",
       "      <td>0.840768</td>\n",
       "      <td>0.843501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>0.841570</td>\n",
       "      <td>0.843677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  weight_decay  last_epoch  best_epoch  train_loss_at_best  \\\n",
       "0           0.01       0.00100          87          47            0.842698   \n",
       "1           0.01       0.00010          92          53            0.840768   \n",
       "2           0.01       0.00001          59          19            0.841570   \n",
       "\n",
       "   best_val_loss  \n",
       "0       0.843331  \n",
       "1       0.843501  \n",
       "2       0.843677  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_model.cross_validate(dh,\n",
    "                             param_grid=SOFTMAX_PARAM_GRID,\n",
    "                             optimizer_choice=OPTIMIZER_CHOICE,\n",
    "                             rung_schedule=SOFTMAX_SCHEDULE\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21d6558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Model Training for SOFTMAX ---\n",
      "Using best hyperparameters from CV: {'learning_rate': 0.01, 'weight_decay': 0.0001, 'last_epoch': 77.66666666666667, 'best_epoch': 37.666666666666664, 'train_loss_at_best': 0.8412514337149682, 'best_val_loss': 0.8426987974822117}\n",
      "Starting final training for up to 150 epochs (Patience: 50)...\n",
      "  Epoch 10/150 - Loss: 0.843121 (Best Loss: 0.843121, Epochs No Improve: 0)\n",
      "  Epoch 20/150 - Loss: 0.841399 (Best Loss: 0.841399, Epochs No Improve: 0)\n",
      "  Epoch 30/150 - Loss: 0.841244 (Best Loss: 0.840897, Epochs No Improve: 3)\n",
      "  Epoch 40/150 - Loss: 0.840943 (Best Loss: 0.840566, Epochs No Improve: 9)\n",
      "  Epoch 50/150 - Loss: 0.843518 (Best Loss: 0.839810, Epochs No Improve: 1)\n",
      "  Epoch 60/150 - Loss: 0.840637 (Best Loss: 0.839810, Epochs No Improve: 11)\n",
      "  Epoch 70/150 - Loss: 0.841734 (Best Loss: 0.839807, Epochs No Improve: 1)\n",
      "  Epoch 80/150 - Loss: 0.840093 (Best Loss: 0.839807, Epochs No Improve: 11)\n",
      "  Epoch 90/150 - Loss: 0.841632 (Best Loss: 0.839559, Epochs No Improve: 6)\n",
      "  Epoch 100/150 - Loss: 0.839543 (Best Loss: 0.838876, Epochs No Improve: 5)\n",
      "  Epoch 110/150 - Loss: 0.839564 (Best Loss: 0.838798, Epochs No Improve: 8)\n",
      "  Epoch 120/150 - Loss: 0.840540 (Best Loss: 0.838798, Epochs No Improve: 18)\n",
      "  Epoch 130/150 - Loss: 0.839607 (Best Loss: 0.838798, Epochs No Improve: 28)\n",
      "  Epoch 140/150 - Loss: 0.840467 (Best Loss: 0.838798, Epochs No Improve: 38)\n",
      "  Epoch 150/150 - Loss: 0.841163 (Best Loss: 0.838798, Epochs No Improve: 48)\n",
      "Loaded model state from epoch with best loss: 0.838798\n",
      "Saved best model state_dict to: ./models/Softmax_final_state_dict.pth\n",
      "Saved training loss history to: ./results/Softmax_final_training_loss.csv\n",
      "--- Finished Final Model Training for SOFTMAX ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=114, out_features=4, bias=True)\n",
       "  (1): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_model.train_final_model(dh,\n",
    "                                final_train_epochs= FINAL_TRAIN_EPOCHS,\n",
    "                                optimizer_choice=OPTIMIZER_CHOICE\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b94ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Predictions for SOFTMAX on Year 2020 ---\n",
      "County-level raw predictions saved to: ./preds/2020_Softmax_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "softmax_preds = softmax_model.predict(dh, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d19ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15207319, 0.02530907, 0.2590983 , 0.32924837],\n",
       "       [0.15103795, 0.06097275, 0.2786021 , 0.29776868],\n",
       "       [0.20952702, 0.0151517 , 0.16357335, 0.4046892 ],\n",
       "       ...,\n",
       "       [0.11637519, 0.05959414, 0.2536456 , 0.2911093 ],\n",
       "       [0.15302789, 0.04937229, 0.29364637, 0.2851832 ],\n",
       "       [0.08838788, 0.12065135, 0.21881565, 0.3679239 ]],\n",
       "      shape=(3090, 4), dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89032675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
