{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2424506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Imports successful.\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 1: Imports\n",
    "# =============================================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import optuna\n",
    "from pprint import pprint # For pretty printing dicts\n",
    "import copy\n",
    "\n",
    "# Import project modules\n",
    "# It's good practice to reload modules if you make changes during development\n",
    "import importlib\n",
    "import utils\n",
    "import data_handling\n",
    "import core_nn\n",
    "import hpo\n",
    "import model_handlers\n",
    "import metrics\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(data_handling)\n",
    "importlib.reload(core_nn)\n",
    "importlib.reload(hpo)\n",
    "importlib.reload(model_handlers)\n",
    "importlib.reload(metrics)\n",
    "\n",
    "# Import specific classes/functions for convenience\n",
    "from data_handling import DataHandler\n",
    "from hpo import HyperparameterConfig, HyperparameterTuner, ObjectiveNN, ObjectiveXGBoost\n",
    "from model_handlers import RidgeModel, XGBoostModel, NNModel\n",
    "from metrics import evaluate_predictions\n",
    "\n",
    "print(\"Imports successful.\")\n",
    "print(f\"Using device: {utils.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c0d4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants and random seeds set.\n",
      "Data Path: ./data/final_dataset.csv\n",
      "Models Dir: ./models\n",
      "Results Dir: ./results\n",
      "Optuna DB: ./results/hpo_studies.db\n",
      "Test Year: 2020\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Global Constants and Random Seed\n",
    "# =============================================================================\n",
    "\n",
    "# --- Configuration ---\n",
    "SEED = 42\n",
    "TEST_YEAR = 2020 # Year held out for final testing\n",
    "BATCH_SIZE = 128 # Batch size for DataLoaders\n",
    "\n",
    "# --- Paths ---\n",
    "# Assumes notebook is run from the project root directory\n",
    "DATA_DIR = \"./data\"\n",
    "MODELS_DIR = \"./models\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "DATA_CSV_PATH = os.path.join(DATA_DIR, 'final_dataset.csv')\n",
    "OPTUNA_DB_PATH = os.path.join(RESULTS_DIR, \"hpo_studies.db\") # SQLite path for Optuna persistence\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# --- Base Optuna Study Name ---\n",
    "BASE_STUDY_NAME = f\"election_pred_{TEST_YEAR}\"\n",
    "\n",
    "# --- Set Random Seeds ---\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if utils.DEVICE.type == 'cuda':\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED) # if using multi-GPU\n",
    "# Optional: For full reproducibility, disable certain cuDNN algorithms\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Constants and random seeds set.\")\n",
    "print(f\"Data Path: {DATA_CSV_PATH}\")\n",
    "print(f\"Models Dir: {MODELS_DIR}\")\n",
    "print(f\"Results Dir: {RESULTS_DIR}\")\n",
    "print(f\"Optuna DB: {OPTUNA_DB_PATH}\")\n",
    "print(f\"Test Year: {TEST_YEAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e467a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized:\n",
      "  Data path: ./data/final_dataset.csv\n",
      "  Using 115 features.\n",
      "  Test year: 2020\n",
      "  Batch size: 128\n",
      "  Datasets and DataLoaders created for CV and final training.\n",
      "\n",
      "DataHandler and HyperparameterConfig instantiated.\n",
      "Input dimension for models: 115\n",
      "\n",
      "--- Active Search Space for 'XGBoost' ---\n",
      "{\n",
      "  \"eta\": {\n",
      "    \"type\": \"float\",\n",
      "    \"low\": 0.01,\n",
      "    \"high\": 0.3,\n",
      "    \"log\": true\n",
      "  },\n",
      "  \"max_depth\": {\n",
      "    \"type\": \"int\",\n",
      "    \"low\": 3,\n",
      "    \"high\": 10,\n",
      "    \"log\": false\n",
      "  },\n",
      "  \"subsample\": {\n",
      "    \"type\": \"float\",\n",
      "    \"low\": 0.5,\n",
      "    \"high\": 1.0,\n",
      "    \"log\": false\n",
      "  },\n",
      "  \"colsample_bytree\": {\n",
      "    \"type\": \"float\",\n",
      "    \"low\": 0.5,\n",
      "    \"high\": 1.0,\n",
      "    \"log\": false\n",
      "  },\n",
      "  \"gamma\": {\n",
      "    \"type\": \"float\",\n",
      "    \"low\": 0.0,\n",
      "    \"high\": 5.0,\n",
      "    \"log\": false\n",
      "  },\n",
      "  \"lambda\": {\n",
      "    \"type\": \"float\",\n",
      "    \"low\": 0.01,\n",
      "    \"high\": 10.0,\n",
      "    \"log\": true\n",
      "  },\n",
      "  \"alpha\": {\n",
      "    \"type\": \"float\",\n",
      "    \"low\": 0.01,\n",
      "    \"high\": 10.0,\n",
      "    \"log\": true\n",
      "  }\n",
      "}\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Instantiate DataHandler and HyperparameterConfig\n",
    "# =============================================================================\n",
    "\n",
    "# Instantiate DataHandler (loads data, creates splits, scalers, dataloaders)\n",
    "dh = DataHandler(\n",
    "    data_csv_path=DATA_CSV_PATH,\n",
    "    test_year=TEST_YEAR,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Instantiate HyperparameterConfig (holds default search spaces)\n",
    "hp_config = HyperparameterConfig()\n",
    "\n",
    "print(\"\\nDataHandler and HyperparameterConfig instantiated.\")\n",
    "print(f\"Input dimension for models: {dh.input_dim}\")\n",
    "\n",
    "# Display default XGBoost space as an example\n",
    "hp_config.display_space(\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce3038d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Ridge Cross-Validation ---\n",
      "Ridge alpha grid: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]\n",
      "\n",
      "--- Starting Cross-Validation for RIDGE ---\n",
      "  Testing alpha = 0.001:\n",
      " Fold 1 - Train: [2008, 2012] - Val: 2016 - Val Loss (W-MSE): 0.001199\n",
      " Fold 2 - Train: [2008, 2016] - Val: 2012 - Val Loss (W-MSE): 0.001349\n",
      " Fold 3 - Train: [2012, 2016] - Val: 2008 - Val Loss (W-MSE): 0.002165\n",
      "    Avg Val Loss (W-MSE): 0.001571\n",
      "  Testing alpha = 0.005:\n",
      " Fold 1 - Train: [2008, 2012] - Val: 2016 - Val Loss (W-MSE): 0.001178\n",
      " Fold 2 - Train: [2008, 2016] - Val: 2012 - Val Loss (W-MSE): 0.001376\n",
      " Fold 3 - Train: [2012, 2016] - Val: 2008 - Val Loss (W-MSE): 0.002118\n",
      "    Avg Val Loss (W-MSE): 0.001557\n",
      "  Testing alpha = 0.01:\n",
      " Fold 1 - Train: [2008, 2012] - Val: 2016 - Val Loss (W-MSE): 0.001173\n",
      " Fold 2 - Train: [2008, 2016] - Val: 2012 - Val Loss (W-MSE): 0.001375\n",
      " Fold 3 - Train: [2012, 2016] - Val: 2008 - Val Loss (W-MSE): 0.002075\n",
      "    Avg Val Loss (W-MSE): 0.001541\n",
      "  Testing alpha = 0.05:\n",
      " Fold 1 - Train: [2008, 2012] - Val: 2016 - Val Loss (W-MSE): 0.001258\n",
      " Fold 2 - Train: [2008, 2016] - Val: 2012 - Val Loss (W-MSE): 0.001374\n",
      " Fold 3 - Train: [2012, 2016] - Val: 2008 - Val Loss (W-MSE): 0.001975\n",
      "    Avg Val Loss (W-MSE): 0.001536\n",
      "  Testing alpha = 0.1:\n",
      " Fold 1 - Train: [2008, 2012] - Val: 2016 - Val Loss (W-MSE): 0.001369\n",
      " Fold 2 - Train: [2008, 2016] - Val: 2012 - Val Loss (W-MSE): 0.001419\n",
      " Fold 3 - Train: [2012, 2016] - Val: 2008 - Val Loss (W-MSE): 0.001985\n",
      "    Avg Val Loss (W-MSE): 0.001591\n",
      "  Testing alpha = 0.5:\n",
      " Fold 1 - Train: [2008, 2012] - Val: 2016 - Val Loss (W-MSE): 0.001742\n",
      " Fold 2 - Train: [2008, 2016] - Val: 2012 - Val Loss (W-MSE): 0.001678\n",
      " Fold 3 - Train: [2012, 2016] - Val: 2008 - Val Loss (W-MSE): 0.002138\n",
      "    Avg Val Loss (W-MSE): 0.001853\n",
      "  Testing alpha = 1.0:\n",
      " Fold 1 - Train: [2008, 2012] - Val: 2016 - Val Loss (W-MSE): 0.001957\n",
      " Fold 2 - Train: [2008, 2016] - Val: 2012 - Val Loss (W-MSE): 0.001854\n",
      " Fold 3 - Train: [2012, 2016] - Val: 2008 - Val Loss (W-MSE): 0.002234\n",
      "    Avg Val Loss (W-MSE): 0.002015\n",
      "  Testing alpha = 5.0:\n",
      " Fold 1 - Train: [2008, 2012] - Val: 2016 - Val Loss (W-MSE): 0.002813\n",
      " Fold 2 - Train: [2008, 2016] - Val: 2012 - Val Loss (W-MSE): 0.002584\n",
      " Fold 3 - Train: [2012, 2016] - Val: 2008 - Val Loss (W-MSE): 0.002713\n",
      "    Avg Val Loss (W-MSE): 0.002703\n",
      "CV results saved to: ./results/ridge_cv_results.csv\n",
      "\n",
      "Best RIDGE CV alpha: 0.05 (Score: 0.001536)\n",
      "--- Finished Cross-Validation for RIDGE ---\n",
      "\n",
      "--- Finished Ridge Cross-Validation ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Ridge - Cross-Validation (Grid Search)\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Ridge Cross-Validation ---\")\n",
    "\n",
    "# Instantiate Ridge Model handler\n",
    "ridge_model = RidgeModel(model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "\n",
    "# Define the parameter grid for Ridge's alpha\n",
    "# Note: This is passed explicitly now, not a default in the class\n",
    "ridge_param_grid = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]\n",
    "print(f\"Ridge alpha grid: {ridge_param_grid}\")\n",
    "\n",
    "# Run cross-validation (uses internal weighted MSE grid search)\n",
    "ridge_model.cross_validate(dh=dh, param_grid=ridge_param_grid)\n",
    "\n",
    "print(\"\\n--- Finished Ridge Cross-Validation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b59a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Ridge Final Training ---\n",
      "\n",
      "--- Starting Final Model Training for RIDGE ---\n",
      "Using best alpha: 0.05\n",
      "Loaded final training data: 9270 samples.\n",
      "Ridge model fitting complete.\n",
      "Saved final trained Ridge model to: ./models/ridge_final_model.joblib\n",
      "--- Finished Final Model Training for RIDGE ---\n",
      "Trained Ridge Model Alpha: 0.05\n",
      "\n",
      "--- Finished Ridge Final Training ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Ridge - Final Training\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Ridge Final Training ---\")\n",
    "\n",
    "# Instantiate handler again (or reuse if previous cell run in sequence)\n",
    "ridge_model = RidgeModel(model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "\n",
    "# Train the final model using the best alpha found during CV\n",
    "# The handler loads the best alpha from the saved CSV internally\n",
    "trained_ridge = ridge_model.train_final_model(dh=dh)\n",
    "\n",
    "print(f\"Trained Ridge Model Alpha: {ridge_model.best_alpha}\") # Access learned alpha\n",
    "print(\"\\n--- Finished Ridge Final Training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a8c74bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 10:18:37,007] A new study created in RDB with name: election_pred_2020_XGBoost\n",
      "[I 2025-04-28 10:18:37,058] Trial 0 finished with value: inf and parameters: {'eta': 0.02249318799978266, 'max_depth': 9, 'subsample': 0.8576984159558247, 'colsample_bytree': 0.9728885904547244, 'gamma': 1.0006443209069638, 'lambda': 0.012703997493923093, 'alpha': 0.04731262377511004}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,083] Trial 1 finished with value: inf and parameters: {'eta': 0.08565084790293678, 'max_depth': 8, 'subsample': 0.6153183499828112, 'colsample_bytree': 0.6518853012774333, 'gamma': 1.3050969659968175, 'lambda': 0.0130566087654922, 'alpha': 0.1545768802817256}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,111] Trial 2 finished with value: inf and parameters: {'eta': 0.047741144264486904, 'max_depth': 4, 'subsample': 0.8205057484142027, 'colsample_bytree': 0.5724266338306842, 'gamma': 4.951262805631169, 'lambda': 2.824386878746777, 'alpha': 0.013926088316725878}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,133] Trial 3 finished with value: inf and parameters: {'eta': 0.022365429436977138, 'max_depth': 5, 'subsample': 0.5796848270479511, 'colsample_bytree': 0.9773738424564877, 'gamma': 1.3072509521820785, 'lambda': 8.637485345635449, 'alpha': 0.8235158670708033}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting XGBoost Cross-Validation (Optuna) ---\n",
      "XGBoost Search Space:\n",
      "{'alpha': {'high': 10.0, 'log': True, 'low': 0.01, 'type': 'float'},\n",
      " 'colsample_bytree': {'high': 1.0, 'log': False, 'low': 0.5, 'type': 'float'},\n",
      " 'eta': {'high': 0.3, 'log': True, 'low': 0.01, 'type': 'float'},\n",
      " 'gamma': {'high': 5.0, 'log': False, 'low': 0.0, 'type': 'float'},\n",
      " 'lambda': {'high': 10.0, 'log': True, 'low': 0.01, 'type': 'float'},\n",
      " 'max_depth': {'high': 10, 'log': False, 'low': 3, 'type': 'int'},\n",
      " 'subsample': {'high': 1.0, 'log': False, 'low': 0.5, 'type': 'float'}}\n",
      "\n",
      "XGBoost Fixed HPO Params:\n",
      "{'early_stopping_rounds': 25, 'n_estimators_max_pruning': 150}\n",
      "\n",
      "--- Starting Cross-Validation (Optuna) for XGBOOST ---\n",
      "\n",
      "--- Starting Optuna HPO ---\n",
      "Study Name: election_pred_2020_XGBoost, N Trials: 30, Pruner: MedianPruner\n",
      "Direction: minimize, Storage: sqlite:///./results/hpo_studies.db\n",
      "Warning: Trial 0, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 1, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 2, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 3, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 10:18:37,165] Trial 4 finished with value: inf and parameters: {'eta': 0.1741186618612467, 'max_depth': 9, 'subsample': 0.5782986897533768, 'colsample_bytree': 0.6875102682334095, 'gamma': 2.7996485160363687, 'lambda': 0.015968822179734297, 'alpha': 1.7888424561995238}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,194] Trial 5 finished with value: inf and parameters: {'eta': 0.026231885985259933, 'max_depth': 10, 'subsample': 0.669521171298662, 'colsample_bytree': 0.8074807591142663, 'gamma': 4.2817577525107575, 'lambda': 0.012873937985957906, 'alpha': 0.30233349711403756}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,219] Trial 6 finished with value: inf and parameters: {'eta': 0.03699890244669174, 'max_depth': 4, 'subsample': 0.8665765961325884, 'colsample_bytree': 0.8153650738183034, 'gamma': 2.248701949227787, 'lambda': 0.06073132108172165, 'alpha': 0.016388359659486557}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,244] Trial 7 finished with value: inf and parameters: {'eta': 0.2703147697904285, 'max_depth': 6, 'subsample': 0.6049888417794006, 'colsample_bytree': 0.7340164139283472, 'gamma': 0.48830855853586597, 'lambda': 0.06235781072683606, 'alpha': 4.940593676271754}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,274] Trial 8 finished with value: inf and parameters: {'eta': 0.05558631813158132, 'max_depth': 10, 'subsample': 0.7193282125522663, 'colsample_bytree': 0.7813732392582535, 'gamma': 2.6267637927094, 'lambda': 0.4649900222158501, 'alpha': 0.06839821806818724}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,297] Trial 9 finished with value: inf and parameters: {'eta': 0.014366086553494518, 'max_depth': 8, 'subsample': 0.5071936809477442, 'colsample_bytree': 0.9489882791556843, 'gamma': 2.5497526802569292, 'lambda': 0.2984211837886591, 'alpha': 0.2328636818287687}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,337] Trial 10 finished with value: inf and parameters: {'eta': 0.01018393610465748, 'max_depth': 7, 'subsample': 0.9959080849427352, 'colsample_bytree': 0.9031326817465279, 'gamma': 0.284967940408264, 'lambda': 0.3599794968905489, 'alpha': 0.05275951521527379}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Trial 4, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 5, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 6, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 7, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 8, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 9, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 10, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 10:18:37,375] Trial 11 finished with value: inf and parameters: {'eta': 0.09885343236619061, 'max_depth': 8, 'subsample': 0.8265438250205207, 'colsample_bytree': 0.6020653581195021, 'gamma': 1.4075846209693343, 'lambda': 0.04298505336468727, 'alpha': 0.08558763370345739}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,413] Trial 12 finished with value: inf and parameters: {'eta': 0.08053523399655324, 'max_depth': 8, 'subsample': 0.9356582630710055, 'colsample_bytree': 0.5023821213472339, 'gamma': 1.3344441797279611, 'lambda': 0.012680850815048215, 'alpha': 0.12184965642561894}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,448] Trial 13 finished with value: inf and parameters: {'eta': 0.11898116320355681, 'max_depth': 7, 'subsample': 0.7477903504661084, 'colsample_bytree': 0.6609097072681406, 'gamma': 0.7820710188119517, 'lambda': 0.14404612310022144, 'alpha': 0.02906196929587958}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,482] Trial 14 finished with value: inf and parameters: {'eta': 0.020786820651431892, 'max_depth': 9, 'subsample': 0.6639358024237323, 'colsample_bytree': 0.8704274672186544, 'gamma': 1.8436921443333105, 'lambda': 0.02616449066632538, 'alpha': 0.576593223586645}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,518] Trial 15 finished with value: inf and parameters: {'eta': 0.0663570474403356, 'max_depth': 9, 'subsample': 0.8844183350341628, 'colsample_bytree': 0.705605100438863, 'gamma': 3.40726640736475, 'lambda': 0.12558181205947377, 'alpha': 0.17630344971711742}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,556] Trial 16 finished with value: inf and parameters: {'eta': 0.14296600210424087, 'max_depth': 6, 'subsample': 0.7818899176683539, 'colsample_bytree': 0.6179593604967684, 'gamma': 0.06847224639222138, 'lambda': 1.4617667556988723, 'alpha': 0.03348990007449855}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Trial 11, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 12, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 13, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 14, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 15, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 16, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 10:18:37,593] Trial 17 finished with value: inf and parameters: {'eta': 0.03372004670560201, 'max_depth': 10, 'subsample': 0.5204525490459304, 'colsample_bytree': 0.871791291769709, 'gamma': 0.8672221413492733, 'lambda': 0.026270487557524803, 'alpha': 9.607390909241627}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,633] Trial 18 finished with value: inf and parameters: {'eta': 0.01591576884203744, 'max_depth': 3, 'subsample': 0.6990764719567624, 'colsample_bytree': 0.5103385995103238, 'gamma': 1.9774555744987299, 'lambda': 0.10210997677349146, 'alpha': 0.5430677731198936}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,668] Trial 19 finished with value: inf and parameters: {'eta': 0.24792711019383715, 'max_depth': 8, 'subsample': 0.6289621568380674, 'colsample_bytree': 0.7570430763346889, 'gamma': 1.6118728662352992, 'lambda': 0.028712742565711748, 'alpha': 0.03328536574463058}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,703] Trial 20 finished with value: inf and parameters: {'eta': 0.041198134596951236, 'max_depth': 7, 'subsample': 0.9560504021511318, 'colsample_bytree': 0.6411259070545261, 'gamma': 3.299275455393136, 'lambda': 0.010562902295916705, 'alpha': 1.4372892078819823}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Trial 17, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 18, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 19, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 20, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 10:18:37,848] Trial 21 finished with value: inf and parameters: {'eta': 0.053500985952765354, 'max_depth': 3, 'subsample': 0.7957334149534441, 'colsample_bytree': 0.578728542112022, 'gamma': 4.9525864713463905, 'lambda': 4.524742761260774, 'alpha': 0.010170699424689324}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,888] Trial 22 finished with value: inf and parameters: {'eta': 0.0440554238879644, 'max_depth': 5, 'subsample': 0.8468695346367127, 'colsample_bytree': 0.5826731999238747, 'gamma': 4.141057891064908, 'lambda': 1.351674220258137, 'alpha': 0.014587209186012811}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,923] Trial 23 finished with value: inf and parameters: {'eta': 0.08472833657469558, 'max_depth': 5, 'subsample': 0.9083759859145203, 'colsample_bytree': 0.5592122342516892, 'gamma': 0.9721348245663339, 'lambda': 1.0202330407597289, 'alpha': 0.02180307092081524}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,959] Trial 24 finished with value: inf and parameters: {'eta': 0.02889756277134458, 'max_depth': 9, 'subsample': 0.8037264509395441, 'colsample_bytree': 0.5342917931580365, 'gamma': 3.047635266304508, 'lambda': 4.205023631011981, 'alpha': 0.049400740659671705}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:37,999] Trial 25 finished with value: inf and parameters: {'eta': 0.06211478857752626, 'max_depth': 4, 'subsample': 0.7559015078300919, 'colsample_bytree': 0.6642939616228459, 'gamma': 3.8427361675211738, 'lambda': 0.7884345472273845, 'alpha': 0.1633410833453079}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Trial 21, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 22, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 23, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 24, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 25, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 10:18:38,042] Trial 26 finished with value: inf and parameters: {'eta': 0.01633190690052563, 'max_depth': 6, 'subsample': 0.8210744169903788, 'colsample_bytree': 0.6232643778759052, 'gamma': 2.159815320529793, 'lambda': 0.21223365121514104, 'alpha': 0.010123178030444817}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:38,089] Trial 27 finished with value: inf and parameters: {'eta': 0.010289676504867253, 'max_depth': 4, 'subsample': 0.7553684486397759, 'colsample_bytree': 0.7176859276797609, 'gamma': 4.903984418543287, 'lambda': 3.425342010586073, 'alpha': 0.09219370616864524}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:38,125] Trial 28 finished with value: inf and parameters: {'eta': 0.19413589944664883, 'max_depth': 8, 'subsample': 0.8981537314364815, 'colsample_bytree': 0.5607216470651115, 'gamma': 1.074311095577773, 'lambda': 2.291494687124606, 'alpha': 0.04631319034093134}. Best is trial 0 with value: inf.\n",
      "[I 2025-04-28 10:18:38,162] Trial 29 finished with value: inf and parameters: {'eta': 0.021234585381146014, 'max_depth': 7, 'subsample': 0.8551516843560274, 'colsample_bytree': 0.9981969933195061, 'gamma': 1.692486141635233, 'lambda': 6.13496483852347, 'alpha': 0.021580452121676304}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Trial 26, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 27, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 28, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "Warning: Trial 29, Fold 1 failed: XGBModel.fit() got an unexpected keyword argument 'evals'\n",
      "\n",
      "--- Optuna HPO Finished ---\n",
      "Best Trial: 0, Best Value: inf\n",
      "Best Parameters: {\n",
      "  \"eta\": 0.02249318799978266,\n",
      "  \"max_depth\": 9,\n",
      "  \"subsample\": 0.8576984159558247,\n",
      "  \"colsample_bytree\": 0.9728885904547244,\n",
      "  \"gamma\": 1.0006443209069638,\n",
      "  \"lambda\": 0.012703997493923093,\n",
      "  \"alpha\": 0.04731262377511004\n",
      "}\n",
      "------------------------------\n",
      "\n",
      "Best XGBOOST CV params found.\n",
      "Best XGBOOST CV score (weighted_mse): inf\n",
      "Saved best HPO parameters to: ./results/xgboost_best_params.json\n",
      "--- Finished Cross-Validation for XGBOOST ---\n",
      "\n",
      "Best XGBoost HPO Params Found:\n",
      "{'alpha': 0.04731262377511004,\n",
      " 'colsample_bytree': 0.9728885904547244,\n",
      " 'eta': 0.02249318799978266,\n",
      " 'gamma': 1.0006443209069638,\n",
      " 'lambda': 0.012703997493923093,\n",
      " 'max_depth': 9,\n",
      " 'subsample': 0.8576984159558247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-28 10:18:41,330] Trial 0 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,330] Trial 1 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,331] Trial 2 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,331] Trial 3 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,331] Trial 4 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,331] Trial 5 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,331] Trial 6 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,331] Trial 7 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,332] Trial 8 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,332] Trial 9 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,332] Trial 10 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,333] Trial 11 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,333] Trial 12 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,334] Trial 13 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,334] Trial 14 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,334] Trial 15 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,335] Trial 16 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,335] Trial 17 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,335] Trial 18 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,336] Trial 19 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,336] Trial 20 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,336] Trial 21 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,337] Trial 22 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,337] Trial 23 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,337] Trial 24 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,337] Trial 25 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,338] Trial 26 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,338] Trial 27 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,338] Trial 28 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,338] Trial 29 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,339] Study instance does not contain completed trials.\n",
      "[W 2025-04-28 10:18:41,341] Trial 0 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,341] Trial 1 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,341] Trial 2 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,342] Trial 3 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,342] Trial 4 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,342] Trial 5 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,342] Trial 6 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,342] Trial 7 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,343] Trial 8 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,343] Trial 9 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,343] Trial 10 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,343] Trial 11 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,343] Trial 12 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,343] Trial 13 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,344] Trial 14 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,344] Trial 15 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,344] Trial 16 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,345] Trial 17 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,345] Trial 18 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,345] Trial 19 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,345] Trial 20 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,346] Trial 21 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,346] Trial 22 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,347] Trial 23 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,348] Trial 24 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,348] Trial 25 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,348] Trial 26 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,348] Trial 27 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,349] Trial 28 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,349] Trial 29 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:18:41,349] Your study does not have any completed trials.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Optuna plots to: ./results/optuna_plots_xgb\n",
      "  Could not save plot 'optimization_history': \n",
      "Image export using the \"kaleido\" engine requires the kaleido package,\n",
      "which can be installed using pip:\n",
      "    $ pip install -U kaleido\n",
      "\n",
      "  Could not save plot 'param_importances': \n",
      "Image export using the \"kaleido\" engine requires the kaleido package,\n",
      "which can be installed using pip:\n",
      "    $ pip install -U kaleido\n",
      "\n",
      "  Could not save plot 'slice_plot': \n",
      "Image export using the \"kaleido\" engine requires the kaleido package,\n",
      "which can be installed using pip:\n",
      "    $ pip install -U kaleido\n",
      "\n",
      "Finished saving plots.\n",
      "\n",
      "--- Finished XGBoost Cross-Validation (Optuna) ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: XGBoost - Cross-Validation (Optuna)\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting XGBoost Cross-Validation (Optuna) ---\")\n",
    "\n",
    "# --- HPO Configuration ---\n",
    "N_TRIALS_XGB = 30 # Number of Optuna trials (adjust as needed)\n",
    "XGB_STUDY_NAME = f\"{BASE_STUDY_NAME}_XGBoost\"\n",
    "# Use persistent storage (SQLite)\n",
    "OPTUNA_STORAGE = f\"sqlite:///{OPTUNA_DB_PATH}\"\n",
    "\n",
    "# Instantiate XGBoost Model handler\n",
    "xgb_model = XGBoostModel(model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "\n",
    "# Get default search space\n",
    "xgb_search_space = hp_config.get_space(\"XGBoost\")\n",
    "print(\"XGBoost Search Space:\")\n",
    "pprint(xgb_search_space)\n",
    "\n",
    "# Define fixed parameters for the Objective function\n",
    "# These are not tuned by Optuna but needed for training within a trial\n",
    "fixed_params_xgb = {\n",
    "    'n_estimators_max_pruning': 150, # Max estimators during HPO (early stopping determines actual)\n",
    "    'early_stopping_rounds': 25     # Patience for early stopping during HPO\n",
    "}\n",
    "print(\"\\nXGBoost Fixed HPO Params:\")\n",
    "pprint(fixed_params_xgb)\n",
    "\n",
    "# Instantiate XGBoost Objective\n",
    "objective_xgb = ObjectiveXGBoost(\n",
    "    data_handler=dh,\n",
    "    search_space=xgb_search_space,\n",
    "    fixed_params=fixed_params_xgb\n",
    ")\n",
    "\n",
    "# Instantiate Optuna Tuner\n",
    "# Consider using a pruner like MedianPruner or SuccessiveHalvingPruner\n",
    "tuner_xgb = HyperparameterTuner(\n",
    "    study_name=XGB_STUDY_NAME,\n",
    "    storage_path=OPTUNA_STORAGE,\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5) # Example pruner\n",
    ")\n",
    "\n",
    "# Run Optuna cross-validation via the model handler\n",
    "xgb_best_params = xgb_model.cross_validate(\n",
    "    dh=dh,\n",
    "    tuner=tuner_xgb,\n",
    "    objective_xgb=objective_xgb,\n",
    "    n_trials=N_TRIALS_XGB\n",
    ")\n",
    "\n",
    "print(\"\\nBest XGBoost HPO Params Found:\")\n",
    "pprint(xgb_best_params)\n",
    "# Optionally save Optuna plots\n",
    "tuner_xgb.save_plots(results_dir=os.path.join(RESULTS_DIR, \"optuna_plots_xgb\"))\n",
    "\n",
    "print(\"\\n--- Finished XGBoost Cross-Validation (Optuna) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e88b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 7: XGBoost - Final Training\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting XGBoost Final Training ---\")\n",
    "\n",
    "# Instantiate handler again\n",
    "xgb_model = XGBoostModel(model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "\n",
    "# Define parameters for the final fit (e.g., potentially more patience)\n",
    "# n_estimators will be set high, early stopping controls the effective number\n",
    "final_fit_params_xgb = {\n",
    "    'n_estimators': 1500,          # High value, overridden by early stopping\n",
    "    'early_stopping_rounds': 50    # Patience for the final fit\n",
    "}\n",
    "print(\"XGBoost Final Fit Params:\")\n",
    "pprint(final_fit_params_xgb)\n",
    "\n",
    "# Train the final model using best HPO parameters (loaded internally)\n",
    "trained_xgb = xgb_model.train_final_model(\n",
    "    dh=dh,\n",
    "    final_fit_params=final_fit_params_xgb\n",
    ")\n",
    "\n",
    "print(f\"\\nTrained XGBoost Model Best Iteration: {trained_xgb.best_iteration}\")\n",
    "print(\"\\n--- Finished XGBoost Final Training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99b96704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 10:27:58,855] A new study created in RDB with name: election_pred_2020_NN_0Layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting NN (0 Hidden Layers) Cross-Validation (Optuna) ---\n",
      "NN (0 Layers) Search Space:\n",
      "{'activation': {'choices': ['ReLU', 'Tanh'], 'type': 'categorical'},\n",
      " 'dropout_rate': {'high': 0.7, 'log': False, 'low': 0.0, 'type': 'float'},\n",
      " 'learning_rate': {'high': 0.1, 'log': True, 'low': 1e-05, 'type': 'float'},\n",
      " 'optimizer': {'choices': ['AdamW', 'Adam', 'SGD'], 'type': 'categorical'},\n",
      " 'weight_decay': {'high': 0.1, 'log': False, 'low': 0.0, 'type': 'float'}}\n",
      "\n",
      "NN (0 Layers) Fixed HPO Params:\n",
      "{'num_hidden_layers': 0, 'patience': 15, 'pruning_epochs': [10, 25, 50]}\n",
      "\n",
      "--- Starting Cross-Validation (Optuna) for NN0 ---\n",
      "\n",
      "--- Starting Optuna HPO ---\n",
      "Study Name: election_pred_2020_NN_0Layer, N Trials: 30, Pruner: MedianPruner\n",
      "Direction: minimize, Storage: sqlite:///./results/hpo_studies.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/math392/lib/python3.11/site-packages/optuna/trial/_trial.py:497: UserWarning:\n",
      "\n",
      "The reported value is ignored because this `step` 10 is already reported.\n",
      "\n",
      "/opt/anaconda3/envs/math392/lib/python3.11/site-packages/optuna/trial/_trial.py:497: UserWarning:\n",
      "\n",
      "The reported value is ignored because this `step` 25 is already reported.\n",
      "\n",
      "/opt/anaconda3/envs/math392/lib/python3.11/site-packages/optuna/trial/_trial.py:497: UserWarning:\n",
      "\n",
      "The reported value is ignored because this `step` 50 is already reported.\n",
      "\n",
      "[I 2025-04-28 10:28:29,660] Trial 0 finished with value: 0.8453814832369485 and parameters: {'learning_rate': 0.0014398544281227442, 'weight_decay': 0.08324919213729007, 'dropout_rate': 0.5061931414196831, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:28:51,678] Trial 1 finished with value: 0.8514492964744568 and parameters: {'learning_rate': 0.0035303724522368963, 'weight_decay': 0.09375257828202677, 'dropout_rate': 0.6874196584023756, 'optimizer': 'Adam', 'activation': 'Tanh'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:29:08,558] Trial 2 finished with value: 0.8493451563517253 and parameters: {'learning_rate': 0.053656510186576724, 'weight_decay': 0.010424950007951551, 'dropout_rate': 0.43171678707232564, 'optimizer': 'Adam', 'activation': 'Tanh'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:29:37,016] Trial 3 finished with value: 0.9005947534243265 and parameters: {'learning_rate': 0.00012385776796328916, 'weight_decay': 0.06107369865691027, 'dropout_rate': 0.016498128168970814, 'optimizer': 'SGD', 'activation': 'Tanh'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:04,893] Trial 4 finished with value: 0.8527775899569194 and parameters: {'learning_rate': 0.013430343796183412, 'weight_decay': 0.0646985159757367, 'dropout_rate': 0.22227212695135612, 'optimizer': 'SGD', 'activation': 'Tanh'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:06,907] Trial 5 finished with value: inf and parameters: {'learning_rate': 5.619805826689648e-05, 'weight_decay': 0.09003128392426646, 'dropout_rate': 0.3336922672938497, 'optimizer': 'AdamW', 'activation': 'Tanh'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:11,837] Trial 6 finished with value: inf and parameters: {'learning_rate': 0.0030757566478980046, 'weight_decay': 0.007096436438124154, 'dropout_rate': 0.5478850791112791, 'optimizer': 'Adam', 'activation': 'ReLU'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:16,760] Trial 7 finished with value: inf and parameters: {'learning_rate': 0.008845755330075335, 'weight_decay': 0.04940462571707976, 'dropout_rate': 0.10036056523768386, 'optimizer': 'Adam', 'activation': 'Tanh'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:28,591] Trial 8 finished with value: 0.8454666535059611 and parameters: {'learning_rate': 0.03721264603471868, 'weight_decay': 0.02022852461338658, 'dropout_rate': 0.09023415343139962, 'optimizer': 'AdamW', 'activation': 'Tanh'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:30,321] Trial 9 finished with value: inf and parameters: {'learning_rate': 0.0003515798775280119, 'weight_decay': 0.02584741220417496, 'dropout_rate': 0.48647592902406844, 'optimizer': 'SGD', 'activation': 'ReLU'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:32,307] Trial 10 finished with value: inf and parameters: {'learning_rate': 1.567056306975301e-05, 'weight_decay': 0.07497676149497426, 'dropout_rate': 0.6894886610591997, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:34,280] Trial 11 finished with value: inf and parameters: {'learning_rate': 0.09258576367943372, 'weight_decay': 0.03980774229934375, 'dropout_rate': 0.28339206974750597, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:36,325] Trial 12 finished with value: inf and parameters: {'learning_rate': 0.0009674105390175867, 'weight_decay': 0.0321615541050444, 'dropout_rate': 0.11872164476339761, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:38,420] Trial 13 finished with value: inf and parameters: {'learning_rate': 0.025354321757347007, 'weight_decay': 0.017521964043217222, 'dropout_rate': 0.5653073433907913, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:30:43,794] Trial 14 finished with value: inf and parameters: {'learning_rate': 0.0007486698489892335, 'weight_decay': 0.07918414533493266, 'dropout_rate': 0.4058620978733046, 'optimizer': 'AdamW', 'activation': 'Tanh'}. Best is trial 0 with value: 0.8453814832369485.\n",
      "[I 2025-04-28 10:31:13,010] Trial 15 finished with value: 0.8432913740475972 and parameters: {'learning_rate': 0.007639990397085731, 'weight_decay': 0.045359170224969145, 'dropout_rate': 0.17829951430973262, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:31:32,647] Trial 16 finished with value: 0.8465998689333598 and parameters: {'learning_rate': 0.003492595725156936, 'weight_decay': 0.04956991141872923, 'dropout_rate': 0.22094198993329148, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:31:34,825] Trial 17 finished with value: inf and parameters: {'learning_rate': 0.00029648712202719657, 'weight_decay': 0.06272367256796499, 'dropout_rate': 0.21538969667488614, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:31:40,234] Trial 18 finished with value: inf and parameters: {'learning_rate': 0.0018824949742901698, 'weight_decay': 0.08062632236282608, 'dropout_rate': 0.5813072816463758, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:31:42,187] Trial 19 finished with value: inf and parameters: {'learning_rate': 0.00997168512688348, 'weight_decay': 0.040281481932764214, 'dropout_rate': 0.3371589908470064, 'optimizer': 'SGD', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:31:44,348] Trial 20 finished with value: inf and parameters: {'learning_rate': 0.00037291109879622545, 'weight_decay': 0.07096126813820355, 'dropout_rate': 0.42188967700148394, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:31:46,515] Trial 21 finished with value: inf and parameters: {'learning_rate': 0.03209207082410604, 'weight_decay': 0.000590494995396193, 'dropout_rate': 0.09605678496950136, 'optimizer': 'AdamW', 'activation': 'Tanh'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:32:03,300] Trial 22 finished with value: 0.845070276260376 and parameters: {'learning_rate': 0.008380960381067795, 'weight_decay': 0.023239743783290523, 'dropout_rate': 0.020597910840756295, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:32:27,919] Trial 23 finished with value: 0.8441986497243246 and parameters: {'learning_rate': 0.007419306368264129, 'weight_decay': 0.03663094942021964, 'dropout_rate': 0.026504744349377503, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:32:48,931] Trial 24 finished with value: 0.844789800643921 and parameters: {'learning_rate': 0.006906269362543977, 'weight_decay': 0.03605312749741417, 'dropout_rate': 0.024633977005032504, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:32:54,280] Trial 25 finished with value: inf and parameters: {'learning_rate': 0.017168975528835975, 'weight_decay': 0.038382907829227264, 'dropout_rate': 0.16080213022193354, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:32:59,743] Trial 26 finished with value: inf and parameters: {'learning_rate': 0.0053541362012529655, 'weight_decay': 0.03173447032411933, 'dropout_rate': 0.035390427264074396, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:33:17,289] Trial 27 finished with value: 0.8451362617810566 and parameters: {'learning_rate': 0.006024822893637404, 'weight_decay': 0.0553259355281012, 'dropout_rate': 0.04989387696878206, 'optimizer': 'AdamW', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:33:19,482] Trial 28 finished with value: inf and parameters: {'learning_rate': 0.002110416151799634, 'weight_decay': 0.04735083920174317, 'dropout_rate': 0.16022491472015116, 'optimizer': 'Adam', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[I 2025-04-28 10:33:21,424] Trial 29 finished with value: inf and parameters: {'learning_rate': 0.018343114748918924, 'weight_decay': 0.031251422866566986, 'dropout_rate': 0.0025402092640209872, 'optimizer': 'SGD', 'activation': 'ReLU'}. Best is trial 15 with value: 0.8432913740475972.\n",
      "[W 2025-04-28 10:33:21,447] Trial 5 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,448] Trial 6 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,448] Trial 7 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,448] Trial 9 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,448] Trial 10 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,449] Trial 11 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,449] Trial 12 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,449] Trial 13 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,449] Trial 14 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,449] Trial 17 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,453] Trial 18 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,454] Trial 19 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,454] Trial 20 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,454] Trial 21 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,455] Trial 25 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,455] Trial 26 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,457] Trial 28 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,459] Trial 29 is omitted in visualization because its objective value is inf or nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optuna HPO Finished ---\n",
      "Best Trial: 15, Best Value: 0.843291\n",
      "Best Parameters: {\n",
      "  \"learning_rate\": 0.007639990397085731,\n",
      "  \"weight_decay\": 0.045359170224969145,\n",
      "  \"dropout_rate\": 0.17829951430973262,\n",
      "  \"optimizer\": \"AdamW\",\n",
      "  \"activation\": \"ReLU\"\n",
      "}\n",
      "------------------------------\n",
      "\n",
      "Best NN0 CV params found.\n",
      "Best NN0 CV score (Loss): 0.843291\n",
      "Saved best parameters and fixed HPO params to: ./results/nn_best_params.json\n",
      "--- Finished Cross-Validation for NN0 ---\n",
      "\n",
      "Best NN (0 Layers) HPO Params Found:\n",
      "{'activation': 'ReLU',\n",
      " 'dropout_rate': 0.17829951430973262,\n",
      " 'learning_rate': 0.007639990397085731,\n",
      " 'optimizer': 'AdamW',\n",
      " 'weight_decay': 0.045359170224969145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-28 10:33:21,683] Trial 5 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,684] Trial 6 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,684] Trial 7 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,684] Trial 9 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,684] Trial 10 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,684] Trial 11 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,685] Trial 12 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,685] Trial 13 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,685] Trial 14 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,685] Trial 17 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,686] Trial 18 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,686] Trial 19 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,686] Trial 20 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,686] Trial 21 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,687] Trial 25 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,687] Trial 26 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,687] Trial 28 is omitted in visualization because its objective value is inf or nan.\n",
      "[W 2025-04-28 10:33:21,687] Trial 29 is omitted in visualization because its objective value is inf or nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Optuna plots to: ./results/optuna_plots_nn0\n",
      "  Could not save plot 'optimization_history': \n",
      "Image export using the \"kaleido\" engine requires the kaleido package,\n",
      "which can be installed using pip:\n",
      "    $ pip install -U kaleido\n",
      "\n",
      "  Could not save plot 'param_importances': \n",
      "Image export using the \"kaleido\" engine requires the kaleido package,\n",
      "which can be installed using pip:\n",
      "    $ pip install -U kaleido\n",
      "\n",
      "  Could not save plot 'slice_plot': \n",
      "Image export using the \"kaleido\" engine requires the kaleido package,\n",
      "which can be installed using pip:\n",
      "    $ pip install -U kaleido\n",
      "\n",
      "Finished saving plots.\n",
      "\n",
      "--- Finished NN (0 Hidden Layers) Cross-Validation (Optuna) ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: NN (0 Hidden Layers) - Cross-Validation (Optuna)\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting NN (0 Hidden Layers) Cross-Validation (Optuna) ---\")\n",
    "\n",
    "# --- Config ---\n",
    "NUM_HIDDEN_LAYERS_NN0 = 0\n",
    "N_TRIALS_NN0 = 30 # Adjust as needed\n",
    "NN0_STUDY_NAME = f\"{BASE_STUDY_NAME}_NN_0Layer\"\n",
    "OPTUNA_STORAGE = f\"sqlite:///{OPTUNA_DB_PATH}\" # Use same DB, different study name\n",
    "\n",
    "# Instantiate NN Model handler\n",
    "nn0_model = NNModel(input_dim=dh.input_dim, model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "nn0_model.MODEL_NAME = \"NN0\" # Customize name for saving\n",
    "\n",
    "# Get base NN search space and customize for 0 layers\n",
    "nn_search_space_base = hp_config.get_space(\"NN\")\n",
    "nn0_search_space = copy.deepcopy(nn_search_space_base)\n",
    "# Remove layer unit suggestions as they are not needed\n",
    "nn0_search_space.pop(\"n_units_l0\", None)\n",
    "nn0_search_space.pop(\"n_units_l1\", None)\n",
    "# Add other layer units here if they exist in the base config\n",
    "print(\"NN (0 Layers) Search Space:\")\n",
    "pprint(nn0_search_space)\n",
    "\n",
    "# Define fixed parameters for the Objective\n",
    "fixed_params_nn0 = {\n",
    "    'num_hidden_layers': NUM_HIDDEN_LAYERS_NN0, # Crucial: Specify architecture\n",
    "    'pruning_epochs': [10, 25, 50],     # Epochs to check for pruning\n",
    "    'patience': 15                      # Early stopping patience within HPO fold\n",
    "}\n",
    "print(\"\\nNN (0 Layers) Fixed HPO Params:\")\n",
    "pprint(fixed_params_nn0)\n",
    "\n",
    "# Instantiate NN Objective\n",
    "objective_nn0 = ObjectiveNN(\n",
    "    data_handler=dh,\n",
    "    search_space=nn0_search_space,\n",
    "    fixed_params=fixed_params_nn0\n",
    ")\n",
    "\n",
    "# Instantiate Optuna Tuner\n",
    "tuner_nn0 = HyperparameterTuner(\n",
    "    study_name=NN0_STUDY_NAME,\n",
    "    storage_path=OPTUNA_STORAGE,\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "# Run Optuna cross-validation\n",
    "nn0_best_params = nn0_model.cross_validate(\n",
    "    dh=dh,\n",
    "    tuner=tuner_nn0,\n",
    "    objective_nn=objective_nn0,\n",
    "    n_trials=N_TRIALS_NN0\n",
    ")\n",
    "\n",
    "print(\"\\nBest NN (0 Layers) HPO Params Found:\")\n",
    "pprint(nn0_best_params)\n",
    "# Optionally save Optuna plots\n",
    "tuner_nn0.save_plots(results_dir=os.path.join(RESULTS_DIR, \"optuna_plots_nn0\"))\n",
    "\n",
    "print(\"\\n--- Finished NN (0 Hidden Layers) Cross-Validation (Optuna) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cb7ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting NN (0 Hidden Layers) Final Training ---\n",
      "\n",
      "--- Starting Final Model Training for NN0 ---\n",
      "Attempting to load best parameters from: ./results/nn_best_params.json\n",
      "Loaded best_params and fixed_params from file.\n",
      "Using best hyperparameters: {'learning_rate': 0.007639990397085731, 'weight_decay': 0.045359170224969145, 'dropout_rate': 0.17829951430973262, 'optimizer': 'AdamW', 'activation': 'ReLU'}\n",
      "Starting final training for max 200 epochs (patience=40)...\n",
      "  Epoch 10/200 - Train Loss: 0.840892 - Val Loss: 0.865626\n",
      "  Epoch 20/200 - Train Loss: 0.840547 - Val Loss: 0.874310\n",
      "  Epoch 30/200 - Train Loss: 0.839897 - Val Loss: 0.877461\n",
      "  Epoch 40/200 - Train Loss: 0.840297 - Val Loss: 0.869872\n",
      "  Early stopping triggered at epoch 45. Best Val Loss: 0.862305\n",
      "Loaded best model state from epoch 5 (Val Loss: 0.862305).\n",
      "Saving final NN0 model state_dict to: ./models/nn_final_model.pth\n",
      "Model state_dict saved successfully.\n",
      "Saving final training loss history to: ./results/nn_final_training_loss.csv\n",
      "Loss history saved successfully.\n",
      "--- Finished Final Model Training for NN0 ---\n",
      "\n",
      "--- Finished NN (0 Hidden Layers) Final Training ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 9: NN (0 Hidden Layers) - Final Training\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting NN (0 Hidden Layers) Final Training ---\")\n",
    "\n",
    "# Instantiate handler again\n",
    "nn0_model = NNModel(input_dim=dh.input_dim, model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "nn0_model.MODEL_NAME = \"NN0\" # Match name used in CV for loading params\n",
    "\n",
    "# Define final training parameters\n",
    "final_epochs_nn0 = 200 # Adjust as needed\n",
    "final_patience_nn0 = 40 # Adjust as needed\n",
    "\n",
    "# Train the final model using best HPO parameters (loaded internally)\n",
    "trained_nn0 = nn0_model.train_final_model(\n",
    "    dh=dh,\n",
    "    final_epochs=final_epochs_nn0,\n",
    "    final_patience=final_patience_nn0\n",
    ")\n",
    "\n",
    "print(\"\\n--- Finished NN (0 Hidden Layers) Final Training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b0293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: NN (1 Hidden Layer) - Cross-Validation (Optuna)\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting NN (1 Hidden Layer) Cross-Validation (Optuna) ---\")\n",
    "import copy # Ensure copy is imported if running cells independently\n",
    "\n",
    "# --- Config ---\n",
    "NUM_HIDDEN_LAYERS_NN1 = 1\n",
    "N_TRIALS_NN1 = 50 # Adjust as needed\n",
    "NN1_STUDY_NAME = f\"{BASE_STUDY_NAME}_NN_1Layer\"\n",
    "OPTUNA_STORAGE = f\"sqlite:///{OPTUNA_DB_PATH}\"\n",
    "\n",
    "# Instantiate NN Model handler\n",
    "nn1_model = NNModel(input_dim=dh.input_dim, model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "nn1_model.MODEL_NAME = \"NN1\" # Customize name\n",
    "\n",
    "# Get base NN search space and customize for 1 layer\n",
    "nn_search_space_base = hp_config.get_space(\"NN\")\n",
    "nn1_search_space = copy.deepcopy(nn_search_space_base)\n",
    "# Keep l0, remove l1\n",
    "nn1_search_space.pop(\"n_units_l1\", None)\n",
    "# Add checks for other layer units if they exist\n",
    "print(\"NN (1 Layer) Search Space:\")\n",
    "pprint(nn1_search_space)\n",
    "\n",
    "# Define fixed parameters for the Objective\n",
    "fixed_params_nn1 = {\n",
    "    'num_hidden_layers': NUM_HIDDEN_LAYERS_NN1, # Specify architecture\n",
    "    'pruning_epochs': [10, 25, 50],\n",
    "    'patience': 15\n",
    "}\n",
    "print(\"\\nNN (1 Layer) Fixed HPO Params:\")\n",
    "pprint(fixed_params_nn1)\n",
    "\n",
    "# Instantiate NN Objective\n",
    "objective_nn1 = ObjectiveNN(\n",
    "    data_handler=dh,\n",
    "    search_space=nn1_search_space,\n",
    "    fixed_params=fixed_params_nn1\n",
    ")\n",
    "\n",
    "# Instantiate Optuna Tuner\n",
    "tuner_nn1 = HyperparameterTuner(\n",
    "    study_name=NN1_STUDY_NAME,\n",
    "    storage_path=OPTUNA_STORAGE,\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "# Run Optuna cross-validation\n",
    "nn1_best_params = nn1_model.cross_validate(\n",
    "    dh=dh,\n",
    "    tuner=tuner_nn1,\n",
    "    objective_nn=objective_nn1,\n",
    "    n_trials=N_TRIALS_NN1\n",
    ")\n",
    "\n",
    "print(\"\\nBest NN (1 Layer) HPO Params Found:\")\n",
    "pprint(nn1_best_params)\n",
    "tuner_nn1.save_plots(results_dir=os.path.join(RESULTS_DIR, \"optuna_plots_nn1\"))\n",
    "\n",
    "print(\"\\n--- Finished NN (1 Hidden Layer) Cross-Validation (Optuna) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eecff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: NN (1 Hidden Layer) - Final Training\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting NN (1 Hidden Layer) Final Training ---\")\n",
    "\n",
    "# Instantiate handler again\n",
    "nn1_model = NNModel(input_dim=dh.input_dim, model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "nn1_model.MODEL_NAME = \"NN1\" # Match name\n",
    "\n",
    "# Define final training parameters\n",
    "final_epochs_nn1 = 200\n",
    "final_patience_nn1 = 40\n",
    "\n",
    "# Train the final model\n",
    "trained_nn1 = nn1_model.train_final_model(\n",
    "    dh=dh,\n",
    "    final_epochs=final_epochs_nn1,\n",
    "    final_patience=final_patience_nn1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Finished NN (1 Hidden Layer) Final Training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34974e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 12: NN (2 Hidden Layers) - Cross-Validation (Optuna)\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting NN (2 Hidden Layers) Cross-Validation (Optuna) ---\")\n",
    "import copy # Ensure copy is imported\n",
    "\n",
    "# --- Config ---\n",
    "NUM_HIDDEN_LAYERS_NN2 = 2\n",
    "N_TRIALS_NN2 = 50 # Adjust\n",
    "NN2_STUDY_NAME = f\"{BASE_STUDY_NAME}_NN_2Layer\"\n",
    "OPTUNA_STORAGE = f\"sqlite:///{OPTUNA_DB_PATH}\"\n",
    "\n",
    "# Instantiate NN Model handler\n",
    "nn2_model = NNModel(input_dim=dh.input_dim, model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "nn2_model.MODEL_NAME = \"NN2\"\n",
    "\n",
    "# Get base NN search space (assume it already includes l0, l1)\n",
    "nn2_search_space = copy.deepcopy(hp_config.get_space(\"NN\"))\n",
    "# Remove suggestions for layers > 1 if they exist in base config\n",
    "# nn2_search_space.pop(\"n_units_l2\", None) ... etc\n",
    "print(\"NN (2 Layers) Search Space:\")\n",
    "pprint(nn2_search_space)\n",
    "\n",
    "# Define fixed parameters\n",
    "fixed_params_nn2 = {\n",
    "    'num_hidden_layers': NUM_HIDDEN_LAYERS_NN2, # Specify architecture\n",
    "    'pruning_epochs': [10, 25, 50],\n",
    "    'patience': 15\n",
    "}\n",
    "print(\"\\nNN (2 Layers) Fixed HPO Params:\")\n",
    "pprint(fixed_params_nn2)\n",
    "\n",
    "# Instantiate NN Objective\n",
    "objective_nn2 = ObjectiveNN(\n",
    "    data_handler=dh,\n",
    "    search_space=nn2_search_space,\n",
    "    fixed_params=fixed_params_nn2\n",
    ")\n",
    "\n",
    "# Instantiate Optuna Tuner\n",
    "tuner_nn2 = HyperparameterTuner(\n",
    "    study_name=NN2_STUDY_NAME,\n",
    "    storage_path=OPTUNA_STORAGE,\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "# Run Optuna cross-validation\n",
    "nn2_best_params = nn2_model.cross_validate(\n",
    "    dh=dh,\n",
    "    tuner=tuner_nn2,\n",
    "    objective_nn=objective_nn2,\n",
    "    n_trials=N_TRIALS_NN2\n",
    ")\n",
    "\n",
    "print(\"\\nBest NN (2 Layers) HPO Params Found:\")\n",
    "pprint(nn2_best_params)\n",
    "tuner_nn2.save_plots(results_dir=os.path.join(RESULTS_DIR, \"optuna_plots_nn2\"))\n",
    "\n",
    "print(\"\\n--- Finished NN (2 Hidden Layers) Cross-Validation (Optuna) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 13: NN (2 Hidden Layers) - Final Training\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting NN (2 Hidden Layers) Final Training ---\")\n",
    "\n",
    "# Instantiate handler again\n",
    "nn2_model = NNModel(input_dim=dh.input_dim, model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "nn2_model.MODEL_NAME = \"NN2\" # Match name\n",
    "\n",
    "# Define final training parameters\n",
    "final_epochs_nn2 = 200\n",
    "final_patience_nn2 = 40\n",
    "\n",
    "# Train the final model\n",
    "trained_nn2 = nn2_model.train_final_model(\n",
    "    dh=dh,\n",
    "    final_epochs=final_epochs_nn2,\n",
    "    final_patience=final_patience_nn2\n",
    ")\n",
    "\n",
    "print(\"\\n--- Finished NN (2 Hidden Layers) Final Training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ca5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 14: NN (3 Hidden Layers) - Cross-Validation (Optuna)\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting NN (3 Hidden Layers) Cross-Validation (Optuna) ---\")\n",
    "import copy # Ensure copy is imported\n",
    "\n",
    "# --- Config ---\n",
    "NUM_HIDDEN_LAYERS_NN3 = 3\n",
    "N_TRIALS_NN3 = 50 # Adjust\n",
    "NN3_STUDY_NAME = f\"{BASE_STUDY_NAME}_NN_3Layer\"\n",
    "OPTUNA_STORAGE = f\"sqlite:///{OPTUNA_DB_PATH}\"\n",
    "\n",
    "# Instantiate NN Model handler\n",
    "nn3_model = NNModel(input_dim=dh.input_dim, model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "nn3_model.MODEL_NAME = \"NN3\"\n",
    "\n",
    "# --- IMPORTANT: Update Search Space ---\n",
    "# The default space only defines l0, l1. Add l2.\n",
    "nn_search_space_base = hp_config.get_space(\"NN\")\n",
    "nn3_search_space = copy.deepcopy(nn_search_space_base)\n",
    "# Add definition for the 3rd layer (index 2)\n",
    "nn3_search_space[\"n_units_l2\"] = {\"type\": \"int\", \"low\": 8, \"high\": 128, \"log\": True}\n",
    "# Remove suggestions for layers > 2 if they exist in base config\n",
    "# nn3_search_space.pop(\"n_units_l3\", None) ... etc\n",
    "print(\"NN (3 Layers) Search Space (Added l2):\")\n",
    "pprint(nn3_search_space)\n",
    "\n",
    "# Define fixed parameters\n",
    "fixed_params_nn3 = {\n",
    "    'num_hidden_layers': NUM_HIDDEN_LAYERS_NN3, # Specify architecture\n",
    "    'pruning_epochs': [10, 25, 50],\n",
    "    'patience': 15\n",
    "}\n",
    "print(\"\\nNN (3 Layers) Fixed HPO Params:\")\n",
    "pprint(fixed_params_nn3)\n",
    "\n",
    "# Instantiate NN Objective\n",
    "objective_nn3 = ObjectiveNN(\n",
    "    data_handler=dh,\n",
    "    search_space=nn3_search_space,\n",
    "    fixed_params=fixed_params_nn3\n",
    ")\n",
    "\n",
    "# Instantiate Optuna Tuner\n",
    "tuner_nn3 = HyperparameterTuner(\n",
    "    study_name=NN3_STUDY_NAME,\n",
    "    storage_path=OPTUNA_STORAGE,\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "# Run Optuna cross-validation\n",
    "nn3_best_params = nn3_model.cross_validate(\n",
    "    dh=dh,\n",
    "    tuner=tuner_nn3,\n",
    "    objective_nn=objective_nn3,\n",
    "    n_trials=N_TRIALS_NN3\n",
    ")\n",
    "\n",
    "print(\"\\nBest NN (3 Layers) HPO Params Found:\")\n",
    "pprint(nn3_best_params)\n",
    "tuner_nn3.save_plots(results_dir=os.path.join(RESULTS_DIR, \"optuna_plots_nn3\"))\n",
    "\n",
    "print(\"\\n--- Finished NN (3 Hidden Layers) Cross-Validation (Optuna) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 15: NN (3 Hidden Layers) - Final Training\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting NN (3 Hidden Layers) Final Training ---\")\n",
    "\n",
    "# Instantiate handler again\n",
    "nn3_model = NNModel(input_dim=dh.input_dim, model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "nn3_model.MODEL_NAME = \"NN3\" # Match name\n",
    "\n",
    "# Define final training parameters\n",
    "final_epochs_nn3 = 200\n",
    "final_patience_nn3 = 40\n",
    "\n",
    "# Train the final model\n",
    "trained_nn3 = nn3_model.train_final_model(\n",
    "    dh=dh,\n",
    "    final_epochs=final_epochs_nn3,\n",
    "    final_patience=final_patience_nn3\n",
    ")\n",
    "\n",
    "print(\"\\n--- Finished NN (3 Hidden Layers) Final Training ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 16: Load Trained Models\n",
    "# =============================================================================\n",
    "print(\"\\n--- Loading Trained Models ---\")\n",
    "\n",
    "# Store loaded model handlers in a dictionary for easier access\n",
    "loaded_models = {}\n",
    "\n",
    "# --- Load Ridge ---\n",
    "try:\n",
    "    ridge_loader = RidgeModel(model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "    ridge_loader.load_model()\n",
    "    loaded_models['Ridge'] = ridge_loader\n",
    "    print(\"Ridge model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Ridge model: {e}\")\n",
    "\n",
    "# --- Load XGBoost ---\n",
    "try:\n",
    "    xgb_loader = XGBoostModel(model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "    # Best params are loaded automatically by train_final_model if needed,\n",
    "    # but load_model just loads the saved XGB artifact.\n",
    "    # Ensure best params file exists if predict needs scaling factors based on HPO settings later.\n",
    "    # Currently predict for XGBoost does not need HPO params.\n",
    "    xgb_loader.load_model()\n",
    "    loaded_models['XGBoost'] = xgb_loader\n",
    "    print(\"XGBoost model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading XGBoost model: {e}\")\n",
    "\n",
    "# --- Load NN Models ---\n",
    "nn_model_names = [\"NN0\", \"NN1\", \"NN2\", \"NN3\"]\n",
    "for name in nn_model_names:\n",
    "    try:\n",
    "        print(f\"Loading {name} model...\")\n",
    "        # Need input_dim from DataHandler\n",
    "        nn_loader = NNModel(input_dim=dh.input_dim, model_dir=MODELS_DIR, results_dir=RESULTS_DIR)\n",
    "        # IMPORTANT: Set the correct MODEL_NAME before loading so it finds the right files\n",
    "        nn_loader.MODEL_NAME = name\n",
    "        # This will trigger internal loading of best params to reconstruct architecture\n",
    "        nn_loader.load_model()\n",
    "        loaded_models[name] = nn_loader\n",
    "        print(f\"{name} model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {name} model: {e}\")\n",
    "\n",
    "print(\"\\n--- Finished Loading Models ---\")\n",
    "# Display loaded model keys\n",
    "print(f\"Models loaded: {list(loaded_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 17: Generate Predictions\n",
    "# =============================================================================\n",
    "print(\"\\n--- Generating Predictions for All Models ---\")\n",
    "\n",
    "# Dictionary to store prediction DataFrames\n",
    "predictions = {}\n",
    "SAVE_INDIVIDUAL_PREDS = True # Set to True to save county-level predictions\n",
    "\n",
    "# Iterate through the loaded model handlers\n",
    "for model_name, model_handler in loaded_models.items():\n",
    "    print(f\"Generating predictions for {model_name}...\")\n",
    "    try:\n",
    "        # Pass dh and optionally results_dir if saving\n",
    "        preds_df = model_handler.predict(dh, save=SAVE_INDIVIDUAL_PREDS, results_dir=RESULTS_DIR)\n",
    "        predictions[model_name] = preds_df\n",
    "        print(f\"  Predictions generated for {model_name} ({preds_df.shape[0]} counties).\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error generating predictions for {model_name}: {e}\")\n",
    "\n",
    "print(\"\\n--- Finished Generating Predictions ---\")\n",
    "# Display keys of generated predictions\n",
    "print(f\"Predictions available for: {list(predictions.keys())}\")\n",
    "# Optional: Display head of one prediction DataFrame\n",
    "# if 'Ridge' in predictions:\n",
    "#     print(\"\\nRidge Predictions Head:\")\n",
    "#     print(predictions['Ridge'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe34616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 18: Collect Predictions into Dictionary\n",
    "# =============================================================================\n",
    "# The `predictions` dictionary created in the previous cell already serves\n",
    "# as the `pred_dict` needed for the evaluation function.\n",
    "\n",
    "# We just assign it to the expected variable name for clarity,\n",
    "# or directly use the `predictions` dictionary.\n",
    "pred_dict = predictions\n",
    "\n",
    "print(\"\\n--- Prediction Dictionary Prepared ---\")\n",
    "print(f\"Models in prediction dictionary: {list(pred_dict.keys())}\")\n",
    "\n",
    "# Verify structure (optional)\n",
    "# if pred_dict:\n",
    "#     first_key = list(pred_dict.keys())[0]\n",
    "#     print(f\"\\nExample DataFrame shape for '{first_key}': {pred_dict[first_key].shape}\")\n",
    "#     print(f\"Columns: {pred_dict[first_key].columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 19: Evaluate All Models\n",
    "# =============================================================================\n",
    "print(\"\\n--- Evaluating All Model Predictions ---\")\n",
    "\n",
    "# Define path for the final aggregate evaluation CSV\n",
    "EVALUATION_SAVE_PATH = os.path.join(RESULTS_DIR, f\"aggregate_evaluation_{TEST_YEAR}.csv\")\n",
    "\n",
    "# Check if pred_dict is populated\n",
    "if not pred_dict:\n",
    "    print(\"Prediction dictionary is empty. Cannot evaluate. Please run prediction cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Call the evaluation function from the metrics module\n",
    "        evaluation_df = metrics.evaluate_predictions(\n",
    "            pred_dict=pred_dict,\n",
    "            dh=dh,\n",
    "            save_path=EVALUATION_SAVE_PATH # Pass the full path\n",
    "        )\n",
    "\n",
    "        print(\"\\nAggregate Evaluation Results:\")\n",
    "        # Display the full dataframe nicely\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "            print(evaluation_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "\n",
    "print(\"\\n--- Finished Evaluation ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
