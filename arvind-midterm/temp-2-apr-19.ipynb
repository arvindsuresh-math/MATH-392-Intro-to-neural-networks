{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf25ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import election_project as ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb01f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized:\n",
      "  Using 115 features\n",
      "  Test year: 2020\n",
      "  Dataloaders created for cross-validation and final training.\n",
      "Attributes of DataHandler:\n",
      "cv_data\n",
      "cv_dataloaders\n",
      "features\n",
      "final_data\n",
      "final_dataloaders\n",
      "folds\n",
      "input_dim\n",
      "test_year\n",
      "train_years\n"
     ]
    }
   ],
   "source": [
    "dh = ep.DataHandler()\n",
    "\n",
    "# lets check all the attributes of the DataHandler\n",
    "print(\"Attributes of DataHandler:\")\n",
    "for attr in dir(dh):\n",
    "    if not attr.startswith('_'):\n",
    "        print(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70894e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = ep.SoftmaxModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8081bff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MODEL_NAME',\n",
       " 'PARAM_GRID',\n",
       " '_SoftmaxNet',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_build_network',\n",
       " '_train_one_fold',\n",
       " 'best_params',\n",
       " 'cross_validate',\n",
       " 'evaluate',\n",
       " 'final_loss_history',\n",
       " 'load_model',\n",
       " 'loss_save_path',\n",
       " 'model',\n",
       " 'results_save_path',\n",
       " 'state_dict_save_path',\n",
       " 'train_final_model',\n",
       " 'weighted_cross_entropy_loss']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58065a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Cross-Validation for SOFTMAX ---\n",
      "  Testing config 1: {'learning_rate': 0.01, 'weight_decay': 0}\n",
      "      Epoch 25/100 -> Train Loss: 1.122014, Val Loss: 1.166772\n",
      "      Epoch 50/100 -> Train Loss: 1.102070, Val Loss: 1.171380\n",
      "    Early stopping at epoch 57. Best val loss 1.147134 at epoch 37.\n",
      "      Epoch 25/100 -> Train Loss: 1.134175, Val Loss: 1.106344\n",
      "    Early stopping at epoch 34. Best val loss 1.083682 at epoch 14.\n",
      "      Epoch 25/100 -> Train Loss: 1.158610, Val Loss: 1.128042\n",
      "      Epoch 50/100 -> Train Loss: 1.166307, Val Loss: 1.117999\n",
      "    Early stopping at epoch 55. Best val loss 1.105568 at epoch 35.\n",
      "    Avg Val Score (Weighted CE): 1.112128\n",
      "  Testing config 2: {'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
      "    Early stopping at epoch 24. Best val loss 1.150952 at epoch 4.\n",
      "      Epoch 25/100 -> Train Loss: 1.138649, Val Loss: 1.092279\n",
      "      Epoch 50/100 -> Train Loss: 1.119999, Val Loss: 1.107140\n",
      "    Early stopping at epoch 64. Best val loss 1.085428 at epoch 44.\n",
      "      Epoch 25/100 -> Train Loss: 1.132171, Val Loss: 1.115400\n",
      "    Early stopping at epoch 31. Best val loss 1.104776 at epoch 11.\n",
      "    Avg Val Score (Weighted CE): 1.113719\n",
      "  Testing config 3: {'learning_rate': 0.01, 'weight_decay': 0.001}\n",
      "      Epoch 25/100 -> Train Loss: 1.098314, Val Loss: 1.168743\n",
      "    Early stopping at epoch 32. Best val loss 1.146120 at epoch 12.\n",
      "      Epoch 25/100 -> Train Loss: 1.137234, Val Loss: 1.113963\n",
      "    Early stopping at epoch 33. Best val loss 1.089015 at epoch 13.\n",
      "      Epoch 25/100 -> Train Loss: 1.157234, Val Loss: 1.172983\n",
      "    Early stopping at epoch 33. Best val loss 1.106732 at epoch 13.\n",
      "    Avg Val Score (Weighted CE): 1.113956\n",
      "  Testing config 4: {'learning_rate': 0.001, 'weight_decay': 0}\n",
      "      Epoch 25/100 -> Train Loss: 1.086335, Val Loss: 1.141900\n",
      "      Epoch 50/100 -> Train Loss: 1.078614, Val Loss: 1.140220\n",
      "    Early stopping at epoch 53. Best val loss 1.139295 at epoch 33.\n",
      "      Epoch 25/100 -> Train Loss: 1.110070, Val Loss: 1.093312\n",
      "      Epoch 50/100 -> Train Loss: 1.105404, Val Loss: 1.086506\n",
      "      Epoch 75/100 -> Train Loss: 1.103880, Val Loss: 1.083464\n",
      "    Early stopping at epoch 90. Best val loss 1.080833 at epoch 70.\n",
      "      Epoch 25/100 -> Train Loss: 1.103268, Val Loss: 1.118328\n",
      "      Epoch 50/100 -> Train Loss: 1.100732, Val Loss: 1.108076\n",
      "      Epoch 75/100 -> Train Loss: 1.099428, Val Loss: 1.105850\n",
      "    Early stopping at epoch 89. Best val loss 1.100171 at epoch 69.\n",
      "    Avg Val Score (Weighted CE): 1.106766\n",
      "  Testing config 5: {'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
      "      Epoch 25/100 -> Train Loss: 1.085430, Val Loss: 1.142256\n",
      "      Epoch 50/100 -> Train Loss: 1.078935, Val Loss: 1.142191\n",
      "    Early stopping at epoch 62. Best val loss 1.139498 at epoch 42.\n",
      "      Epoch 25/100 -> Train Loss: 1.109142, Val Loss: 1.094684\n",
      "      Epoch 50/100 -> Train Loss: 1.105556, Val Loss: 1.083780\n",
      "      Epoch 75/100 -> Train Loss: 1.104688, Val Loss: 1.086784\n",
      "    Early stopping at epoch 98. Best val loss 1.080751 at epoch 78.\n",
      "      Epoch 25/100 -> Train Loss: 1.105526, Val Loss: 1.116741\n",
      "      Epoch 50/100 -> Train Loss: 1.097941, Val Loss: 1.106112\n",
      "      Epoch 75/100 -> Train Loss: 1.098092, Val Loss: 1.108546\n",
      "    Early stopping at epoch 76. Best val loss 1.102754 at epoch 56.\n",
      "    Avg Val Score (Weighted CE): 1.107668\n",
      "  Testing config 6: {'learning_rate': 0.001, 'weight_decay': 0.001}\n",
      "      Epoch 25/100 -> Train Loss: 1.085051, Val Loss: 1.144539\n",
      "      Epoch 50/100 -> Train Loss: 1.080949, Val Loss: 1.141287\n",
      "      Epoch 75/100 -> Train Loss: 1.079020, Val Loss: 1.144753\n",
      "    Early stopping at epoch 82. Best val loss 1.139515 at epoch 62.\n",
      "      Epoch 25/100 -> Train Loss: 1.109917, Val Loss: 1.093303\n",
      "      Epoch 50/100 -> Train Loss: 1.105186, Val Loss: 1.085069\n",
      "      Epoch 75/100 -> Train Loss: 1.102950, Val Loss: 1.085669\n",
      "      Epoch 100/100 -> Train Loss: 1.104029, Val Loss: 1.083489\n",
      "    Reached max_epochs (100). Best val loss 1.081169 at epoch 87.\n",
      "      Epoch 25/100 -> Train Loss: 1.104650, Val Loss: 1.123434\n",
      "      Epoch 50/100 -> Train Loss: 1.099968, Val Loss: 1.112416\n",
      "      Epoch 75/100 -> Train Loss: 1.098394, Val Loss: 1.113624\n",
      "      Epoch 100/100 -> Train Loss: 1.098499, Val Loss: 1.110064\n",
      "    Reached max_epochs (100). Best val loss 1.103888 at epoch 88.\n",
      "    Avg Val Score (Weighted CE): 1.108191\n",
      "  Testing config 7: {'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "      Epoch 25/100 -> Train Loss: 1.153467, Val Loss: 1.199314\n",
      "      Epoch 50/100 -> Train Loss: 1.123302, Val Loss: 1.173126\n",
      "      Epoch 75/100 -> Train Loss: 1.108515, Val Loss: 1.160535\n",
      "      Epoch 100/100 -> Train Loss: 1.095816, Val Loss: 1.152230\n",
      "    Reached max_epochs (100). Best val loss 1.152078 at epoch 99.\n",
      "      Epoch 25/100 -> Train Loss: 1.169332, Val Loss: 1.173154\n",
      "      Epoch 50/100 -> Train Loss: 1.141861, Val Loss: 1.139500\n",
      "      Epoch 75/100 -> Train Loss: 1.127308, Val Loss: 1.119387\n",
      "      Epoch 100/100 -> Train Loss: 1.117098, Val Loss: 1.106288\n",
      "    Reached max_epochs (100). Best val loss 1.106288 at epoch 100.\n",
      "      Epoch 25/100 -> Train Loss: 1.166766, Val Loss: 1.212085\n",
      "      Epoch 50/100 -> Train Loss: 1.134705, Val Loss: 1.175409\n",
      "      Epoch 75/100 -> Train Loss: 1.120281, Val Loss: 1.150362\n",
      "      Epoch 100/100 -> Train Loss: 1.112297, Val Loss: 1.135823\n",
      "    Reached max_epochs (100). Best val loss 1.135617 at epoch 98.\n",
      "    Avg Val Score (Weighted CE): 1.131328\n",
      "  Testing config 8: {'learning_rate': 0.0001, 'weight_decay': 1e-05}\n",
      "      Epoch 25/100 -> Train Loss: 1.147919, Val Loss: 1.189352\n",
      "      Epoch 50/100 -> Train Loss: 1.121031, Val Loss: 1.167235\n",
      "      Epoch 75/100 -> Train Loss: 1.104848, Val Loss: 1.156370\n",
      "      Epoch 100/100 -> Train Loss: 1.095257, Val Loss: 1.149708\n",
      "    Reached max_epochs (100). Best val loss 1.149708 at epoch 100.\n",
      "      Epoch 25/100 -> Train Loss: 1.175350, Val Loss: 1.183222\n",
      "      Epoch 50/100 -> Train Loss: 1.143901, Val Loss: 1.148132\n",
      "      Epoch 75/100 -> Train Loss: 1.127918, Val Loss: 1.125386\n",
      "      Epoch 100/100 -> Train Loss: 1.118409, Val Loss: 1.111937\n",
      "    Reached max_epochs (100). Best val loss 1.111694 at epoch 99.\n",
      "      Epoch 25/100 -> Train Loss: 1.164698, Val Loss: 1.214291\n",
      "      Epoch 50/100 -> Train Loss: 1.136723, Val Loss: 1.179293\n",
      "      Epoch 75/100 -> Train Loss: 1.121453, Val Loss: 1.153796\n",
      "      Epoch 100/100 -> Train Loss: 1.113476, Val Loss: 1.136982\n",
      "    Reached max_epochs (100). Best val loss 1.136982 at epoch 100.\n",
      "    Avg Val Score (Weighted CE): 1.132795\n",
      "  Testing config 9: {'learning_rate': 0.0001, 'weight_decay': 0.001}\n",
      "      Epoch 25/100 -> Train Loss: 1.167439, Val Loss: 1.216791\n",
      "      Epoch 50/100 -> Train Loss: 1.131543, Val Loss: 1.180499\n",
      "      Epoch 75/100 -> Train Loss: 1.112438, Val Loss: 1.163454\n",
      "      Epoch 100/100 -> Train Loss: 1.098995, Val Loss: 1.154007\n",
      "    Reached max_epochs (100). Best val loss 1.154007 at epoch 100.\n",
      "      Epoch 25/100 -> Train Loss: 1.176333, Val Loss: 1.180988\n",
      "      Epoch 50/100 -> Train Loss: 1.144666, Val Loss: 1.146290\n",
      "      Epoch 75/100 -> Train Loss: 1.129885, Val Loss: 1.126049\n",
      "      Epoch 100/100 -> Train Loss: 1.120143, Val Loss: 1.111263\n",
      "    Reached max_epochs (100). Best val loss 1.111077 at epoch 99.\n",
      "      Epoch 25/100 -> Train Loss: 1.163600, Val Loss: 1.213660\n",
      "      Epoch 50/100 -> Train Loss: 1.136617, Val Loss: 1.176855\n",
      "      Epoch 75/100 -> Train Loss: 1.120386, Val Loss: 1.152342\n",
      "      Epoch 100/100 -> Train Loss: 1.112865, Val Loss: 1.136359\n",
      "    Reached max_epochs (100). Best val loss 1.136359 at epoch 100.\n",
      "    Avg Val Score (Weighted CE): 1.133814\n",
      "CV results saved to: ./results/softmax_cv_results.csv\n",
      "\n",
      "Best SOFTMAX CV params: {'learning_rate': 0.001, 'weight_decay': 0} (Score: 1.106766)\n",
      "--- Finished Cross-Validation for SOFTMAX ---\n"
     ]
    }
   ],
   "source": [
    "softmax.cross_validate(dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01fc3ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001, 'weight_decay': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "035b93ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.model == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c42ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Model Training for SOFTMAX ---\n",
      "Using best hyperparameters from CV: {'learning_rate': 0.001, 'weight_decay': 0}\n",
      "Starting training for 300 epochs...\n",
      "  Epoch 10/300 - Loss: 1.111147 (best 1.111147, no_improve=0)\n",
      "  Epoch 20/300 - Loss: 1.099595 (best 1.098883, no_improve=1)\n",
      "  Epoch 30/300 - Loss: 1.095612 (best 1.094998, no_improve=2)\n",
      "  Epoch 40/300 - Loss: 1.096891 (best 1.094251, no_improve=2)\n",
      "  Epoch 50/300 - Loss: 1.094237 (best 1.093531, no_improve=4)\n",
      "  Epoch 60/300 - Loss: 1.094228 (best 1.093531, no_improve=14)\n",
      "  Epoch 70/300 - Loss: 1.094621 (best 1.093531, no_improve=24)\n",
      "  Epoch 80/300 - Loss: 1.093691 (best 1.093387, no_improve=6)\n",
      "  Epoch 90/300 - Loss: 1.095068 (best 1.093387, no_improve=16)\n",
      "  Epoch 100/300 - Loss: 1.096174 (best 1.093387, no_improve=26)\n",
      "  Epoch 110/300 - Loss: 1.093775 (best 1.092672, no_improve=6)\n",
      "  Epoch 120/300 - Loss: 1.094388 (best 1.092672, no_improve=16)\n",
      "  Epoch 130/300 - Loss: 1.095911 (best 1.092672, no_improve=26)\n",
      "  Epoch 140/300 - Loss: 1.095010 (best 1.092672, no_improve=36)\n",
      "  Epoch 150/300 - Loss: 1.095318 (best 1.092672, no_improve=46)\n",
      "  Early stopping at epoch 154 (best loss 1.092672)\n",
      "Saved best model state_dict to: ./models/softmax_final_state_dict.pth\n",
      "Saved training loss history to: ./results/softmax_final_training_loss.csv\n",
      "--- Finished Final Model Training for SOFTMAX ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_SoftmaxNet(\n",
       "  (linear): Linear(in_features=115, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.train_final_model(dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "086f7ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_SoftmaxNet(\n",
       "  (linear): Linear(in_features=115, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b2781f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 1.0262e-01,  3.2159e-02, -4.3917e-02,  7.1066e-03, -3.3304e-02,\n",
       "                        5.8377e-03,  1.1807e-02,  3.8407e-03,  4.1079e-03, -4.7309e-02,\n",
       "                       -6.2890e-02, -5.6462e-02, -5.7723e-02, -1.4348e-02, -8.9207e-02,\n",
       "                        3.1268e-03,  2.3236e-02, -4.1331e-02,  4.0031e-02,  9.0804e-03,\n",
       "                       -3.6460e-02,  1.6860e-02, -3.2538e-02,  1.1887e-02,  2.2897e-03,\n",
       "                        4.6022e-02,  6.0197e-02,  4.9524e-02,  5.9695e-02,  2.2570e-02,\n",
       "                       -3.3635e-02,  5.2662e-02, -4.9301e-02,  9.3512e-02, -2.5059e-02,\n",
       "                       -5.4969e-02,  3.1818e-02,  5.1300e-03,  6.6161e-02, -3.9675e-02,\n",
       "                       -3.2185e-02, -6.1491e-02, -5.3421e-03,  8.6028e-02, -3.0599e-02,\n",
       "                        1.4690e-02, -7.4947e-02, -6.5251e-02, -8.8465e-03, -2.0431e-02,\n",
       "                       -3.9401e-01,  1.3688e-01,  2.2158e-01,  1.8167e-01,  5.7909e-02,\n",
       "                        1.4843e-02, -1.5298e-02, -1.2439e-03,  3.8237e-02, -2.5488e-02,\n",
       "                        6.1483e-02,  2.2272e-02,  8.7506e-02,  5.5469e-03, -5.1047e-02,\n",
       "                       -4.0738e-03, -3.6571e-02,  2.8925e-02, -2.4692e-03, -2.9037e-03,\n",
       "                        1.4425e-02, -2.5695e-03, -7.4915e-03, -4.2913e-02,  5.9805e-03,\n",
       "                       -9.5024e-03, -1.4713e-03,  7.4067e-02, -6.1135e-02, -6.5206e-02,\n",
       "                        1.0485e-02, -8.5703e-02, -6.3190e-02, -8.1618e-02,  4.6381e-02,\n",
       "                        2.0451e-02,  5.4372e-03, -2.3436e-02,  4.1820e-02,  3.6845e-02,\n",
       "                       -2.3739e-02,  3.1081e-02,  3.3163e-02,  1.5051e-02,  2.2875e-02,\n",
       "                        3.3485e-02,  4.1348e-02,  1.3883e-02, -1.4511e-02, -8.8988e-02,\n",
       "                       -3.0311e-02, -7.5647e-02, -4.2488e-02, -2.5118e-03, -5.1814e-02,\n",
       "                        8.1239e-02, -3.2406e-02,  2.8372e-02,  1.1694e-01, -2.2639e-01,\n",
       "                       -3.6427e-03,  7.8427e-03,  1.6472e-01,  7.3154e-02, -1.6300e-01],\n",
       "                      [-1.5602e-01,  3.6803e-02,  1.3245e-02,  4.2497e-03, -2.1668e-02,\n",
       "                        1.5069e-02, -8.8119e-03, -5.5514e-02, -2.7218e-02,  8.8880e-03,\n",
       "                       -7.3684e-02, -2.3550e-02,  1.1946e-02, -5.4290e-02, -6.1821e-03,\n",
       "                       -2.3154e-02,  5.3408e-02, -7.5804e-02,  1.4220e-01, -2.2653e-02,\n",
       "                       -1.6763e-02, -3.3819e-02, -6.6391e-03,  1.5518e-02, -5.8238e-02,\n",
       "                        2.6682e-02, -5.8504e-02, -2.5610e-02, -1.6125e-02,  7.4118e-02,\n",
       "                       -2.2900e-02,  8.7251e-02, -8.4931e-04,  1.7429e-02, -3.9687e-02,\n",
       "                        9.9931e-02,  5.1802e-02,  6.7405e-02,  6.8886e-03,  3.1289e-02,\n",
       "                        2.8762e-02, -8.1247e-02, -1.8035e-03,  1.9227e-01, -2.6088e-02,\n",
       "                        4.2884e-02, -5.6191e-02, -2.5521e-02, -2.0597e-01, -1.8768e-02,\n",
       "                        2.1933e-01, -6.8101e-02, -7.3397e-02, -2.8547e-01,  5.2145e-02,\n",
       "                       -4.4397e-02, -1.5443e-02,  4.5966e-02, -2.7812e-02, -4.1596e-02,\n",
       "                       -4.7899e-02, -8.3173e-03,  1.1929e-01, -7.8378e-03, -6.9695e-02,\n",
       "                       -6.7274e-02,  5.4798e-02, -3.0378e-02, -1.9009e-02, -9.6416e-02,\n",
       "                        3.5809e-02, -9.5533e-02,  2.6883e-03, -7.3440e-02, -2.2180e-03,\n",
       "                       -9.8763e-04, -1.6519e-02, -1.5238e-02,  5.7090e-02,  4.3498e-02,\n",
       "                       -1.6881e-02,  4.5213e-02,  7.2645e-02, -4.8734e-02, -8.4782e-03,\n",
       "                        1.0331e-01, -1.6478e-02,  1.1654e-01,  2.4265e-02, -6.4040e-02,\n",
       "                        3.7307e-02, -2.1747e-01,  3.3634e-02, -4.3044e-02,  6.3361e-02,\n",
       "                        1.1601e-01,  3.9391e-02,  9.9375e-02,  2.6279e-02,  1.2449e-01,\n",
       "                       -4.7680e-02,  6.9339e-02,  8.2217e-02,  3.5510e-02, -6.3062e-02,\n",
       "                       -1.5842e-01,  3.2713e-02,  6.4385e-02,  2.5164e-01, -4.3262e-03,\n",
       "                       -3.9271e-02, -5.2964e-02, -2.1823e-02,  4.1463e-03, -1.2310e-01],\n",
       "                      [-2.9809e-02,  4.0873e-02, -2.6818e-02,  1.6652e-02,  2.6718e-03,\n",
       "                        3.4241e-02, -1.0549e-02,  9.4135e-02,  8.8671e-03, -2.3366e-02,\n",
       "                       -6.1115e-03,  2.5037e-02, -2.1720e-02, -2.0442e-02, -4.3378e-02,\n",
       "                        4.1167e-02,  2.8816e-03, -1.4733e-02,  5.1191e-03, -4.8291e-02,\n",
       "                       -8.4823e-02, -9.0432e-03, -3.9075e-03, -1.5870e-02, -7.1590e-02,\n",
       "                       -7.7148e-02,  1.5613e-02, -7.6633e-02, -1.7299e-02,  4.5171e-02,\n",
       "                       -3.2390e-02, -1.3979e-01, -1.0824e-01,  7.9629e-02, -6.1478e-02,\n",
       "                       -6.0975e-02,  4.0915e-02, -4.7116e-02,  7.7862e-02,  1.4224e-02,\n",
       "                        4.7111e-02,  1.8630e-01, -2.4811e-02, -7.7888e-02, -4.9997e-02,\n",
       "                        3.8380e-02, -4.2129e-02,  2.7660e-02,  1.6025e-01,  1.8515e-02,\n",
       "                        1.0909e-01,  2.9019e-02, -1.0470e-01,  1.5558e-02,  7.2687e-02,\n",
       "                        3.7874e-02, -1.3682e-02,  1.6578e-02,  6.9595e-02,  3.9004e-02,\n",
       "                        7.1089e-02,  9.2320e-03,  4.0670e-03, -9.9054e-03, -3.6461e-02,\n",
       "                        3.4452e-02, -2.6006e-02,  3.1988e-02, -4.4201e-02, -4.1673e-02,\n",
       "                       -3.1385e-02, -7.3621e-02, -2.1621e-02, -4.7377e-02,  1.5784e-02,\n",
       "                       -4.2394e-02,  8.8771e-02,  1.2099e-01, -1.9731e-02,  3.1282e-02,\n",
       "                       -1.9781e-02,  1.1954e-02,  1.2273e-02,  3.5779e-02,  6.5139e-03,\n",
       "                       -3.8318e-02, -8.2343e-02,  4.9661e-02, -1.2180e-02,  9.6113e-02,\n",
       "                       -4.0472e-02, -2.1731e-02,  4.8639e-02,  7.2254e-02,  3.4995e-02,\n",
       "                       -9.1568e-02,  5.5253e-02, -4.2478e-03,  1.0183e-02, -4.7296e-02,\n",
       "                        9.3831e-03,  1.7621e-02, -2.0975e-02, -7.0699e-02, -2.9591e-02,\n",
       "                       -2.7114e-02, -5.2128e-02,  8.6446e-02, -1.2543e-01, -1.7790e-02,\n",
       "                       -9.8500e-02, -9.2429e-02, -1.1395e-01, -1.1604e-02,  1.0716e-01],\n",
       "                      [-1.0321e-03,  3.0967e-02, -2.8720e-02,  6.1750e-03,  6.5632e-03,\n",
       "                        3.9640e-02,  4.1935e-03,  8.1081e-02,  1.0720e-02, -4.0087e-02,\n",
       "                       -1.9034e-02,  4.4244e-02,  8.0003e-03, -1.8609e-02, -2.5385e-02,\n",
       "                        4.2010e-02,  8.6766e-03, -4.1126e-02,  3.3195e-02, -4.7869e-04,\n",
       "                       -5.9294e-02, -4.4260e-03,  1.6237e-02, -3.0295e-02, -1.0793e-01,\n",
       "                       -8.0199e-03,  2.2209e-02,  1.2753e-04, -2.1054e-02,  3.6034e-02,\n",
       "                       -5.8774e-02, -1.3661e-01, -3.4198e-02,  7.1390e-02, -2.6410e-02,\n",
       "                       -2.0677e-02,  3.5981e-02,  7.1542e-02,  5.4103e-02,  3.9231e-02,\n",
       "                        1.6570e-02,  5.0422e-02, -8.9646e-03, -1.5348e-02, -4.5345e-02,\n",
       "                        7.9856e-02,  3.6141e-03, -8.1532e-02,  1.1684e-01,  3.1583e-02,\n",
       "                        2.4384e-01, -1.6965e-01, -1.4775e-01, -5.4128e-04,  7.2445e-02,\n",
       "                        1.7375e-02, -1.7547e-02,  2.0652e-02,  7.8121e-02,  6.5337e-02,\n",
       "                        6.3713e-02, -6.9883e-03,  3.9446e-02,  1.0441e-02, -1.9751e-02,\n",
       "                        3.2216e-02,  3.4507e-03,  1.9249e-02, -2.2379e-02, -4.6019e-02,\n",
       "                       -3.8629e-02, -1.3181e-02, -1.5246e-02, -4.6566e-02, -6.7322e-04,\n",
       "                       -3.3077e-02,  5.4033e-02,  6.0935e-02, -1.1575e-02,  8.9281e-03,\n",
       "                       -9.9945e-03, -3.4850e-02, -1.4893e-02, -7.1373e-03, -1.4733e-02,\n",
       "                        6.5033e-03, -7.7830e-02,  2.4642e-02,  7.1513e-03,  1.9608e-02,\n",
       "                        5.0780e-03, -8.2666e-02,  6.0444e-02,  6.6680e-02,  3.0640e-02,\n",
       "                       -4.3635e-02,  8.1109e-02, -3.3588e-02,  3.1652e-02,  1.1889e-02,\n",
       "                       -9.8753e-04,  1.9872e-03, -7.2765e-02, -3.7156e-02, -6.7462e-02,\n",
       "                        4.7566e-02, -2.6079e-02, -4.4753e-02, -8.6734e-02, -1.0972e-01,\n",
       "                       -3.3109e-02, -2.7748e-02, -5.5359e-02, -3.0368e-03,  1.0841e-01]],\n",
       "                     device='mps:0')),\n",
       "             ('linear.bias',\n",
       "              tensor([ 0.4457, -2.3158,  0.9392,  1.1919], device='mps:0'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a261d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SOFTMAX on year '2020' with Aggregate Cross-Entropy (CPU)...\n",
      "Test data loaded: 3090 samples.\n",
      "  County-level predictions saved to: ./results/softmax_2020_predictions.csv\n",
      "  Aggregate True Distribution: [0.31386703 0.01158256 0.28773913 0.38681123]\n",
      "  Aggregate Predicted Distribution: [0.2852508  0.05439513 0.22603104 0.4343231 ]\n",
      "  Aggregate Cross-Entropy: 1.177912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.2852508 , 0.05439513, 0.22603104, 0.4343231 ], dtype=float32),\n",
       " 1.1779124736785889)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate \n",
    "softmax.evaluate(dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a9d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
