{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a26ccad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error # Using MSE for Ridge simplicity\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a3576f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Loading Function ---\n",
    "\n",
    "def load_data(suffix):\n",
    "    \"\"\"\n",
    "    Loads features (X), targets (y), and weights (wts) CSV files for a given prefix.\n",
    "\n",
    "    Args:\n",
    "        suffix (string): suffix for csv file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three pandas DataFrames (X_df, y_df, wts_df).\n",
    "    \"\"\"\n",
    "    X = pd.read_csv(f'X_{suffix}.csv')\n",
    "    y = pd.read_csv(f'y_{suffix}.csv')\n",
    "    wts = pd.read_csv(f'wts_{suffix}.csv')\n",
    "    return X, y, wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ce4b34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Dataset and DataLoader Creation Function ---\n",
    "\n",
    "def create_pytorch_datasets_loaders(X_train_df, \n",
    "                                    y_train_df, \n",
    "                                    wts_train_df,\n",
    "                                    X_val_df, \n",
    "                                    y_val_df, \n",
    "                                    wts_val_df,\n",
    "                                    batch_size):\n",
    "    \"\"\"\n",
    "    Converts pandas DataFrames into PyTorch Tensors, creates TensorDatasets,\n",
    "    and returns DataLoaders for training and validation.\n",
    "\n",
    "    Args:\n",
    "        X_train_df (pd.DataFrame): Training features.\n",
    "        y_train_df (pd.DataFrame): Training targets.\n",
    "        wts_train_df (pd.DataFrame): Training weights ('P(C)' column expected).\n",
    "        X_val_df (pd.DataFrame): Validation features.\n",
    "        y_val_df (pd.DataFrame): Validation targets.\n",
    "        wts_val_df (pd.DataFrame): Validation weights ('P(C)' column expected).\n",
    "        batch_size (int): The batch size for the DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (train_loader, val_loader).\n",
    "    \"\"\"\n",
    "    # --- Convert DataFrames to Tensors ---\n",
    "    # Use .values to get NumPy arrays, then convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_df.values, dtype=torch.float32) \n",
    "    y_train_tensor = torch.tensor(y_train_df.values, dtype=torch.float32)\n",
    "    # Ensure weights tensor is [N, 1] for potential broadcasting in loss\n",
    "    wts_train_tensor = torch.tensor(wts_train_df[['P(C)']].values, dtype=torch.float32)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val_df.values, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val_df.values, dtype=torch.float32)\n",
    "    wts_val_tensor = torch.tensor(wts_val_df[['P(C)']].values, dtype=torch.float32)\n",
    "\n",
    "    # --- Create TensorDatasets ---\n",
    "    # Combines tensors along the first dimension. Each sample will be retrieved as a tuple.\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor, wts_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor, wts_val_tensor)\n",
    "\n",
    "    #set the manual seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # --- Create DataLoaders ---\n",
    "    # Handles batching, shuffling, and parallel loading\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True  # Shuffle training data each epoch\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size, # Can use larger batch for validation if memory allows\n",
    "        shuffle=False # No need to shuffle validation data\n",
    "    )\n",
    "\n",
    "    print(f\"Created DataLoaders - Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "81ebf69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Weighted cross-entropy loss function ---\n",
    "\n",
    "def weighted_cross_entropy_loss(outputs, targets, weights):\n",
    "    \"\"\"\n",
    "    Compute weighted cross entropy loss.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model predictions (batch_size, n_classes)\n",
    "        targets (torch.Tensor): True probability distributions (batch_size, n_classes)\n",
    "        weights (torch.Tensor): Sample weights P(C) (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Expected value of cross entropy loss\n",
    "                        computed as Σ(P(C) * Loss(C)) / Σ(P(C))\n",
    "    \"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-7\n",
    "    outputs = torch.clamp(outputs, \n",
    "                          min=eps)\n",
    "    \n",
    "    # Compute cross entropy loss for each sample\n",
    "    sample_losses = -torch.sum(targets * torch.log(outputs), dim=1)\n",
    "    \n",
    "    # Weight each sample loss by P(C)\n",
    "    weighted_losses = sample_losses * weights\n",
    "    \n",
    "    # Return expected value (weighted average)\n",
    "    return weighted_losses.sum() / weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "30cf07dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. PyTorch Model Training Function with Early Stopping ---\n",
    "\n",
    "def train_pytorch_model(model, \n",
    "                        train_loader, \n",
    "                        val_loader, \n",
    "                        loss_fn, \n",
    "                        optimizer, \n",
    "                        device,\n",
    "                        max_epochs=200, \n",
    "                        patience=15, \n",
    "                        verbose=False):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model with early stopping based on validation loss.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        loss_fn (callable): The loss function. Should accept (outputs, targets, weights).\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "        device (torch.device): The device to perform computations on.\n",
    "        max_epochs (int): Maximum number of epochs to train.\n",
    "        patience (int): Number of epochs to wait for improvement before stopping.\n",
    "        verbose (bool): If True, prints loss per epoch. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: The best validation loss achieved during training.\n",
    "    \"\"\"\n",
    "    best_validation_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    if not val_loader: # Handle case with no validation data, though unlikely in CV\n",
    "        print(\"Warning: No validation loader provided. Cannot perform early stopping.\")\n",
    "        # Optionally train for max_epochs without validation, but not recommended for tuning\n",
    "        return float('inf') # Or handle differently\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()  # Set model to training mode (enables dropout, batch norm updates)\n",
    "        train_loss_epoch = 0.0\n",
    "        for batch_idx, (features, targets, weights) in enumerate(train_loader):\n",
    "            # Data already on the correct device from DataLoader preparation if done right\n",
    "            features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            outputs = model(features)\n",
    "\n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(outputs, targets, weights)\n",
    "\n",
    "            # 3. Backward pass and optimization\n",
    "            optimizer.zero_grad() # Clear previous gradients\n",
    "            loss.backward()       # Compute gradients\n",
    "            optimizer.step()      # Update weights\n",
    "\n",
    "            train_loss_epoch += loss.item() # Accumulate batch loss\n",
    "\n",
    "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout, uses running stats for batch norm)\n",
    "        validation_loss_epoch = 0.0\n",
    "        with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "            for features, targets, weights in val_loader:\n",
    "                features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = loss_fn(outputs, targets, weights)\n",
    "                validation_loss_epoch += loss.item()\n",
    "\n",
    "        avg_validation_loss = validation_loss_epoch / len(val_loader)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{max_epochs} - Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_validation_loss:.6f}\")\n",
    "\n",
    "        # --- Early Stopping Check ---\n",
    "        if avg_validation_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_validation_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_epoch = epoch + 1\n",
    "            # Optional: Save the best model state here if needed later\n",
    "            # torch.save(model.state_dict(), 'best_model_fold_X.pth')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs. Best val loss {best_validation_loss:.6f} at epoch {best_epoch}.\")\n",
    "            break # Exit the training loop\n",
    "\n",
    "\n",
    "    return best_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "FOLD_DEFINITIONS = [\n",
    "    {'train_suffix': '2008_2012', 'val_suffix': '2016'},\n",
    "    {'train_suffix': '2008_2016', 'val_suffix': '2012'},\n",
    "    {'train_suffix': '2012_2016', 'val_suffix': '2008'},\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "MAX_EPOCHS = 200 # Max epochs for early stopping\n",
    "PATIENCE = 15    # Patience for early stopping\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") #set device\n",
    "\n",
    "\n",
    "#make dictionary to hold results for each model\n",
    "results = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4cd8f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get Input/Output Dimensions Once ---\n",
    "temp_X, temp_y, _ = load_data(FOLD_DEFINITIONS[0]['train_suffix'])\n",
    "INPUT_DIM = temp_X.shape[1]\n",
    "OUTPUT_DIM = temp_y.shape[1]\n",
    "del temp_X, temp_y\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e48300ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Cross-Validation for Ridge Regression ---\n",
      "  Testing alpha=0.01:\n",
      "  Avg Val Score (MSE) for config 0.01: 0.001830\n",
      "  Testing alpha=0.1:\n",
      "  Avg Val Score (MSE) for config 0.1: 0.001830\n",
      "  Testing alpha=1.0:\n",
      "  Avg Val Score (MSE) for config 1.0: 0.001830\n",
      "  Testing alpha=10.0:\n",
      "  Avg Val Score (MSE) for config 10.0: 0.001830\n",
      "  Testing alpha=100.0:\n",
      "  Avg Val Score (MSE) for config 100.0: 0.001831\n",
      "  Testing alpha=1000.0:\n",
      "  Avg Val Score (MSE) for config 1000.0: 0.001853\n",
      "\n",
      "Best Ridge alpha found: 10.0 with score 0.001830\n",
      "--- Finished Cross-Validation for Ridge Regression ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>mean_cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.001830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.001830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.001830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.00</td>\n",
       "      <td>0.001831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.001853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     alpha  mean_cv_score\n",
       "3    10.00       0.001830\n",
       "2     1.00       0.001830\n",
       "1     0.10       0.001830\n",
       "0     0.01       0.001830\n",
       "4   100.00       0.001831\n",
       "5  1000.00       0.001853"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Ridge Regression Model Builder (Wrapper) ---\n",
    "\n",
    "def build_ridge_model(alpha):\n",
    "  \"\"\"\n",
    "  Builds (instantiates) a scikit-learn Ridge regression model.\n",
    "\n",
    "  Args:\n",
    "      alpha (float): Regularization strength (alpha >= 0). Larger values\n",
    "                     specify stronger regularization. Corresponds to L2 penalty.\n",
    "\n",
    "  Returns:\n",
    "      sklearn.linear_model.Ridge: An instance of the Ridge model, ready to be fit.\n",
    "  \"\"\"\n",
    "  # Instantiate the Ridge model with the specified alpha parameter\n",
    "  model = Ridge(alpha=alpha)\n",
    "  return model\n",
    "\n",
    "# --- 1. Cross-Validation Function for Ridge Regression (Modified) ---\n",
    "\n",
    "def cross_val_ridge(fold_definitions=FOLD_DEFINITIONS):\n",
    "    \"\"\"\n",
    "    Performs 3-fold CV hyperparameter tuning for Ridge Regression.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory containing the data CSV files.\n",
    "        fold_definitions (list): List defining train/val prefixes for each fold.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: DataFrame with columns ['alpha', 'mean_cv_score'], sorted by score.\n",
    "            - sklearn.linear_model.Ridge: Untrained Ridge model instance with the best alpha.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Cross-Validation for Ridge Regression ---\")\n",
    "    alphas = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "    results_list = []\n",
    "    best_score = float('inf')\n",
    "    best_alpha = None\n",
    "\n",
    "    for alpha in alphas:\n",
    "        print(f\"  Testing alpha={alpha}:\")\n",
    "        fold_validation_scores = []\n",
    "\n",
    "        for i, fold in enumerate(fold_definitions):\n",
    "            X_train_pd, y_train_pd, wts_train_pd = load_data(fold['train_suffix'])\n",
    "            X_val_pd, y_val_pd, wts_val_pd = load_data(fold['val_suffix'])\n",
    "\n",
    "            model = build_ridge_model(alpha=alpha) # Use the builder\n",
    "            model.fit(X_train_pd.values, y_train_pd.values)\n",
    "            y_pred_val = model.predict(X_val_pd.values)\n",
    "            validation_score = mean_squared_error(y_val_pd.values, y_pred_val)\n",
    "            fold_validation_scores.append(validation_score)\n",
    "\n",
    "        avg_validation_score = np.mean(fold_validation_scores)\n",
    "        print(f\"  Avg Val Score (MSE) for config {alpha}: {avg_validation_score:.6f}\")\n",
    "        results_list.append({'alpha': alpha, 'mean_cv_score': avg_validation_score})\n",
    "\n",
    "        # Track best configuration\n",
    "        if avg_validation_score < best_score:\n",
    "            best_score = avg_validation_score\n",
    "            best_alpha = alpha\n",
    "\n",
    "    # Compile results\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    results_df = results_df.sort_values(by='mean_cv_score', ascending=True)\n",
    "\n",
    "    # Build the best model instance (untrained)\n",
    "    print(f\"\\nBest Ridge alpha found: {best_alpha} with score {best_score:.6f}\")\n",
    "    best_model_instance = build_ridge_model(alpha=best_alpha)\n",
    "\n",
    "    print(\"--- Finished Cross-Validation for Ridge Regression ---\")\n",
    "    return results_df, best_model_instance\n",
    "\n",
    "# get results and model for ridge cross-validation\n",
    "results['ridge'], models['ridge'] = cross_val_ridge()\n",
    "\n",
    "# display ridge results\n",
    "results['ridge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "03695c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Cross-Validation for Softmax Regression ---\n",
      "  Testing config: {'learning_rate': 0.01, 'weight_decay': 0}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 46.905209, Val Loss: 44.169698\n",
      "Epoch 2/200 - Train Loss: 42.642436, Val Loss: 43.582655\n",
      "Epoch 3/200 - Train Loss: 42.104975, Val Loss: 43.865024\n",
      "Epoch 4/200 - Train Loss: 41.904845, Val Loss: 43.856537\n",
      "Epoch 5/200 - Train Loss: 41.904641, Val Loss: 43.841016\n",
      "Epoch 6/200 - Train Loss: 41.847481, Val Loss: 43.869766\n",
      "Epoch 7/200 - Train Loss: 41.826627, Val Loss: 43.911656\n",
      "Epoch 8/200 - Train Loss: 41.741646, Val Loss: 44.101544\n",
      "Epoch 9/200 - Train Loss: 41.865080, Val Loss: 44.020007\n",
      "Epoch 10/200 - Train Loss: 41.831663, Val Loss: 44.052717\n",
      "Epoch 11/200 - Train Loss: 41.781452, Val Loss: 44.506106\n",
      "Epoch 12/200 - Train Loss: 41.906978, Val Loss: 43.954545\n",
      "Epoch 13/200 - Train Loss: 41.861042, Val Loss: 44.058443\n",
      "Epoch 14/200 - Train Loss: 41.803274, Val Loss: 44.028854\n",
      "Epoch 15/200 - Train Loss: 41.779206, Val Loss: 44.230240\n",
      "Epoch 16/200 - Train Loss: 41.757584, Val Loss: 44.069249\n",
      "Epoch 17/200 - Train Loss: 41.770506, Val Loss: 44.045657\n",
      "Early stopping triggered after 17 epochs. Best val loss 43.582655 at epoch 2.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 46.991070, Val Loss: 43.371308\n",
      "Epoch 2/200 - Train Loss: 43.053001, Val Loss: 42.586682\n",
      "Epoch 3/200 - Train Loss: 42.719393, Val Loss: 42.320005\n",
      "Epoch 4/200 - Train Loss: 42.605459, Val Loss: 41.866973\n",
      "Epoch 5/200 - Train Loss: 42.628885, Val Loss: 41.997413\n",
      "Epoch 6/200 - Train Loss: 42.569207, Val Loss: 42.020495\n",
      "Epoch 7/200 - Train Loss: 42.546565, Val Loss: 41.981294\n",
      "Epoch 8/200 - Train Loss: 42.525353, Val Loss: 41.959613\n",
      "Epoch 9/200 - Train Loss: 42.629573, Val Loss: 41.985044\n",
      "Epoch 10/200 - Train Loss: 42.611678, Val Loss: 41.983583\n",
      "Epoch 11/200 - Train Loss: 42.527863, Val Loss: 42.087039\n",
      "Epoch 12/200 - Train Loss: 42.577422, Val Loss: 41.763269\n",
      "Epoch 13/200 - Train Loss: 42.571261, Val Loss: 42.139933\n",
      "Epoch 14/200 - Train Loss: 42.524440, Val Loss: 41.909479\n",
      "Epoch 15/200 - Train Loss: 42.596007, Val Loss: 41.971695\n",
      "Epoch 16/200 - Train Loss: 42.523387, Val Loss: 41.785712\n",
      "Epoch 17/200 - Train Loss: 42.564437, Val Loss: 41.842896\n",
      "Epoch 18/200 - Train Loss: 42.577576, Val Loss: 41.852351\n",
      "Epoch 19/200 - Train Loss: 42.575186, Val Loss: 41.907821\n",
      "Epoch 20/200 - Train Loss: 42.581908, Val Loss: 41.974807\n",
      "Epoch 21/200 - Train Loss: 42.576241, Val Loss: 41.882615\n",
      "Epoch 22/200 - Train Loss: 42.635341, Val Loss: 41.846112\n",
      "Epoch 23/200 - Train Loss: 42.530601, Val Loss: 41.830968\n",
      "Epoch 24/200 - Train Loss: 42.520327, Val Loss: 42.000224\n",
      "Epoch 25/200 - Train Loss: 42.593659, Val Loss: 41.979008\n",
      "Epoch 26/200 - Train Loss: 42.583409, Val Loss: 42.234123\n",
      "Epoch 27/200 - Train Loss: 42.681595, Val Loss: 41.849907\n",
      "Early stopping triggered after 27 epochs. Best val loss 41.763269 at epoch 12.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 47.005503, Val Loss: 43.851060\n",
      "Epoch 2/200 - Train Loss: 42.955568, Val Loss: 43.072250\n",
      "Epoch 3/200 - Train Loss: 42.650439, Val Loss: 42.550317\n",
      "Epoch 4/200 - Train Loss: 42.491889, Val Loss: 42.374034\n",
      "Epoch 5/200 - Train Loss: 42.509359, Val Loss: 42.542023\n",
      "Epoch 6/200 - Train Loss: 42.486568, Val Loss: 42.385880\n",
      "Epoch 7/200 - Train Loss: 42.478282, Val Loss: 42.331935\n",
      "Epoch 8/200 - Train Loss: 42.453900, Val Loss: 42.293980\n",
      "Epoch 9/200 - Train Loss: 42.564261, Val Loss: 42.390269\n",
      "Epoch 10/200 - Train Loss: 42.478240, Val Loss: 42.557227\n",
      "Epoch 11/200 - Train Loss: 42.416962, Val Loss: 42.457906\n",
      "Epoch 12/200 - Train Loss: 42.551564, Val Loss: 42.270193\n",
      "Epoch 13/200 - Train Loss: 42.545611, Val Loss: 42.766751\n",
      "Epoch 14/200 - Train Loss: 42.450603, Val Loss: 42.549265\n",
      "Epoch 15/200 - Train Loss: 42.475364, Val Loss: 42.322126\n",
      "Epoch 16/200 - Train Loss: 42.407994, Val Loss: 42.231213\n",
      "Epoch 17/200 - Train Loss: 42.568743, Val Loss: 42.214298\n",
      "Epoch 18/200 - Train Loss: 42.502019, Val Loss: 42.298045\n",
      "Epoch 19/200 - Train Loss: 42.516499, Val Loss: 42.312803\n",
      "Epoch 20/200 - Train Loss: 42.474839, Val Loss: 42.372289\n",
      "Epoch 21/200 - Train Loss: 42.494390, Val Loss: 42.316182\n",
      "Epoch 22/200 - Train Loss: 42.537373, Val Loss: 42.513244\n",
      "Epoch 23/200 - Train Loss: 42.476884, Val Loss: 42.352692\n",
      "Epoch 24/200 - Train Loss: 42.485486, Val Loss: 42.381672\n",
      "Epoch 25/200 - Train Loss: 42.518544, Val Loss: 42.294062\n",
      "Epoch 26/200 - Train Loss: 42.458010, Val Loss: 42.439377\n",
      "Epoch 27/200 - Train Loss: 42.576371, Val Loss: 42.565756\n",
      "Epoch 28/200 - Train Loss: 42.523125, Val Loss: 42.253775\n",
      "Epoch 29/200 - Train Loss: 42.479294, Val Loss: 42.343399\n",
      "Epoch 30/200 - Train Loss: 42.442891, Val Loss: 42.305065\n",
      "Epoch 31/200 - Train Loss: 42.473868, Val Loss: 42.263255\n",
      "Epoch 32/200 - Train Loss: 42.503618, Val Loss: 42.222969\n",
      "Early stopping triggered after 32 epochs. Best val loss 42.214298 at epoch 17.\n",
      "  Avg Val Score for config {'learning_rate': 0.01, 'weight_decay': 0}: 42.520074\n",
      "  Testing config: {'learning_rate': 0.01, 'weight_decay': 1e-05}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 46.905219, Val Loss: 44.169713\n",
      "Epoch 2/200 - Train Loss: 42.642458, Val Loss: 43.582655\n",
      "Epoch 3/200 - Train Loss: 42.104992, Val Loss: 43.865012\n",
      "Epoch 4/200 - Train Loss: 41.904859, Val Loss: 43.856509\n",
      "Epoch 5/200 - Train Loss: 41.904653, Val Loss: 43.840969\n",
      "Epoch 6/200 - Train Loss: 41.847488, Val Loss: 43.869708\n",
      "Epoch 7/200 - Train Loss: 41.826632, Val Loss: 43.911597\n",
      "Epoch 8/200 - Train Loss: 41.741650, Val Loss: 44.101472\n",
      "Epoch 9/200 - Train Loss: 41.865092, Val Loss: 44.019937\n",
      "Epoch 10/200 - Train Loss: 41.831662, Val Loss: 44.052647\n",
      "Epoch 11/200 - Train Loss: 41.781451, Val Loss: 44.506030\n",
      "Epoch 12/200 - Train Loss: 41.906976, Val Loss: 43.954483\n",
      "Epoch 13/200 - Train Loss: 41.861041, Val Loss: 44.058387\n",
      "Epoch 14/200 - Train Loss: 41.803273, Val Loss: 44.028795\n",
      "Epoch 15/200 - Train Loss: 41.779206, Val Loss: 44.230184\n",
      "Epoch 16/200 - Train Loss: 41.757583, Val Loss: 44.069200\n",
      "Epoch 17/200 - Train Loss: 41.770505, Val Loss: 44.045598\n",
      "Early stopping triggered after 17 epochs. Best val loss 43.582655 at epoch 2.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 46.991078, Val Loss: 43.371330\n",
      "Epoch 2/200 - Train Loss: 43.053020, Val Loss: 42.586703\n",
      "Epoch 3/200 - Train Loss: 42.719406, Val Loss: 42.320022\n",
      "Epoch 4/200 - Train Loss: 42.605466, Val Loss: 41.866991\n",
      "Epoch 5/200 - Train Loss: 42.628890, Val Loss: 41.997428\n",
      "Epoch 6/200 - Train Loss: 42.569210, Val Loss: 42.020509\n",
      "Epoch 7/200 - Train Loss: 42.546567, Val Loss: 41.981309\n",
      "Epoch 8/200 - Train Loss: 42.525353, Val Loss: 41.959625\n",
      "Epoch 9/200 - Train Loss: 42.629577, Val Loss: 41.985064\n",
      "Epoch 10/200 - Train Loss: 42.611677, Val Loss: 41.983597\n",
      "Epoch 11/200 - Train Loss: 42.527863, Val Loss: 42.087054\n",
      "Epoch 12/200 - Train Loss: 42.577421, Val Loss: 41.763280\n",
      "Epoch 13/200 - Train Loss: 42.571261, Val Loss: 42.139944\n",
      "Epoch 14/200 - Train Loss: 42.524440, Val Loss: 41.909489\n",
      "Epoch 15/200 - Train Loss: 42.596006, Val Loss: 41.971706\n",
      "Epoch 16/200 - Train Loss: 42.523386, Val Loss: 41.785720\n",
      "Epoch 17/200 - Train Loss: 42.564436, Val Loss: 41.842903\n",
      "Epoch 18/200 - Train Loss: 42.577575, Val Loss: 41.852357\n",
      "Epoch 19/200 - Train Loss: 42.575184, Val Loss: 41.907828\n",
      "Epoch 20/200 - Train Loss: 42.581907, Val Loss: 41.974817\n",
      "Epoch 21/200 - Train Loss: 42.576241, Val Loss: 41.882621\n",
      "Epoch 22/200 - Train Loss: 42.635341, Val Loss: 41.846121\n",
      "Epoch 23/200 - Train Loss: 42.530601, Val Loss: 41.830969\n",
      "Epoch 24/200 - Train Loss: 42.520328, Val Loss: 42.000222\n",
      "Epoch 25/200 - Train Loss: 42.593660, Val Loss: 41.979012\n",
      "Epoch 26/200 - Train Loss: 42.583410, Val Loss: 42.234132\n",
      "Epoch 27/200 - Train Loss: 42.681595, Val Loss: 41.849911\n",
      "Early stopping triggered after 27 epochs. Best val loss 41.763280 at epoch 12.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 47.005511, Val Loss: 43.851077\n",
      "Epoch 2/200 - Train Loss: 42.955586, Val Loss: 43.072268\n",
      "Epoch 3/200 - Train Loss: 42.650452, Val Loss: 42.550333\n",
      "Epoch 4/200 - Train Loss: 42.491897, Val Loss: 42.374049\n",
      "Epoch 5/200 - Train Loss: 42.509365, Val Loss: 42.542038\n",
      "Epoch 6/200 - Train Loss: 42.486571, Val Loss: 42.385892\n",
      "Epoch 7/200 - Train Loss: 42.478283, Val Loss: 42.331948\n",
      "Epoch 8/200 - Train Loss: 42.453900, Val Loss: 42.293995\n",
      "Epoch 9/200 - Train Loss: 42.564260, Val Loss: 42.390282\n",
      "Epoch 10/200 - Train Loss: 42.478239, Val Loss: 42.557240\n",
      "Epoch 11/200 - Train Loss: 42.416961, Val Loss: 42.457919\n",
      "Epoch 12/200 - Train Loss: 42.551563, Val Loss: 42.270203\n",
      "Epoch 13/200 - Train Loss: 42.545610, Val Loss: 42.766753\n",
      "Epoch 14/200 - Train Loss: 42.450602, Val Loss: 42.549272\n",
      "Epoch 15/200 - Train Loss: 42.475364, Val Loss: 42.322127\n",
      "Epoch 16/200 - Train Loss: 42.407993, Val Loss: 42.231216\n",
      "Epoch 17/200 - Train Loss: 42.568743, Val Loss: 42.214302\n",
      "Epoch 18/200 - Train Loss: 42.502018, Val Loss: 42.298044\n",
      "Epoch 19/200 - Train Loss: 42.516497, Val Loss: 42.312811\n",
      "Epoch 20/200 - Train Loss: 42.474838, Val Loss: 42.372296\n",
      "Epoch 21/200 - Train Loss: 42.494390, Val Loss: 42.316186\n",
      "Epoch 22/200 - Train Loss: 42.537373, Val Loss: 42.513245\n",
      "Epoch 23/200 - Train Loss: 42.476884, Val Loss: 42.352698\n",
      "Epoch 24/200 - Train Loss: 42.485485, Val Loss: 42.381670\n",
      "Epoch 25/200 - Train Loss: 42.518544, Val Loss: 42.294070\n",
      "Epoch 26/200 - Train Loss: 42.458010, Val Loss: 42.439385\n",
      "Epoch 27/200 - Train Loss: 42.576371, Val Loss: 42.565742\n",
      "Epoch 28/200 - Train Loss: 42.523123, Val Loss: 42.253773\n",
      "Epoch 29/200 - Train Loss: 42.479293, Val Loss: 42.343392\n",
      "Epoch 30/200 - Train Loss: 42.442891, Val Loss: 42.305069\n",
      "Epoch 31/200 - Train Loss: 42.473866, Val Loss: 42.263247\n",
      "Epoch 32/200 - Train Loss: 42.503616, Val Loss: 42.222964\n",
      "Early stopping triggered after 32 epochs. Best val loss 42.214302 at epoch 17.\n",
      "  Avg Val Score for config {'learning_rate': 0.01, 'weight_decay': 1e-05}: 42.520079\n",
      "  Testing config: {'learning_rate': 0.01, 'weight_decay': 0.0001}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 46.905295, Val Loss: 44.169829\n",
      "Epoch 2/200 - Train Loss: 42.642629, Val Loss: 43.582666\n",
      "Epoch 3/200 - Train Loss: 42.105146, Val Loss: 43.864913\n",
      "Epoch 4/200 - Train Loss: 41.904974, Val Loss: 43.856297\n",
      "Epoch 5/200 - Train Loss: 41.904736, Val Loss: 43.840650\n",
      "Epoch 6/200 - Train Loss: 41.847538, Val Loss: 43.869309\n",
      "Epoch 7/200 - Train Loss: 41.826666, Val Loss: 43.911184\n",
      "Epoch 8/200 - Train Loss: 41.741675, Val Loss: 44.100966\n",
      "Epoch 9/200 - Train Loss: 41.865158, Val Loss: 44.019451\n",
      "Epoch 10/200 - Train Loss: 41.831662, Val Loss: 44.052164\n",
      "Epoch 11/200 - Train Loss: 41.781446, Val Loss: 44.505474\n",
      "Epoch 12/200 - Train Loss: 41.906963, Val Loss: 43.954037\n",
      "Epoch 13/200 - Train Loss: 41.861034, Val Loss: 44.057983\n",
      "Epoch 14/200 - Train Loss: 41.803268, Val Loss: 44.028375\n",
      "Epoch 15/200 - Train Loss: 41.779199, Val Loss: 44.229778\n",
      "Epoch 16/200 - Train Loss: 41.757574, Val Loss: 44.068847\n",
      "Epoch 17/200 - Train Loss: 41.770499, Val Loss: 44.045163\n",
      "Early stopping triggered after 17 epochs. Best val loss 43.582666 at epoch 2.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 46.991142, Val Loss: 43.371498\n",
      "Epoch 2/200 - Train Loss: 43.053159, Val Loss: 42.586875\n",
      "Epoch 3/200 - Train Loss: 42.719515, Val Loss: 42.320178\n",
      "Epoch 4/200 - Train Loss: 42.605535, Val Loss: 41.867142\n",
      "Epoch 5/200 - Train Loss: 42.628932, Val Loss: 41.997543\n",
      "Epoch 6/200 - Train Loss: 42.569239, Val Loss: 42.020607\n",
      "Epoch 7/200 - Train Loss: 42.546576, Val Loss: 41.981395\n",
      "Epoch 8/200 - Train Loss: 42.525354, Val Loss: 41.959697\n",
      "Epoch 9/200 - Train Loss: 42.629594, Val Loss: 41.985182\n",
      "Epoch 10/200 - Train Loss: 42.611677, Val Loss: 41.983683\n",
      "Epoch 11/200 - Train Loss: 42.527862, Val Loss: 42.087146\n",
      "Epoch 12/200 - Train Loss: 42.577421, Val Loss: 41.763344\n",
      "Epoch 13/200 - Train Loss: 42.571253, Val Loss: 42.140007\n",
      "Epoch 14/200 - Train Loss: 42.524440, Val Loss: 41.909562\n",
      "Epoch 15/200 - Train Loss: 42.596006, Val Loss: 41.971778\n",
      "Epoch 16/200 - Train Loss: 42.523374, Val Loss: 41.785769\n",
      "Epoch 17/200 - Train Loss: 42.564425, Val Loss: 41.842939\n",
      "Epoch 18/200 - Train Loss: 42.577566, Val Loss: 41.852398\n",
      "Epoch 19/200 - Train Loss: 42.575172, Val Loss: 41.907865\n",
      "Epoch 20/200 - Train Loss: 42.581901, Val Loss: 41.974876\n",
      "Epoch 21/200 - Train Loss: 42.576245, Val Loss: 41.882652\n",
      "Epoch 22/200 - Train Loss: 42.635340, Val Loss: 41.846178\n",
      "Epoch 23/200 - Train Loss: 42.530601, Val Loss: 41.830973\n",
      "Epoch 24/200 - Train Loss: 42.520339, Val Loss: 42.000194\n",
      "Epoch 25/200 - Train Loss: 42.593662, Val Loss: 41.979036\n",
      "Epoch 26/200 - Train Loss: 42.583411, Val Loss: 42.234197\n",
      "Epoch 27/200 - Train Loss: 42.681592, Val Loss: 41.849936\n",
      "Early stopping triggered after 27 epochs. Best val loss 41.763344 at epoch 12.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 47.005577, Val Loss: 43.851213\n",
      "Epoch 2/200 - Train Loss: 42.955725, Val Loss: 43.072421\n",
      "Epoch 3/200 - Train Loss: 42.650560, Val Loss: 42.550495\n",
      "Epoch 4/200 - Train Loss: 42.491962, Val Loss: 42.374174\n",
      "Epoch 5/200 - Train Loss: 42.509404, Val Loss: 42.542148\n",
      "Epoch 6/200 - Train Loss: 42.486592, Val Loss: 42.385976\n",
      "Epoch 7/200 - Train Loss: 42.478291, Val Loss: 42.332045\n",
      "Epoch 8/200 - Train Loss: 42.453897, Val Loss: 42.294096\n",
      "Epoch 9/200 - Train Loss: 42.564256, Val Loss: 42.390372\n",
      "Epoch 10/200 - Train Loss: 42.478233, Val Loss: 42.557322\n",
      "Epoch 11/200 - Train Loss: 42.416954, Val Loss: 42.457996\n",
      "Epoch 12/200 - Train Loss: 42.551555, Val Loss: 42.270267\n",
      "Epoch 13/200 - Train Loss: 42.545604, Val Loss: 42.766756\n",
      "Epoch 14/200 - Train Loss: 42.450596, Val Loss: 42.549325\n",
      "Epoch 15/200 - Train Loss: 42.475362, Val Loss: 42.322137\n",
      "Epoch 16/200 - Train Loss: 42.407987, Val Loss: 42.231231\n",
      "Epoch 17/200 - Train Loss: 42.568740, Val Loss: 42.214326\n",
      "Epoch 18/200 - Train Loss: 42.502011, Val Loss: 42.298029\n",
      "Epoch 19/200 - Train Loss: 42.516488, Val Loss: 42.312860\n",
      "Epoch 20/200 - Train Loss: 42.474828, Val Loss: 42.372339\n",
      "Epoch 21/200 - Train Loss: 42.494391, Val Loss: 42.316214\n",
      "Epoch 22/200 - Train Loss: 42.537371, Val Loss: 42.513232\n",
      "Epoch 23/200 - Train Loss: 42.476881, Val Loss: 42.352738\n",
      "Epoch 24/200 - Train Loss: 42.485474, Val Loss: 42.381644\n",
      "Epoch 25/200 - Train Loss: 42.518548, Val Loss: 42.294131\n",
      "Epoch 26/200 - Train Loss: 42.458014, Val Loss: 42.439444\n",
      "Epoch 27/200 - Train Loss: 42.576366, Val Loss: 42.565615\n",
      "Epoch 28/200 - Train Loss: 42.523106, Val Loss: 42.253755\n",
      "Epoch 29/200 - Train Loss: 42.479298, Val Loss: 42.343338\n",
      "Epoch 30/200 - Train Loss: 42.442882, Val Loss: 42.305091\n",
      "Epoch 31/200 - Train Loss: 42.473851, Val Loss: 42.263177\n",
      "Epoch 32/200 - Train Loss: 42.503602, Val Loss: 42.222923\n",
      "Early stopping triggered after 32 epochs. Best val loss 42.214326 at epoch 17.\n",
      "  Avg Val Score for config {'learning_rate': 0.01, 'weight_decay': 0.0001}: 42.520112\n",
      "  Testing config: {'learning_rate': 0.001, 'weight_decay': 0}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 53.026153, Val Loss: 51.078779\n",
      "Epoch 2/200 - Train Loss: 49.698772, Val Loss: 48.993274\n",
      "Epoch 3/200 - Train Loss: 48.006836, Val Loss: 47.604226\n",
      "Epoch 4/200 - Train Loss: 46.784682, Val Loss: 46.654561\n",
      "Epoch 5/200 - Train Loss: 45.841986, Val Loss: 45.882479\n",
      "Epoch 6/200 - Train Loss: 45.096340, Val Loss: 45.363682\n",
      "Epoch 7/200 - Train Loss: 44.495440, Val Loss: 44.863932\n",
      "Epoch 8/200 - Train Loss: 44.012819, Val Loss: 44.475775\n",
      "Epoch 9/200 - Train Loss: 43.614956, Val Loss: 44.239394\n",
      "Epoch 10/200 - Train Loss: 43.286679, Val Loss: 44.038039\n",
      "Epoch 11/200 - Train Loss: 43.015783, Val Loss: 43.799381\n",
      "Epoch 12/200 - Train Loss: 42.789018, Val Loss: 43.676103\n",
      "Epoch 13/200 - Train Loss: 42.600833, Val Loss: 43.612036\n",
      "Epoch 14/200 - Train Loss: 42.445444, Val Loss: 43.504888\n",
      "Epoch 15/200 - Train Loss: 42.314470, Val Loss: 43.454975\n",
      "Epoch 16/200 - Train Loss: 42.196839, Val Loss: 43.358297\n",
      "Epoch 17/200 - Train Loss: 42.097753, Val Loss: 43.376271\n",
      "Epoch 18/200 - Train Loss: 42.018147, Val Loss: 43.385324\n",
      "Epoch 19/200 - Train Loss: 41.946798, Val Loss: 43.350216\n",
      "Epoch 20/200 - Train Loss: 41.886134, Val Loss: 43.310991\n",
      "Epoch 21/200 - Train Loss: 41.836073, Val Loss: 43.321798\n",
      "Epoch 22/200 - Train Loss: 41.789124, Val Loss: 43.330314\n",
      "Epoch 23/200 - Train Loss: 41.751818, Val Loss: 43.356487\n",
      "Epoch 24/200 - Train Loss: 41.716531, Val Loss: 43.355633\n",
      "Epoch 25/200 - Train Loss: 41.687815, Val Loss: 43.388703\n",
      "Epoch 26/200 - Train Loss: 41.659644, Val Loss: 43.401185\n",
      "Epoch 27/200 - Train Loss: 41.640091, Val Loss: 43.360765\n",
      "Epoch 28/200 - Train Loss: 41.617562, Val Loss: 43.375759\n",
      "Epoch 29/200 - Train Loss: 41.606025, Val Loss: 43.484655\n",
      "Epoch 30/200 - Train Loss: 41.592085, Val Loss: 43.430891\n",
      "Epoch 31/200 - Train Loss: 41.576134, Val Loss: 43.437736\n",
      "Epoch 32/200 - Train Loss: 41.569734, Val Loss: 43.476966\n",
      "Epoch 33/200 - Train Loss: 41.556273, Val Loss: 43.454367\n",
      "Epoch 34/200 - Train Loss: 41.549550, Val Loss: 43.521473\n",
      "Epoch 35/200 - Train Loss: 41.546283, Val Loss: 43.531985\n",
      "Early stopping triggered after 35 epochs. Best val loss 43.310991 at epoch 20.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 52.953585, Val Loss: 50.943587\n",
      "Epoch 2/200 - Train Loss: 49.616612, Val Loss: 48.852926\n",
      "Epoch 3/200 - Train Loss: 47.962523, Val Loss: 47.466517\n",
      "Epoch 4/200 - Train Loss: 46.804322, Val Loss: 46.374545\n",
      "Epoch 5/200 - Train Loss: 45.920089, Val Loss: 45.541157\n",
      "Epoch 6/200 - Train Loss: 45.229773, Val Loss: 44.870779\n",
      "Epoch 7/200 - Train Loss: 44.683480, Val Loss: 44.330711\n",
      "Epoch 8/200 - Train Loss: 44.248015, Val Loss: 43.930632\n",
      "Epoch 9/200 - Train Loss: 43.897923, Val Loss: 43.535884\n",
      "Epoch 10/200 - Train Loss: 43.611153, Val Loss: 43.234726\n",
      "Epoch 11/200 - Train Loss: 43.378085, Val Loss: 43.032798\n",
      "Epoch 12/200 - Train Loss: 43.189448, Val Loss: 42.811637\n",
      "Epoch 13/200 - Train Loss: 43.032483, Val Loss: 42.614332\n",
      "Epoch 14/200 - Train Loss: 42.905996, Val Loss: 42.486491\n",
      "Epoch 15/200 - Train Loss: 42.799546, Val Loss: 42.358515\n",
      "Epoch 16/200 - Train Loss: 42.708652, Val Loss: 42.291506\n",
      "Epoch 17/200 - Train Loss: 42.631963, Val Loss: 42.171317\n",
      "Epoch 18/200 - Train Loss: 42.573182, Val Loss: 42.082056\n",
      "Epoch 19/200 - Train Loss: 42.523988, Val Loss: 42.021204\n",
      "Epoch 20/200 - Train Loss: 42.478218, Val Loss: 41.969040\n",
      "Epoch 21/200 - Train Loss: 42.443004, Val Loss: 41.936305\n",
      "Epoch 22/200 - Train Loss: 42.409834, Val Loss: 41.881194\n",
      "Epoch 23/200 - Train Loss: 42.386753, Val Loss: 41.841459\n",
      "Epoch 24/200 - Train Loss: 42.362822, Val Loss: 41.802777\n",
      "Epoch 25/200 - Train Loss: 42.345938, Val Loss: 41.842211\n",
      "Epoch 26/200 - Train Loss: 42.331646, Val Loss: 41.755309\n",
      "Epoch 27/200 - Train Loss: 42.317214, Val Loss: 41.749730\n",
      "Epoch 28/200 - Train Loss: 42.305177, Val Loss: 41.726631\n",
      "Epoch 29/200 - Train Loss: 42.298532, Val Loss: 41.695864\n",
      "Epoch 30/200 - Train Loss: 42.290785, Val Loss: 41.717377\n",
      "Epoch 31/200 - Train Loss: 42.282598, Val Loss: 41.671876\n",
      "Epoch 32/200 - Train Loss: 42.278267, Val Loss: 41.692308\n",
      "Epoch 33/200 - Train Loss: 42.272102, Val Loss: 41.686861\n",
      "Epoch 34/200 - Train Loss: 42.272797, Val Loss: 41.640037\n",
      "Epoch 35/200 - Train Loss: 42.268702, Val Loss: 41.642824\n",
      "Epoch 36/200 - Train Loss: 42.267683, Val Loss: 41.681922\n",
      "Epoch 37/200 - Train Loss: 42.263778, Val Loss: 41.639769\n",
      "Epoch 38/200 - Train Loss: 42.261599, Val Loss: 41.616597\n",
      "Epoch 39/200 - Train Loss: 42.263530, Val Loss: 41.621173\n",
      "Epoch 40/200 - Train Loss: 42.259316, Val Loss: 41.623631\n",
      "Epoch 41/200 - Train Loss: 42.258242, Val Loss: 41.625202\n",
      "Epoch 42/200 - Train Loss: 42.257743, Val Loss: 41.611137\n",
      "Epoch 43/200 - Train Loss: 42.255810, Val Loss: 41.614812\n",
      "Epoch 44/200 - Train Loss: 42.258903, Val Loss: 41.646077\n",
      "Epoch 45/200 - Train Loss: 42.260399, Val Loss: 41.607418\n",
      "Epoch 46/200 - Train Loss: 42.259214, Val Loss: 41.618161\n",
      "Epoch 47/200 - Train Loss: 42.253131, Val Loss: 41.608026\n",
      "Epoch 48/200 - Train Loss: 42.253430, Val Loss: 41.613002\n",
      "Epoch 49/200 - Train Loss: 42.254930, Val Loss: 41.612017\n",
      "Epoch 50/200 - Train Loss: 42.256457, Val Loss: 41.583371\n",
      "Epoch 51/200 - Train Loss: 42.259339, Val Loss: 41.625986\n",
      "Epoch 52/200 - Train Loss: 42.257923, Val Loss: 41.608087\n",
      "Epoch 53/200 - Train Loss: 42.255587, Val Loss: 41.595196\n",
      "Epoch 54/200 - Train Loss: 42.256473, Val Loss: 41.633059\n",
      "Epoch 55/200 - Train Loss: 42.253564, Val Loss: 41.605266\n",
      "Epoch 56/200 - Train Loss: 42.253584, Val Loss: 41.598981\n",
      "Epoch 57/200 - Train Loss: 42.258009, Val Loss: 41.612162\n",
      "Epoch 58/200 - Train Loss: 42.257838, Val Loss: 41.628151\n",
      "Epoch 59/200 - Train Loss: 42.255351, Val Loss: 41.599246\n",
      "Epoch 60/200 - Train Loss: 42.251302, Val Loss: 41.630990\n",
      "Epoch 61/200 - Train Loss: 42.257606, Val Loss: 41.584178\n",
      "Epoch 62/200 - Train Loss: 42.252436, Val Loss: 41.630647\n",
      "Epoch 63/200 - Train Loss: 42.250213, Val Loss: 41.607921\n",
      "Epoch 64/200 - Train Loss: 42.254670, Val Loss: 41.610098\n",
      "Epoch 65/200 - Train Loss: 42.251200, Val Loss: 41.604435\n",
      "Early stopping triggered after 65 epochs. Best val loss 41.583371 at epoch 50.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 53.186132, Val Loss: 50.705145\n",
      "Epoch 2/200 - Train Loss: 49.704687, Val Loss: 48.809049\n",
      "Epoch 3/200 - Train Loss: 47.988038, Val Loss: 47.548887\n",
      "Epoch 4/200 - Train Loss: 46.797088, Val Loss: 46.587049\n",
      "Epoch 5/200 - Train Loss: 45.897087, Val Loss: 45.833829\n",
      "Epoch 6/200 - Train Loss: 45.194600, Val Loss: 45.196733\n",
      "Epoch 7/200 - Train Loss: 44.632307, Val Loss: 44.693772\n",
      "Epoch 8/200 - Train Loss: 44.187026, Val Loss: 44.311726\n",
      "Epoch 9/200 - Train Loss: 43.830103, Val Loss: 43.939338\n",
      "Epoch 10/200 - Train Loss: 43.536205, Val Loss: 43.662434\n",
      "Epoch 11/200 - Train Loss: 43.299423, Val Loss: 43.470690\n",
      "Epoch 12/200 - Train Loss: 43.106826, Val Loss: 43.240941\n",
      "Epoch 13/200 - Train Loss: 42.944166, Val Loss: 43.047204\n",
      "Epoch 14/200 - Train Loss: 42.813528, Val Loss: 42.914293\n",
      "Epoch 15/200 - Train Loss: 42.707499, Val Loss: 42.819581\n",
      "Epoch 16/200 - Train Loss: 42.615907, Val Loss: 42.713679\n",
      "Epoch 17/200 - Train Loss: 42.538659, Val Loss: 42.614796\n",
      "Epoch 18/200 - Train Loss: 42.477984, Val Loss: 42.529827\n",
      "Epoch 19/200 - Train Loss: 42.426244, Val Loss: 42.464527\n",
      "Epoch 20/200 - Train Loss: 42.384760, Val Loss: 42.405913\n",
      "Epoch 21/200 - Train Loss: 42.343363, Val Loss: 42.354476\n",
      "Epoch 22/200 - Train Loss: 42.313526, Val Loss: 42.329961\n",
      "Epoch 23/200 - Train Loss: 42.291453, Val Loss: 42.284066\n",
      "Epoch 24/200 - Train Loss: 42.268645, Val Loss: 42.247669\n",
      "Epoch 25/200 - Train Loss: 42.251527, Val Loss: 42.231089\n",
      "Epoch 26/200 - Train Loss: 42.236005, Val Loss: 42.176725\n",
      "Epoch 27/200 - Train Loss: 42.225068, Val Loss: 42.174150\n",
      "Epoch 28/200 - Train Loss: 42.212614, Val Loss: 42.163707\n",
      "Epoch 29/200 - Train Loss: 42.205865, Val Loss: 42.134824\n",
      "Epoch 30/200 - Train Loss: 42.198922, Val Loss: 42.133658\n",
      "Epoch 31/200 - Train Loss: 42.192127, Val Loss: 42.137048\n",
      "Epoch 32/200 - Train Loss: 42.185467, Val Loss: 42.124175\n",
      "Epoch 33/200 - Train Loss: 42.184300, Val Loss: 42.092449\n",
      "Epoch 34/200 - Train Loss: 42.182371, Val Loss: 42.098539\n",
      "Epoch 35/200 - Train Loss: 42.179915, Val Loss: 42.059774\n",
      "Epoch 36/200 - Train Loss: 42.178106, Val Loss: 42.087108\n",
      "Epoch 37/200 - Train Loss: 42.173889, Val Loss: 42.049367\n",
      "Epoch 38/200 - Train Loss: 42.173896, Val Loss: 42.049256\n",
      "Epoch 39/200 - Train Loss: 42.172579, Val Loss: 42.061717\n",
      "Epoch 40/200 - Train Loss: 42.173037, Val Loss: 42.063297\n",
      "Epoch 41/200 - Train Loss: 42.167229, Val Loss: 42.044265\n",
      "Epoch 42/200 - Train Loss: 42.169288, Val Loss: 42.055034\n",
      "Epoch 43/200 - Train Loss: 42.169785, Val Loss: 42.066076\n",
      "Epoch 44/200 - Train Loss: 42.173528, Val Loss: 42.051230\n",
      "Epoch 45/200 - Train Loss: 42.174532, Val Loss: 42.054547\n",
      "Epoch 46/200 - Train Loss: 42.173996, Val Loss: 42.014084\n",
      "Epoch 47/200 - Train Loss: 42.170308, Val Loss: 42.057582\n",
      "Epoch 48/200 - Train Loss: 42.165181, Val Loss: 42.069661\n",
      "Epoch 49/200 - Train Loss: 42.169921, Val Loss: 42.051390\n",
      "Epoch 50/200 - Train Loss: 42.170699, Val Loss: 42.032539\n",
      "Epoch 51/200 - Train Loss: 42.173254, Val Loss: 42.065376\n",
      "Epoch 52/200 - Train Loss: 42.174385, Val Loss: 42.032306\n",
      "Epoch 53/200 - Train Loss: 42.167907, Val Loss: 42.019710\n",
      "Epoch 54/200 - Train Loss: 42.171502, Val Loss: 42.057593\n",
      "Epoch 55/200 - Train Loss: 42.170220, Val Loss: 42.031540\n",
      "Epoch 56/200 - Train Loss: 42.168354, Val Loss: 42.010712\n",
      "Epoch 57/200 - Train Loss: 42.172399, Val Loss: 42.027168\n",
      "Epoch 58/200 - Train Loss: 42.172156, Val Loss: 42.031870\n",
      "Epoch 59/200 - Train Loss: 42.173001, Val Loss: 42.034797\n",
      "Epoch 60/200 - Train Loss: 42.170550, Val Loss: 42.048563\n",
      "Epoch 61/200 - Train Loss: 42.175868, Val Loss: 42.041457\n",
      "Epoch 62/200 - Train Loss: 42.167597, Val Loss: 42.045641\n",
      "Epoch 63/200 - Train Loss: 42.169756, Val Loss: 42.061858\n",
      "Epoch 64/200 - Train Loss: 42.173180, Val Loss: 42.018037\n",
      "Epoch 65/200 - Train Loss: 42.168500, Val Loss: 42.042665\n",
      "Epoch 66/200 - Train Loss: 42.170985, Val Loss: 42.098305\n",
      "Epoch 67/200 - Train Loss: 42.172101, Val Loss: 42.023404\n",
      "Epoch 68/200 - Train Loss: 42.173479, Val Loss: 42.005338\n",
      "Epoch 69/200 - Train Loss: 42.172470, Val Loss: 42.006019\n",
      "Epoch 70/200 - Train Loss: 42.170957, Val Loss: 42.036013\n",
      "Epoch 71/200 - Train Loss: 42.169612, Val Loss: 42.022474\n",
      "Epoch 72/200 - Train Loss: 42.171049, Val Loss: 42.043165\n",
      "Epoch 73/200 - Train Loss: 42.170864, Val Loss: 42.023279\n",
      "Epoch 74/200 - Train Loss: 42.171597, Val Loss: 42.010564\n",
      "Epoch 75/200 - Train Loss: 42.171028, Val Loss: 42.026320\n",
      "Epoch 76/200 - Train Loss: 42.171642, Val Loss: 42.033890\n",
      "Epoch 77/200 - Train Loss: 42.171747, Val Loss: 42.061962\n",
      "Epoch 78/200 - Train Loss: 42.172284, Val Loss: 42.015643\n",
      "Epoch 79/200 - Train Loss: 42.170104, Val Loss: 42.083160\n",
      "Epoch 80/200 - Train Loss: 42.168762, Val Loss: 42.044294\n",
      "Epoch 81/200 - Train Loss: 42.173238, Val Loss: 42.054547\n",
      "Epoch 82/200 - Train Loss: 42.171059, Val Loss: 42.033729\n",
      "Epoch 83/200 - Train Loss: 42.170475, Val Loss: 42.035638\n",
      "Early stopping triggered after 83 epochs. Best val loss 42.005338 at epoch 68.\n",
      "  Avg Val Score for config {'learning_rate': 0.001, 'weight_decay': 0}: 42.299900\n",
      "  Testing config: {'learning_rate': 0.001, 'weight_decay': 1e-05}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 53.026153, Val Loss: 51.078779\n",
      "Epoch 2/200 - Train Loss: 49.698772, Val Loss: 48.993274\n",
      "Epoch 3/200 - Train Loss: 48.006836, Val Loss: 47.604226\n",
      "Epoch 4/200 - Train Loss: 46.784682, Val Loss: 46.654561\n",
      "Epoch 5/200 - Train Loss: 45.841986, Val Loss: 45.882479\n",
      "Epoch 6/200 - Train Loss: 45.096340, Val Loss: 45.363682\n",
      "Epoch 7/200 - Train Loss: 44.495440, Val Loss: 44.863932\n",
      "Epoch 8/200 - Train Loss: 44.012819, Val Loss: 44.475775\n",
      "Epoch 9/200 - Train Loss: 43.614956, Val Loss: 44.239394\n",
      "Epoch 10/200 - Train Loss: 43.286679, Val Loss: 44.038039\n",
      "Epoch 11/200 - Train Loss: 43.015783, Val Loss: 43.799381\n",
      "Epoch 12/200 - Train Loss: 42.789018, Val Loss: 43.676103\n",
      "Epoch 13/200 - Train Loss: 42.600833, Val Loss: 43.612036\n",
      "Epoch 14/200 - Train Loss: 42.445444, Val Loss: 43.504888\n",
      "Epoch 15/200 - Train Loss: 42.314470, Val Loss: 43.454975\n",
      "Epoch 16/200 - Train Loss: 42.196839, Val Loss: 43.358297\n",
      "Epoch 17/200 - Train Loss: 42.097753, Val Loss: 43.376271\n",
      "Epoch 18/200 - Train Loss: 42.018147, Val Loss: 43.385324\n",
      "Epoch 19/200 - Train Loss: 41.946798, Val Loss: 43.350216\n",
      "Epoch 20/200 - Train Loss: 41.886134, Val Loss: 43.310991\n",
      "Epoch 21/200 - Train Loss: 41.836073, Val Loss: 43.321798\n",
      "Epoch 22/200 - Train Loss: 41.789124, Val Loss: 43.330314\n",
      "Epoch 23/200 - Train Loss: 41.751818, Val Loss: 43.356487\n",
      "Epoch 24/200 - Train Loss: 41.716531, Val Loss: 43.355633\n",
      "Epoch 25/200 - Train Loss: 41.687815, Val Loss: 43.388703\n",
      "Epoch 26/200 - Train Loss: 41.659644, Val Loss: 43.401185\n",
      "Epoch 27/200 - Train Loss: 41.640091, Val Loss: 43.360765\n",
      "Epoch 28/200 - Train Loss: 41.617562, Val Loss: 43.375759\n",
      "Epoch 29/200 - Train Loss: 41.606025, Val Loss: 43.484655\n",
      "Epoch 30/200 - Train Loss: 41.592085, Val Loss: 43.430891\n",
      "Epoch 31/200 - Train Loss: 41.576134, Val Loss: 43.437736\n",
      "Epoch 32/200 - Train Loss: 41.569734, Val Loss: 43.476966\n",
      "Epoch 33/200 - Train Loss: 41.556273, Val Loss: 43.454367\n",
      "Epoch 34/200 - Train Loss: 41.549550, Val Loss: 43.521473\n",
      "Epoch 35/200 - Train Loss: 41.546283, Val Loss: 43.531985\n",
      "Early stopping triggered after 35 epochs. Best val loss 43.310991 at epoch 20.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 52.953585, Val Loss: 50.943587\n",
      "Epoch 2/200 - Train Loss: 49.616612, Val Loss: 48.852926\n",
      "Epoch 3/200 - Train Loss: 47.962523, Val Loss: 47.466517\n",
      "Epoch 4/200 - Train Loss: 46.804322, Val Loss: 46.374545\n",
      "Epoch 5/200 - Train Loss: 45.920089, Val Loss: 45.541157\n",
      "Epoch 6/200 - Train Loss: 45.229773, Val Loss: 44.870779\n",
      "Epoch 7/200 - Train Loss: 44.683480, Val Loss: 44.330711\n",
      "Epoch 8/200 - Train Loss: 44.248015, Val Loss: 43.930632\n",
      "Epoch 9/200 - Train Loss: 43.897923, Val Loss: 43.535884\n",
      "Epoch 10/200 - Train Loss: 43.611153, Val Loss: 43.234726\n",
      "Epoch 11/200 - Train Loss: 43.378085, Val Loss: 43.032798\n",
      "Epoch 12/200 - Train Loss: 43.189448, Val Loss: 42.811637\n",
      "Epoch 13/200 - Train Loss: 43.032483, Val Loss: 42.614332\n",
      "Epoch 14/200 - Train Loss: 42.905996, Val Loss: 42.486491\n",
      "Epoch 15/200 - Train Loss: 42.799546, Val Loss: 42.358515\n",
      "Epoch 16/200 - Train Loss: 42.708652, Val Loss: 42.291506\n",
      "Epoch 17/200 - Train Loss: 42.631963, Val Loss: 42.171317\n",
      "Epoch 18/200 - Train Loss: 42.573182, Val Loss: 42.082056\n",
      "Epoch 19/200 - Train Loss: 42.523988, Val Loss: 42.021204\n",
      "Epoch 20/200 - Train Loss: 42.478218, Val Loss: 41.969040\n",
      "Epoch 21/200 - Train Loss: 42.443004, Val Loss: 41.936305\n",
      "Epoch 22/200 - Train Loss: 42.409834, Val Loss: 41.881194\n",
      "Epoch 23/200 - Train Loss: 42.386753, Val Loss: 41.841459\n",
      "Epoch 24/200 - Train Loss: 42.362822, Val Loss: 41.802777\n",
      "Epoch 25/200 - Train Loss: 42.345938, Val Loss: 41.842211\n",
      "Epoch 26/200 - Train Loss: 42.331646, Val Loss: 41.755309\n",
      "Epoch 27/200 - Train Loss: 42.317214, Val Loss: 41.749730\n",
      "Epoch 28/200 - Train Loss: 42.305177, Val Loss: 41.726631\n",
      "Epoch 29/200 - Train Loss: 42.298532, Val Loss: 41.695864\n",
      "Epoch 30/200 - Train Loss: 42.290785, Val Loss: 41.717377\n",
      "Epoch 31/200 - Train Loss: 42.282598, Val Loss: 41.671876\n",
      "Epoch 32/200 - Train Loss: 42.278267, Val Loss: 41.692308\n",
      "Epoch 33/200 - Train Loss: 42.272102, Val Loss: 41.686861\n",
      "Epoch 34/200 - Train Loss: 42.272797, Val Loss: 41.640037\n",
      "Epoch 35/200 - Train Loss: 42.268702, Val Loss: 41.642824\n",
      "Epoch 36/200 - Train Loss: 42.267683, Val Loss: 41.681922\n",
      "Epoch 37/200 - Train Loss: 42.263778, Val Loss: 41.639769\n",
      "Epoch 38/200 - Train Loss: 42.261599, Val Loss: 41.616597\n",
      "Epoch 39/200 - Train Loss: 42.263530, Val Loss: 41.621173\n",
      "Epoch 40/200 - Train Loss: 42.259316, Val Loss: 41.623631\n",
      "Epoch 41/200 - Train Loss: 42.258242, Val Loss: 41.625202\n",
      "Epoch 42/200 - Train Loss: 42.257743, Val Loss: 41.611137\n",
      "Epoch 43/200 - Train Loss: 42.255810, Val Loss: 41.614812\n",
      "Epoch 44/200 - Train Loss: 42.258903, Val Loss: 41.646077\n",
      "Epoch 45/200 - Train Loss: 42.260399, Val Loss: 41.607418\n",
      "Epoch 46/200 - Train Loss: 42.259214, Val Loss: 41.618161\n",
      "Epoch 47/200 - Train Loss: 42.253131, Val Loss: 41.608026\n",
      "Epoch 48/200 - Train Loss: 42.253430, Val Loss: 41.613002\n",
      "Epoch 49/200 - Train Loss: 42.254930, Val Loss: 41.612017\n",
      "Epoch 50/200 - Train Loss: 42.256457, Val Loss: 41.583371\n",
      "Epoch 51/200 - Train Loss: 42.259339, Val Loss: 41.625986\n",
      "Epoch 52/200 - Train Loss: 42.257923, Val Loss: 41.608087\n",
      "Epoch 53/200 - Train Loss: 42.255587, Val Loss: 41.595196\n",
      "Epoch 54/200 - Train Loss: 42.256473, Val Loss: 41.633059\n",
      "Epoch 55/200 - Train Loss: 42.253564, Val Loss: 41.605266\n",
      "Epoch 56/200 - Train Loss: 42.253584, Val Loss: 41.598981\n",
      "Epoch 57/200 - Train Loss: 42.258009, Val Loss: 41.612162\n",
      "Epoch 58/200 - Train Loss: 42.257838, Val Loss: 41.628151\n",
      "Epoch 59/200 - Train Loss: 42.255351, Val Loss: 41.599246\n",
      "Epoch 60/200 - Train Loss: 42.251302, Val Loss: 41.630990\n",
      "Epoch 61/200 - Train Loss: 42.257606, Val Loss: 41.584178\n",
      "Epoch 62/200 - Train Loss: 42.252436, Val Loss: 41.630647\n",
      "Epoch 63/200 - Train Loss: 42.250213, Val Loss: 41.607921\n",
      "Epoch 64/200 - Train Loss: 42.254670, Val Loss: 41.610098\n",
      "Epoch 65/200 - Train Loss: 42.251200, Val Loss: 41.604435\n",
      "Early stopping triggered after 65 epochs. Best val loss 41.583371 at epoch 50.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 53.186132, Val Loss: 50.705145\n",
      "Epoch 2/200 - Train Loss: 49.704687, Val Loss: 48.809049\n",
      "Epoch 3/200 - Train Loss: 47.988038, Val Loss: 47.548887\n",
      "Epoch 4/200 - Train Loss: 46.797088, Val Loss: 46.587049\n",
      "Epoch 5/200 - Train Loss: 45.897087, Val Loss: 45.833829\n",
      "Epoch 6/200 - Train Loss: 45.194600, Val Loss: 45.196733\n",
      "Epoch 7/200 - Train Loss: 44.632307, Val Loss: 44.693772\n",
      "Epoch 8/200 - Train Loss: 44.187026, Val Loss: 44.311726\n",
      "Epoch 9/200 - Train Loss: 43.830103, Val Loss: 43.939338\n",
      "Epoch 10/200 - Train Loss: 43.536205, Val Loss: 43.662434\n",
      "Epoch 11/200 - Train Loss: 43.299423, Val Loss: 43.470690\n",
      "Epoch 12/200 - Train Loss: 43.106826, Val Loss: 43.240941\n",
      "Epoch 13/200 - Train Loss: 42.944166, Val Loss: 43.047204\n",
      "Epoch 14/200 - Train Loss: 42.813528, Val Loss: 42.914293\n",
      "Epoch 15/200 - Train Loss: 42.707499, Val Loss: 42.819581\n",
      "Epoch 16/200 - Train Loss: 42.615907, Val Loss: 42.713679\n",
      "Epoch 17/200 - Train Loss: 42.538659, Val Loss: 42.614796\n",
      "Epoch 18/200 - Train Loss: 42.477984, Val Loss: 42.529827\n",
      "Epoch 19/200 - Train Loss: 42.426244, Val Loss: 42.464527\n",
      "Epoch 20/200 - Train Loss: 42.384760, Val Loss: 42.405913\n",
      "Epoch 21/200 - Train Loss: 42.343363, Val Loss: 42.354476\n",
      "Epoch 22/200 - Train Loss: 42.313526, Val Loss: 42.329961\n",
      "Epoch 23/200 - Train Loss: 42.291453, Val Loss: 42.284066\n",
      "Epoch 24/200 - Train Loss: 42.268645, Val Loss: 42.247669\n",
      "Epoch 25/200 - Train Loss: 42.251527, Val Loss: 42.231089\n",
      "Epoch 26/200 - Train Loss: 42.236005, Val Loss: 42.176725\n",
      "Epoch 27/200 - Train Loss: 42.225068, Val Loss: 42.174150\n",
      "Epoch 28/200 - Train Loss: 42.212614, Val Loss: 42.163707\n",
      "Epoch 29/200 - Train Loss: 42.205865, Val Loss: 42.134824\n",
      "Epoch 30/200 - Train Loss: 42.198922, Val Loss: 42.133658\n",
      "Epoch 31/200 - Train Loss: 42.192127, Val Loss: 42.137048\n",
      "Epoch 32/200 - Train Loss: 42.185467, Val Loss: 42.124175\n",
      "Epoch 33/200 - Train Loss: 42.184300, Val Loss: 42.092449\n",
      "Epoch 34/200 - Train Loss: 42.182371, Val Loss: 42.098539\n",
      "Epoch 35/200 - Train Loss: 42.179915, Val Loss: 42.059774\n",
      "Epoch 36/200 - Train Loss: 42.178106, Val Loss: 42.087108\n",
      "Epoch 37/200 - Train Loss: 42.173889, Val Loss: 42.049367\n",
      "Epoch 38/200 - Train Loss: 42.173896, Val Loss: 42.049256\n",
      "Epoch 39/200 - Train Loss: 42.172579, Val Loss: 42.061717\n",
      "Epoch 40/200 - Train Loss: 42.173037, Val Loss: 42.063297\n",
      "Epoch 41/200 - Train Loss: 42.167229, Val Loss: 42.044265\n",
      "Epoch 42/200 - Train Loss: 42.169288, Val Loss: 42.055034\n",
      "Epoch 43/200 - Train Loss: 42.169785, Val Loss: 42.066076\n",
      "Epoch 44/200 - Train Loss: 42.173528, Val Loss: 42.051230\n",
      "Epoch 45/200 - Train Loss: 42.174532, Val Loss: 42.054547\n",
      "Epoch 46/200 - Train Loss: 42.173996, Val Loss: 42.014084\n",
      "Epoch 47/200 - Train Loss: 42.170308, Val Loss: 42.057582\n",
      "Epoch 48/200 - Train Loss: 42.165181, Val Loss: 42.069661\n",
      "Epoch 49/200 - Train Loss: 42.169921, Val Loss: 42.051390\n",
      "Epoch 50/200 - Train Loss: 42.170699, Val Loss: 42.032539\n",
      "Epoch 51/200 - Train Loss: 42.173254, Val Loss: 42.065376\n",
      "Epoch 52/200 - Train Loss: 42.174385, Val Loss: 42.032306\n",
      "Epoch 53/200 - Train Loss: 42.167907, Val Loss: 42.019710\n",
      "Epoch 54/200 - Train Loss: 42.171502, Val Loss: 42.057593\n",
      "Epoch 55/200 - Train Loss: 42.170220, Val Loss: 42.031540\n",
      "Epoch 56/200 - Train Loss: 42.168354, Val Loss: 42.010712\n",
      "Epoch 57/200 - Train Loss: 42.172399, Val Loss: 42.027168\n",
      "Epoch 58/200 - Train Loss: 42.172156, Val Loss: 42.031870\n",
      "Epoch 59/200 - Train Loss: 42.173001, Val Loss: 42.034797\n",
      "Epoch 60/200 - Train Loss: 42.170550, Val Loss: 42.048563\n",
      "Epoch 61/200 - Train Loss: 42.175868, Val Loss: 42.041457\n",
      "Epoch 62/200 - Train Loss: 42.167597, Val Loss: 42.045641\n",
      "Epoch 63/200 - Train Loss: 42.169756, Val Loss: 42.061858\n",
      "Epoch 64/200 - Train Loss: 42.173180, Val Loss: 42.018037\n",
      "Epoch 65/200 - Train Loss: 42.168500, Val Loss: 42.042665\n",
      "Epoch 66/200 - Train Loss: 42.170985, Val Loss: 42.098305\n",
      "Epoch 67/200 - Train Loss: 42.172101, Val Loss: 42.023404\n",
      "Epoch 68/200 - Train Loss: 42.173479, Val Loss: 42.005338\n",
      "Epoch 69/200 - Train Loss: 42.172470, Val Loss: 42.006019\n",
      "Epoch 70/200 - Train Loss: 42.170957, Val Loss: 42.036013\n",
      "Epoch 71/200 - Train Loss: 42.169612, Val Loss: 42.022474\n",
      "Epoch 72/200 - Train Loss: 42.171049, Val Loss: 42.043165\n",
      "Epoch 73/200 - Train Loss: 42.170864, Val Loss: 42.023279\n",
      "Epoch 74/200 - Train Loss: 42.171597, Val Loss: 42.010564\n",
      "Epoch 75/200 - Train Loss: 42.171028, Val Loss: 42.026320\n",
      "Epoch 76/200 - Train Loss: 42.171642, Val Loss: 42.033890\n",
      "Epoch 77/200 - Train Loss: 42.171747, Val Loss: 42.061962\n",
      "Epoch 78/200 - Train Loss: 42.172284, Val Loss: 42.015643\n",
      "Epoch 79/200 - Train Loss: 42.170104, Val Loss: 42.083160\n",
      "Epoch 80/200 - Train Loss: 42.168762, Val Loss: 42.044294\n",
      "Epoch 81/200 - Train Loss: 42.173238, Val Loss: 42.054547\n",
      "Epoch 82/200 - Train Loss: 42.171059, Val Loss: 42.033729\n",
      "Epoch 83/200 - Train Loss: 42.170475, Val Loss: 42.035638\n",
      "Early stopping triggered after 83 epochs. Best val loss 42.005338 at epoch 68.\n",
      "  Avg Val Score for config {'learning_rate': 0.001, 'weight_decay': 1e-05}: 42.299900\n",
      "  Testing config: {'learning_rate': 0.001, 'weight_decay': 0.0001}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 53.026138, Val Loss: 51.078767\n",
      "Epoch 2/200 - Train Loss: 49.698773, Val Loss: 48.993292\n",
      "Epoch 3/200 - Train Loss: 48.006872, Val Loss: 47.604279\n",
      "Epoch 4/200 - Train Loss: 46.784751, Val Loss: 46.654639\n",
      "Epoch 5/200 - Train Loss: 45.842093, Val Loss: 45.882594\n",
      "Epoch 6/200 - Train Loss: 45.096481, Val Loss: 45.363819\n",
      "Epoch 7/200 - Train Loss: 44.495602, Val Loss: 44.864072\n",
      "Epoch 8/200 - Train Loss: 44.012989, Val Loss: 44.475917\n",
      "Epoch 9/200 - Train Loss: 43.615132, Val Loss: 44.239538\n",
      "Epoch 10/200 - Train Loss: 43.286879, Val Loss: 44.038196\n",
      "Epoch 11/200 - Train Loss: 43.016000, Val Loss: 43.799532\n",
      "Epoch 12/200 - Train Loss: 42.789241, Val Loss: 43.676248\n",
      "Epoch 13/200 - Train Loss: 42.601058, Val Loss: 43.612171\n",
      "Epoch 14/200 - Train Loss: 42.445667, Val Loss: 43.505009\n",
      "Epoch 15/200 - Train Loss: 42.314688, Val Loss: 43.455079\n",
      "Epoch 16/200 - Train Loss: 42.197051, Val Loss: 43.358382\n",
      "Epoch 17/200 - Train Loss: 42.097957, Val Loss: 43.376339\n",
      "Epoch 18/200 - Train Loss: 42.018341, Val Loss: 43.385371\n",
      "Epoch 19/200 - Train Loss: 41.946983, Val Loss: 43.350247\n",
      "Epoch 20/200 - Train Loss: 41.886309, Val Loss: 43.310999\n",
      "Epoch 21/200 - Train Loss: 41.836237, Val Loss: 43.321795\n",
      "Epoch 22/200 - Train Loss: 41.789278, Val Loss: 43.330298\n",
      "Epoch 23/200 - Train Loss: 41.751962, Val Loss: 43.356448\n",
      "Epoch 24/200 - Train Loss: 41.716665, Val Loss: 43.355573\n",
      "Epoch 25/200 - Train Loss: 41.687940, Val Loss: 43.388632\n",
      "Epoch 26/200 - Train Loss: 41.659765, Val Loss: 43.401095\n",
      "Epoch 27/200 - Train Loss: 41.640207, Val Loss: 43.360653\n",
      "Epoch 28/200 - Train Loss: 41.617673, Val Loss: 43.375639\n",
      "Epoch 29/200 - Train Loss: 41.606131, Val Loss: 43.484518\n",
      "Epoch 30/200 - Train Loss: 41.592187, Val Loss: 43.430738\n",
      "Epoch 31/200 - Train Loss: 41.576232, Val Loss: 43.437561\n",
      "Epoch 32/200 - Train Loss: 41.569826, Val Loss: 43.476773\n",
      "Epoch 33/200 - Train Loss: 41.556359, Val Loss: 43.454156\n",
      "Epoch 34/200 - Train Loss: 41.549630, Val Loss: 43.521252\n",
      "Epoch 35/200 - Train Loss: 41.546357, Val Loss: 43.531757\n",
      "Early stopping triggered after 35 epochs. Best val loss 43.310999 at epoch 20.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 52.953570, Val Loss: 50.943574\n",
      "Epoch 2/200 - Train Loss: 49.616615, Val Loss: 48.852944\n",
      "Epoch 3/200 - Train Loss: 47.962559, Val Loss: 47.466570\n",
      "Epoch 4/200 - Train Loss: 46.804386, Val Loss: 46.374627\n",
      "Epoch 5/200 - Train Loss: 45.920186, Val Loss: 45.541279\n",
      "Epoch 6/200 - Train Loss: 45.229900, Val Loss: 44.870929\n",
      "Epoch 7/200 - Train Loss: 44.683623, Val Loss: 44.330873\n",
      "Epoch 8/200 - Train Loss: 44.248164, Val Loss: 43.930800\n",
      "Epoch 9/200 - Train Loss: 43.898076, Val Loss: 43.536063\n",
      "Epoch 10/200 - Train Loss: 43.611320, Val Loss: 43.234929\n",
      "Epoch 11/200 - Train Loss: 43.378266, Val Loss: 43.033018\n",
      "Epoch 12/200 - Train Loss: 43.189634, Val Loss: 42.811862\n",
      "Epoch 13/200 - Train Loss: 43.032670, Val Loss: 42.614557\n",
      "Epoch 14/200 - Train Loss: 42.906179, Val Loss: 42.486713\n",
      "Epoch 15/200 - Train Loss: 42.799723, Val Loss: 42.358734\n",
      "Epoch 16/200 - Train Loss: 42.708822, Val Loss: 42.291716\n",
      "Epoch 17/200 - Train Loss: 42.632124, Val Loss: 42.171519\n",
      "Epoch 18/200 - Train Loss: 42.573332, Val Loss: 42.082253\n",
      "Epoch 19/200 - Train Loss: 42.524128, Val Loss: 42.021388\n",
      "Epoch 20/200 - Train Loss: 42.478348, Val Loss: 41.969219\n",
      "Epoch 21/200 - Train Loss: 42.443124, Val Loss: 41.936471\n",
      "Epoch 22/200 - Train Loss: 42.409944, Val Loss: 41.881352\n",
      "Epoch 23/200 - Train Loss: 42.386854, Val Loss: 41.841611\n",
      "Epoch 24/200 - Train Loss: 42.362915, Val Loss: 41.802922\n",
      "Epoch 25/200 - Train Loss: 42.346023, Val Loss: 41.842342\n",
      "Epoch 26/200 - Train Loss: 42.331722, Val Loss: 41.755436\n",
      "Epoch 27/200 - Train Loss: 42.317283, Val Loss: 41.749851\n",
      "Epoch 28/200 - Train Loss: 42.305238, Val Loss: 41.726742\n",
      "Epoch 29/200 - Train Loss: 42.298589, Val Loss: 41.695976\n",
      "Epoch 30/200 - Train Loss: 42.290839, Val Loss: 41.717484\n",
      "Epoch 31/200 - Train Loss: 42.282647, Val Loss: 41.671982\n",
      "Epoch 32/200 - Train Loss: 42.278311, Val Loss: 41.692411\n",
      "Epoch 33/200 - Train Loss: 42.272143, Val Loss: 41.686964\n",
      "Epoch 34/200 - Train Loss: 42.272834, Val Loss: 41.640134\n",
      "Epoch 35/200 - Train Loss: 42.268735, Val Loss: 41.642914\n",
      "Epoch 36/200 - Train Loss: 42.267712, Val Loss: 41.682006\n",
      "Epoch 37/200 - Train Loss: 42.263803, Val Loss: 41.639853\n",
      "Epoch 38/200 - Train Loss: 42.261622, Val Loss: 41.616678\n",
      "Epoch 39/200 - Train Loss: 42.263550, Val Loss: 41.621248\n",
      "Epoch 40/200 - Train Loss: 42.259333, Val Loss: 41.623701\n",
      "Epoch 41/200 - Train Loss: 42.258257, Val Loss: 41.625269\n",
      "Epoch 42/200 - Train Loss: 42.257756, Val Loss: 41.611205\n",
      "Epoch 43/200 - Train Loss: 42.255821, Val Loss: 41.614878\n",
      "Epoch 44/200 - Train Loss: 42.258913, Val Loss: 41.646137\n",
      "Epoch 45/200 - Train Loss: 42.260408, Val Loss: 41.607479\n",
      "Epoch 46/200 - Train Loss: 42.259222, Val Loss: 41.618218\n",
      "Epoch 47/200 - Train Loss: 42.253138, Val Loss: 41.608085\n",
      "Epoch 48/200 - Train Loss: 42.253435, Val Loss: 41.613055\n",
      "Epoch 49/200 - Train Loss: 42.254935, Val Loss: 41.612069\n",
      "Epoch 50/200 - Train Loss: 42.256461, Val Loss: 41.583421\n",
      "Epoch 51/200 - Train Loss: 42.259343, Val Loss: 41.626035\n",
      "Epoch 52/200 - Train Loss: 42.257926, Val Loss: 41.608132\n",
      "Epoch 53/200 - Train Loss: 42.255591, Val Loss: 41.595237\n",
      "Epoch 54/200 - Train Loss: 42.256476, Val Loss: 41.633098\n",
      "Epoch 55/200 - Train Loss: 42.253566, Val Loss: 41.605307\n",
      "Epoch 56/200 - Train Loss: 42.253588, Val Loss: 41.599021\n",
      "Epoch 57/200 - Train Loss: 42.258013, Val Loss: 41.612198\n",
      "Epoch 58/200 - Train Loss: 42.257840, Val Loss: 41.628182\n",
      "Epoch 59/200 - Train Loss: 42.255354, Val Loss: 41.599276\n",
      "Epoch 60/200 - Train Loss: 42.251304, Val Loss: 41.631023\n",
      "Epoch 61/200 - Train Loss: 42.257608, Val Loss: 41.584209\n",
      "Epoch 62/200 - Train Loss: 42.252439, Val Loss: 41.630676\n",
      "Epoch 63/200 - Train Loss: 42.250215, Val Loss: 41.607945\n",
      "Epoch 64/200 - Train Loss: 42.254672, Val Loss: 41.610121\n",
      "Epoch 65/200 - Train Loss: 42.251202, Val Loss: 41.604456\n",
      "Early stopping triggered after 65 epochs. Best val loss 41.583421 at epoch 50.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 53.186117, Val Loss: 50.705131\n",
      "Epoch 2/200 - Train Loss: 49.704690, Val Loss: 48.809062\n",
      "Epoch 3/200 - Train Loss: 47.988076, Val Loss: 47.548930\n",
      "Epoch 4/200 - Train Loss: 46.797154, Val Loss: 46.587117\n",
      "Epoch 5/200 - Train Loss: 45.897186, Val Loss: 45.833934\n",
      "Epoch 6/200 - Train Loss: 45.194730, Val Loss: 45.196862\n",
      "Epoch 7/200 - Train Loss: 44.632454, Val Loss: 44.693917\n",
      "Epoch 8/200 - Train Loss: 44.187180, Val Loss: 44.311877\n",
      "Epoch 9/200 - Train Loss: 43.830262, Val Loss: 43.939503\n",
      "Epoch 10/200 - Train Loss: 43.536377, Val Loss: 43.662617\n",
      "Epoch 11/200 - Train Loss: 43.299609, Val Loss: 43.470887\n",
      "Epoch 12/200 - Train Loss: 43.107017, Val Loss: 43.241143\n",
      "Epoch 13/200 - Train Loss: 42.944357, Val Loss: 43.047409\n",
      "Epoch 14/200 - Train Loss: 42.813714, Val Loss: 42.914496\n",
      "Epoch 15/200 - Train Loss: 42.707678, Val Loss: 42.819783\n",
      "Epoch 16/200 - Train Loss: 42.616079, Val Loss: 42.713880\n",
      "Epoch 17/200 - Train Loss: 42.538821, Val Loss: 42.614986\n",
      "Epoch 18/200 - Train Loss: 42.478135, Val Loss: 42.530011\n",
      "Epoch 19/200 - Train Loss: 42.426384, Val Loss: 42.464704\n",
      "Epoch 20/200 - Train Loss: 42.384890, Val Loss: 42.406085\n",
      "Epoch 21/200 - Train Loss: 42.343482, Val Loss: 42.354636\n",
      "Epoch 22/200 - Train Loss: 42.313635, Val Loss: 42.330112\n",
      "Epoch 23/200 - Train Loss: 42.291552, Val Loss: 42.284215\n",
      "Epoch 24/200 - Train Loss: 42.268736, Val Loss: 42.247810\n",
      "Epoch 25/200 - Train Loss: 42.251608, Val Loss: 42.231217\n",
      "Epoch 26/200 - Train Loss: 42.236079, Val Loss: 42.176850\n",
      "Epoch 27/200 - Train Loss: 42.225133, Val Loss: 42.174268\n",
      "Epoch 28/200 - Train Loss: 42.212673, Val Loss: 42.163819\n",
      "Epoch 29/200 - Train Loss: 42.205921, Val Loss: 42.134938\n",
      "Epoch 30/200 - Train Loss: 42.198973, Val Loss: 42.133768\n",
      "Epoch 31/200 - Train Loss: 42.192174, Val Loss: 42.137156\n",
      "Epoch 32/200 - Train Loss: 42.185509, Val Loss: 42.124281\n",
      "Epoch 33/200 - Train Loss: 42.184338, Val Loss: 42.092559\n",
      "Epoch 34/200 - Train Loss: 42.182404, Val Loss: 42.098643\n",
      "Epoch 35/200 - Train Loss: 42.179945, Val Loss: 42.059870\n",
      "Epoch 36/200 - Train Loss: 42.178131, Val Loss: 42.087199\n",
      "Epoch 37/200 - Train Loss: 42.173911, Val Loss: 42.049459\n",
      "Epoch 38/200 - Train Loss: 42.173915, Val Loss: 42.049345\n",
      "Epoch 39/200 - Train Loss: 42.172595, Val Loss: 42.061796\n",
      "Epoch 40/200 - Train Loss: 42.173051, Val Loss: 42.063371\n",
      "Epoch 41/200 - Train Loss: 42.167240, Val Loss: 42.044339\n",
      "Epoch 42/200 - Train Loss: 42.169297, Val Loss: 42.055103\n",
      "Epoch 43/200 - Train Loss: 42.169792, Val Loss: 42.066144\n",
      "Epoch 44/200 - Train Loss: 42.173534, Val Loss: 42.051291\n",
      "Epoch 45/200 - Train Loss: 42.174537, Val Loss: 42.054607\n",
      "Epoch 46/200 - Train Loss: 42.174000, Val Loss: 42.014142\n",
      "Epoch 47/200 - Train Loss: 42.170311, Val Loss: 42.057635\n",
      "Epoch 48/200 - Train Loss: 42.165184, Val Loss: 42.069709\n",
      "Epoch 49/200 - Train Loss: 42.169923, Val Loss: 42.051438\n",
      "Epoch 50/200 - Train Loss: 42.170701, Val Loss: 42.032585\n",
      "Epoch 51/200 - Train Loss: 42.173256, Val Loss: 42.065417\n",
      "Epoch 52/200 - Train Loss: 42.174386, Val Loss: 42.032344\n",
      "Epoch 53/200 - Train Loss: 42.167908, Val Loss: 42.019741\n",
      "Epoch 54/200 - Train Loss: 42.171503, Val Loss: 42.057633\n",
      "Epoch 55/200 - Train Loss: 42.170220, Val Loss: 42.031570\n",
      "Epoch 56/200 - Train Loss: 42.168354, Val Loss: 42.010744\n",
      "Epoch 57/200 - Train Loss: 42.172399, Val Loss: 42.027196\n",
      "Epoch 58/200 - Train Loss: 42.172156, Val Loss: 42.031896\n",
      "Epoch 59/200 - Train Loss: 42.173001, Val Loss: 42.034818\n",
      "Epoch 60/200 - Train Loss: 42.170550, Val Loss: 42.048583\n",
      "Epoch 61/200 - Train Loss: 42.175869, Val Loss: 42.041479\n",
      "Epoch 62/200 - Train Loss: 42.167597, Val Loss: 42.045657\n",
      "Epoch 63/200 - Train Loss: 42.169756, Val Loss: 42.061873\n",
      "Epoch 64/200 - Train Loss: 42.173181, Val Loss: 42.018051\n",
      "Epoch 65/200 - Train Loss: 42.168500, Val Loss: 42.042684\n",
      "Epoch 66/200 - Train Loss: 42.170986, Val Loss: 42.098308\n",
      "Epoch 67/200 - Train Loss: 42.172101, Val Loss: 42.023415\n",
      "Epoch 68/200 - Train Loss: 42.173479, Val Loss: 42.005346\n",
      "Epoch 69/200 - Train Loss: 42.172471, Val Loss: 42.006030\n",
      "Epoch 70/200 - Train Loss: 42.170958, Val Loss: 42.036019\n",
      "Epoch 71/200 - Train Loss: 42.169612, Val Loss: 42.022485\n",
      "Epoch 72/200 - Train Loss: 42.171049, Val Loss: 42.043168\n",
      "Epoch 73/200 - Train Loss: 42.170864, Val Loss: 42.023280\n",
      "Epoch 74/200 - Train Loss: 42.171597, Val Loss: 42.010566\n",
      "Epoch 75/200 - Train Loss: 42.171029, Val Loss: 42.026323\n",
      "Epoch 76/200 - Train Loss: 42.171642, Val Loss: 42.033888\n",
      "Epoch 77/200 - Train Loss: 42.171747, Val Loss: 42.061963\n",
      "Epoch 78/200 - Train Loss: 42.172284, Val Loss: 42.015644\n",
      "Epoch 79/200 - Train Loss: 42.170104, Val Loss: 42.083160\n",
      "Epoch 80/200 - Train Loss: 42.168762, Val Loss: 42.044291\n",
      "Epoch 81/200 - Train Loss: 42.173238, Val Loss: 42.054544\n",
      "Epoch 82/200 - Train Loss: 42.171060, Val Loss: 42.033736\n",
      "Epoch 83/200 - Train Loss: 42.170475, Val Loss: 42.035644\n",
      "Early stopping triggered after 83 epochs. Best val loss 42.005346 at epoch 68.\n",
      "  Avg Val Score for config {'learning_rate': 0.001, 'weight_decay': 0.0001}: 42.299922\n",
      "  Testing config: {'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 56.852308, Val Loss: 55.908414\n",
      "Epoch 2/200 - Train Loss: 54.814100, Val Loss: 54.655990\n",
      "Epoch 3/200 - Train Loss: 53.894083, Val Loss: 53.905132\n",
      "Epoch 4/200 - Train Loss: 53.248553, Val Loss: 53.320170\n",
      "Epoch 5/200 - Train Loss: 52.724035, Val Loss: 52.830719\n",
      "Epoch 6/200 - Train Loss: 52.275897, Val Loss: 52.408116\n",
      "Epoch 7/200 - Train Loss: 51.877878, Val Loss: 52.017943\n",
      "Epoch 8/200 - Train Loss: 51.521027, Val Loss: 51.680044\n",
      "Epoch 9/200 - Train Loss: 51.197664, Val Loss: 51.362495\n",
      "Epoch 10/200 - Train Loss: 50.899886, Val Loss: 51.070599\n",
      "Epoch 11/200 - Train Loss: 50.624775, Val Loss: 50.807211\n",
      "Epoch 12/200 - Train Loss: 50.366766, Val Loss: 50.547942\n",
      "Epoch 13/200 - Train Loss: 50.123208, Val Loss: 50.308566\n",
      "Epoch 14/200 - Train Loss: 49.895598, Val Loss: 50.089160\n",
      "Epoch 15/200 - Train Loss: 49.677986, Val Loss: 49.872505\n",
      "Epoch 16/200 - Train Loss: 49.469726, Val Loss: 49.667104\n",
      "Epoch 17/200 - Train Loss: 49.271560, Val Loss: 49.471649\n",
      "Epoch 18/200 - Train Loss: 49.081328, Val Loss: 49.285959\n",
      "Epoch 19/200 - Train Loss: 48.899347, Val Loss: 49.114455\n",
      "Epoch 20/200 - Train Loss: 48.722907, Val Loss: 48.943259\n",
      "Epoch 21/200 - Train Loss: 48.551846, Val Loss: 48.773072\n",
      "Epoch 22/200 - Train Loss: 48.387736, Val Loss: 48.625578\n",
      "Epoch 23/200 - Train Loss: 48.228268, Val Loss: 48.473624\n",
      "Epoch 24/200 - Train Loss: 48.073220, Val Loss: 48.321471\n",
      "Epoch 25/200 - Train Loss: 47.924860, Val Loss: 48.168904\n",
      "Epoch 26/200 - Train Loss: 47.775853, Val Loss: 48.038103\n",
      "Epoch 27/200 - Train Loss: 47.634482, Val Loss: 47.905032\n",
      "Epoch 28/200 - Train Loss: 47.495027, Val Loss: 47.763259\n",
      "Epoch 29/200 - Train Loss: 47.359910, Val Loss: 47.636616\n",
      "Epoch 30/200 - Train Loss: 47.229350, Val Loss: 47.524983\n",
      "Epoch 31/200 - Train Loss: 47.099561, Val Loss: 47.403921\n",
      "Epoch 32/200 - Train Loss: 46.974888, Val Loss: 47.293469\n",
      "Epoch 33/200 - Train Loss: 46.852136, Val Loss: 47.168163\n",
      "Epoch 34/200 - Train Loss: 46.731101, Val Loss: 47.052724\n",
      "Epoch 35/200 - Train Loss: 46.613881, Val Loss: 46.947134\n",
      "Epoch 36/200 - Train Loss: 46.498506, Val Loss: 46.839999\n",
      "Epoch 37/200 - Train Loss: 46.386724, Val Loss: 46.744700\n",
      "Epoch 38/200 - Train Loss: 46.274671, Val Loss: 46.629072\n",
      "Epoch 39/200 - Train Loss: 46.167824, Val Loss: 46.536656\n",
      "Epoch 40/200 - Train Loss: 46.062788, Val Loss: 46.439853\n",
      "Epoch 41/200 - Train Loss: 45.959215, Val Loss: 46.345594\n",
      "Epoch 42/200 - Train Loss: 45.857437, Val Loss: 46.266030\n",
      "Epoch 43/200 - Train Loss: 45.757311, Val Loss: 46.152600\n",
      "Epoch 44/200 - Train Loss: 45.660965, Val Loss: 46.091357\n",
      "Epoch 45/200 - Train Loss: 45.565808, Val Loss: 45.988633\n",
      "Epoch 46/200 - Train Loss: 45.473274, Val Loss: 45.910679\n",
      "Epoch 47/200 - Train Loss: 45.380613, Val Loss: 45.828524\n",
      "Epoch 48/200 - Train Loss: 45.290795, Val Loss: 45.747132\n",
      "Epoch 49/200 - Train Loss: 45.202996, Val Loss: 45.672589\n",
      "Epoch 50/200 - Train Loss: 45.115510, Val Loss: 45.581850\n",
      "Epoch 51/200 - Train Loss: 45.030890, Val Loss: 45.511568\n",
      "Epoch 52/200 - Train Loss: 44.948699, Val Loss: 45.450860\n",
      "Epoch 53/200 - Train Loss: 44.867945, Val Loss: 45.372863\n",
      "Epoch 54/200 - Train Loss: 44.788716, Val Loss: 45.303359\n",
      "Epoch 55/200 - Train Loss: 44.709347, Val Loss: 45.239953\n",
      "Epoch 56/200 - Train Loss: 44.633304, Val Loss: 45.191962\n",
      "Epoch 57/200 - Train Loss: 44.557965, Val Loss: 45.130649\n",
      "Epoch 58/200 - Train Loss: 44.485409, Val Loss: 45.027456\n",
      "Epoch 59/200 - Train Loss: 44.412575, Val Loss: 44.983205\n",
      "Epoch 60/200 - Train Loss: 44.342105, Val Loss: 44.933745\n",
      "Epoch 61/200 - Train Loss: 44.272967, Val Loss: 44.860945\n",
      "Epoch 62/200 - Train Loss: 44.204786, Val Loss: 44.806580\n",
      "Epoch 63/200 - Train Loss: 44.138505, Val Loss: 44.745574\n",
      "Epoch 64/200 - Train Loss: 44.072981, Val Loss: 44.698592\n",
      "Epoch 65/200 - Train Loss: 44.009463, Val Loss: 44.653340\n",
      "Epoch 66/200 - Train Loss: 43.946776, Val Loss: 44.606915\n",
      "Epoch 67/200 - Train Loss: 43.885702, Val Loss: 44.561539\n",
      "Epoch 68/200 - Train Loss: 43.825784, Val Loss: 44.501820\n",
      "Epoch 69/200 - Train Loss: 43.766614, Val Loss: 44.450597\n",
      "Epoch 70/200 - Train Loss: 43.709221, Val Loss: 44.400167\n",
      "Epoch 71/200 - Train Loss: 43.653174, Val Loss: 44.355221\n",
      "Epoch 72/200 - Train Loss: 43.597994, Val Loss: 44.320343\n",
      "Epoch 73/200 - Train Loss: 43.544237, Val Loss: 44.277072\n",
      "Epoch 74/200 - Train Loss: 43.491313, Val Loss: 44.239827\n",
      "Epoch 75/200 - Train Loss: 43.439469, Val Loss: 44.192451\n",
      "Epoch 76/200 - Train Loss: 43.388777, Val Loss: 44.145070\n",
      "Epoch 77/200 - Train Loss: 43.339418, Val Loss: 44.122781\n",
      "Epoch 78/200 - Train Loss: 43.290943, Val Loss: 44.097853\n",
      "Epoch 79/200 - Train Loss: 43.244172, Val Loss: 44.057685\n",
      "Epoch 80/200 - Train Loss: 43.198137, Val Loss: 44.009943\n",
      "Epoch 81/200 - Train Loss: 43.152250, Val Loss: 43.983826\n",
      "Epoch 82/200 - Train Loss: 43.107847, Val Loss: 43.944210\n",
      "Epoch 83/200 - Train Loss: 43.064809, Val Loss: 43.910138\n",
      "Epoch 84/200 - Train Loss: 43.021845, Val Loss: 43.880059\n",
      "Epoch 85/200 - Train Loss: 42.980269, Val Loss: 43.857549\n",
      "Epoch 86/200 - Train Loss: 42.939751, Val Loss: 43.823445\n",
      "Epoch 87/200 - Train Loss: 42.900246, Val Loss: 43.789617\n",
      "Epoch 88/200 - Train Loss: 42.861474, Val Loss: 43.766124\n",
      "Epoch 89/200 - Train Loss: 42.823404, Val Loss: 43.745934\n",
      "Epoch 90/200 - Train Loss: 42.787173, Val Loss: 43.716370\n",
      "Epoch 91/200 - Train Loss: 42.750277, Val Loss: 43.689851\n",
      "Epoch 92/200 - Train Loss: 42.715957, Val Loss: 43.677772\n",
      "Epoch 93/200 - Train Loss: 42.679798, Val Loss: 43.654484\n",
      "Epoch 94/200 - Train Loss: 42.646784, Val Loss: 43.622220\n",
      "Epoch 95/200 - Train Loss: 42.613849, Val Loss: 43.615140\n",
      "Epoch 96/200 - Train Loss: 42.581576, Val Loss: 43.594879\n",
      "Epoch 97/200 - Train Loss: 42.550701, Val Loss: 43.571178\n",
      "Epoch 98/200 - Train Loss: 42.519720, Val Loss: 43.558675\n",
      "Epoch 99/200 - Train Loss: 42.489863, Val Loss: 43.527214\n",
      "Epoch 100/200 - Train Loss: 42.460089, Val Loss: 43.512439\n",
      "Epoch 101/200 - Train Loss: 42.431958, Val Loss: 43.512447\n",
      "Epoch 102/200 - Train Loss: 42.405053, Val Loss: 43.489096\n",
      "Epoch 103/200 - Train Loss: 42.376982, Val Loss: 43.468764\n",
      "Epoch 104/200 - Train Loss: 42.351017, Val Loss: 43.461292\n",
      "Epoch 105/200 - Train Loss: 42.324633, Val Loss: 43.449084\n",
      "Epoch 106/200 - Train Loss: 42.299195, Val Loss: 43.430117\n",
      "Epoch 107/200 - Train Loss: 42.274469, Val Loss: 43.428437\n",
      "Epoch 108/200 - Train Loss: 42.251066, Val Loss: 43.399939\n",
      "Epoch 109/200 - Train Loss: 42.227187, Val Loss: 43.395505\n",
      "Epoch 110/200 - Train Loss: 42.204348, Val Loss: 43.383881\n",
      "Epoch 111/200 - Train Loss: 42.182042, Val Loss: 43.378217\n",
      "Epoch 112/200 - Train Loss: 42.160256, Val Loss: 43.371379\n",
      "Epoch 113/200 - Train Loss: 42.139227, Val Loss: 43.357259\n",
      "Epoch 114/200 - Train Loss: 42.118234, Val Loss: 43.354301\n",
      "Epoch 115/200 - Train Loss: 42.098410, Val Loss: 43.337103\n",
      "Epoch 116/200 - Train Loss: 42.078925, Val Loss: 43.321904\n",
      "Epoch 117/200 - Train Loss: 42.060012, Val Loss: 43.334576\n",
      "Epoch 118/200 - Train Loss: 42.041463, Val Loss: 43.322942\n",
      "Epoch 119/200 - Train Loss: 42.022975, Val Loss: 43.312936\n",
      "Epoch 120/200 - Train Loss: 42.006062, Val Loss: 43.319140\n",
      "Epoch 121/200 - Train Loss: 41.988764, Val Loss: 43.303043\n",
      "Epoch 122/200 - Train Loss: 41.972037, Val Loss: 43.298453\n",
      "Epoch 123/200 - Train Loss: 41.956004, Val Loss: 43.308535\n",
      "Epoch 124/200 - Train Loss: 41.939877, Val Loss: 43.281800\n",
      "Epoch 125/200 - Train Loss: 41.924630, Val Loss: 43.297325\n",
      "Epoch 126/200 - Train Loss: 41.909964, Val Loss: 43.289938\n",
      "Epoch 127/200 - Train Loss: 41.895988, Val Loss: 43.279205\n",
      "Epoch 128/200 - Train Loss: 41.881354, Val Loss: 43.292570\n",
      "Epoch 129/200 - Train Loss: 41.868354, Val Loss: 43.292975\n",
      "Epoch 130/200 - Train Loss: 41.855044, Val Loss: 43.280629\n",
      "Epoch 131/200 - Train Loss: 41.842330, Val Loss: 43.276566\n",
      "Epoch 132/200 - Train Loss: 41.829147, Val Loss: 43.285695\n",
      "Epoch 133/200 - Train Loss: 41.817390, Val Loss: 43.276085\n",
      "Epoch 134/200 - Train Loss: 41.805575, Val Loss: 43.283660\n",
      "Epoch 135/200 - Train Loss: 41.794292, Val Loss: 43.275380\n",
      "Epoch 136/200 - Train Loss: 41.782771, Val Loss: 43.287347\n",
      "Epoch 137/200 - Train Loss: 41.772452, Val Loss: 43.280802\n",
      "Epoch 138/200 - Train Loss: 41.762141, Val Loss: 43.290184\n",
      "Epoch 139/200 - Train Loss: 41.752018, Val Loss: 43.284078\n",
      "Epoch 140/200 - Train Loss: 41.741651, Val Loss: 43.301373\n",
      "Epoch 141/200 - Train Loss: 41.732394, Val Loss: 43.291849\n",
      "Epoch 142/200 - Train Loss: 41.723439, Val Loss: 43.296530\n",
      "Epoch 143/200 - Train Loss: 41.713681, Val Loss: 43.296250\n",
      "Epoch 144/200 - Train Loss: 41.705158, Val Loss: 43.293863\n",
      "Epoch 145/200 - Train Loss: 41.697277, Val Loss: 43.294767\n",
      "Epoch 146/200 - Train Loss: 41.688601, Val Loss: 43.299282\n",
      "Epoch 147/200 - Train Loss: 41.681304, Val Loss: 43.295948\n",
      "Epoch 148/200 - Train Loss: 41.673412, Val Loss: 43.301483\n",
      "Epoch 149/200 - Train Loss: 41.665650, Val Loss: 43.307707\n",
      "Epoch 150/200 - Train Loss: 41.658644, Val Loss: 43.328483\n",
      "Early stopping triggered after 150 epochs. Best val loss 43.275380 at epoch 135.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 56.816713, Val Loss: 55.722200\n",
      "Epoch 2/200 - Train Loss: 54.778458, Val Loss: 54.515932\n",
      "Epoch 3/200 - Train Loss: 53.844412, Val Loss: 53.785892\n",
      "Epoch 4/200 - Train Loss: 53.185596, Val Loss: 53.205970\n",
      "Epoch 5/200 - Train Loss: 52.651913, Val Loss: 52.721145\n",
      "Epoch 6/200 - Train Loss: 52.198429, Val Loss: 52.292433\n",
      "Epoch 7/200 - Train Loss: 51.797466, Val Loss: 51.916610\n",
      "Epoch 8/200 - Train Loss: 51.439439, Val Loss: 51.568709\n",
      "Epoch 9/200 - Train Loss: 51.115172, Val Loss: 51.251169\n",
      "Epoch 10/200 - Train Loss: 50.817318, Val Loss: 50.959713\n",
      "Epoch 11/200 - Train Loss: 50.542131, Val Loss: 50.686267\n",
      "Epoch 12/200 - Train Loss: 50.284152, Val Loss: 50.430445\n",
      "Epoch 13/200 - Train Loss: 50.042155, Val Loss: 50.193484\n",
      "Epoch 14/200 - Train Loss: 49.815907, Val Loss: 49.964079\n",
      "Epoch 15/200 - Train Loss: 49.600592, Val Loss: 49.745650\n",
      "Epoch 16/200 - Train Loss: 49.393842, Val Loss: 49.543278\n",
      "Epoch 17/200 - Train Loss: 49.198586, Val Loss: 49.344441\n",
      "Epoch 18/200 - Train Loss: 49.011462, Val Loss: 49.154487\n",
      "Epoch 19/200 - Train Loss: 48.832305, Val Loss: 48.971034\n",
      "Epoch 20/200 - Train Loss: 48.660334, Val Loss: 48.795313\n",
      "Epoch 21/200 - Train Loss: 48.493380, Val Loss: 48.630385\n",
      "Epoch 22/200 - Train Loss: 48.333158, Val Loss: 48.460863\n",
      "Epoch 23/200 - Train Loss: 48.178211, Val Loss: 48.303439\n",
      "Epoch 24/200 - Train Loss: 48.028447, Val Loss: 48.149280\n",
      "Epoch 25/200 - Train Loss: 47.885000, Val Loss: 48.008156\n",
      "Epoch 26/200 - Train Loss: 47.741679, Val Loss: 47.854492\n",
      "Epoch 27/200 - Train Loss: 47.605830, Val Loss: 47.712122\n",
      "Epoch 28/200 - Train Loss: 47.471727, Val Loss: 47.574829\n",
      "Epoch 29/200 - Train Loss: 47.342282, Val Loss: 47.444392\n",
      "Epoch 30/200 - Train Loss: 47.216778, Val Loss: 47.309105\n",
      "Epoch 31/200 - Train Loss: 47.092897, Val Loss: 47.181477\n",
      "Epoch 32/200 - Train Loss: 46.973685, Val Loss: 47.053267\n",
      "Epoch 33/200 - Train Loss: 46.857171, Val Loss: 46.930995\n",
      "Epoch 34/200 - Train Loss: 46.740805, Val Loss: 46.813044\n",
      "Epoch 35/200 - Train Loss: 46.629761, Val Loss: 46.694169\n",
      "Epoch 36/200 - Train Loss: 46.520068, Val Loss: 46.576971\n",
      "Epoch 37/200 - Train Loss: 46.414143, Val Loss: 46.462756\n",
      "Epoch 38/200 - Train Loss: 46.307720, Val Loss: 46.357640\n",
      "Epoch 39/200 - Train Loss: 46.207027, Val Loss: 46.244823\n",
      "Epoch 40/200 - Train Loss: 46.107920, Val Loss: 46.141363\n",
      "Epoch 41/200 - Train Loss: 46.009693, Val Loss: 46.035625\n",
      "Epoch 42/200 - Train Loss: 45.914049, Val Loss: 45.929059\n",
      "Epoch 43/200 - Train Loss: 45.819677, Val Loss: 45.843448\n",
      "Epoch 44/200 - Train Loss: 45.729761, Val Loss: 45.732417\n",
      "Epoch 45/200 - Train Loss: 45.640001, Val Loss: 45.638640\n",
      "Epoch 46/200 - Train Loss: 45.553316, Val Loss: 45.544342\n",
      "Epoch 47/200 - Train Loss: 45.466699, Val Loss: 45.452076\n",
      "Epoch 48/200 - Train Loss: 45.382935, Val Loss: 45.366450\n",
      "Epoch 49/200 - Train Loss: 45.300979, Val Loss: 45.273159\n",
      "Epoch 50/200 - Train Loss: 45.219972, Val Loss: 45.190902\n",
      "Epoch 51/200 - Train Loss: 45.141704, Val Loss: 45.110525\n",
      "Epoch 52/200 - Train Loss: 45.065111, Val Loss: 45.019705\n",
      "Epoch 53/200 - Train Loss: 44.990459, Val Loss: 44.938933\n",
      "Epoch 54/200 - Train Loss: 44.917141, Val Loss: 44.858027\n",
      "Epoch 55/200 - Train Loss: 44.844320, Val Loss: 44.777751\n",
      "Epoch 56/200 - Train Loss: 44.773769, Val Loss: 44.694018\n",
      "Epoch 57/200 - Train Loss: 44.705538, Val Loss: 44.619026\n",
      "Epoch 58/200 - Train Loss: 44.638617, Val Loss: 44.560583\n",
      "Epoch 59/200 - Train Loss: 44.571800, Val Loss: 44.480076\n",
      "Epoch 60/200 - Train Loss: 44.507823, Val Loss: 44.407975\n",
      "Epoch 61/200 - Train Loss: 44.444627, Val Loss: 44.339150\n",
      "Epoch 62/200 - Train Loss: 44.383046, Val Loss: 44.269548\n",
      "Epoch 63/200 - Train Loss: 44.322675, Val Loss: 44.205364\n",
      "Epoch 64/200 - Train Loss: 44.263384, Val Loss: 44.139871\n",
      "Epoch 65/200 - Train Loss: 44.205811, Val Loss: 44.068429\n",
      "Epoch 66/200 - Train Loss: 44.149536, Val Loss: 44.006793\n",
      "Epoch 67/200 - Train Loss: 44.094443, Val Loss: 43.947680\n",
      "Epoch 68/200 - Train Loss: 44.040821, Val Loss: 43.890593\n",
      "Epoch 69/200 - Train Loss: 43.988080, Val Loss: 43.827661\n",
      "Epoch 70/200 - Train Loss: 43.936914, Val Loss: 43.776064\n",
      "Epoch 71/200 - Train Loss: 43.887187, Val Loss: 43.718666\n",
      "Epoch 72/200 - Train Loss: 43.837759, Val Loss: 43.657900\n",
      "Epoch 73/200 - Train Loss: 43.790186, Val Loss: 43.603423\n",
      "Epoch 74/200 - Train Loss: 43.743697, Val Loss: 43.547816\n",
      "Epoch 75/200 - Train Loss: 43.697767, Val Loss: 43.499998\n",
      "Epoch 76/200 - Train Loss: 43.653712, Val Loss: 43.457450\n",
      "Epoch 77/200 - Train Loss: 43.610214, Val Loss: 43.399586\n",
      "Epoch 78/200 - Train Loss: 43.567583, Val Loss: 43.345982\n",
      "Epoch 79/200 - Train Loss: 43.527417, Val Loss: 43.303953\n",
      "Epoch 80/200 - Train Loss: 43.487435, Val Loss: 43.256583\n",
      "Epoch 81/200 - Train Loss: 43.447476, Val Loss: 43.206938\n",
      "Epoch 82/200 - Train Loss: 43.409008, Val Loss: 43.168044\n",
      "Epoch 83/200 - Train Loss: 43.371604, Val Loss: 43.127455\n",
      "Epoch 84/200 - Train Loss: 43.334893, Val Loss: 43.078816\n",
      "Epoch 85/200 - Train Loss: 43.299394, Val Loss: 43.039351\n",
      "Epoch 86/200 - Train Loss: 43.265165, Val Loss: 43.002300\n",
      "Epoch 87/200 - Train Loss: 43.231645, Val Loss: 42.960284\n",
      "Epoch 88/200 - Train Loss: 43.198837, Val Loss: 42.924769\n",
      "Epoch 89/200 - Train Loss: 43.166846, Val Loss: 42.879923\n",
      "Epoch 90/200 - Train Loss: 43.136113, Val Loss: 42.841650\n",
      "Epoch 91/200 - Train Loss: 43.105140, Val Loss: 42.809165\n",
      "Epoch 92/200 - Train Loss: 43.076913, Val Loss: 42.771259\n",
      "Epoch 93/200 - Train Loss: 43.046756, Val Loss: 42.737830\n",
      "Epoch 94/200 - Train Loss: 43.019503, Val Loss: 42.704192\n",
      "Epoch 95/200 - Train Loss: 42.991941, Val Loss: 42.673385\n",
      "Epoch 96/200 - Train Loss: 42.965914, Val Loss: 42.637383\n",
      "Epoch 97/200 - Train Loss: 42.940131, Val Loss: 42.610519\n",
      "Epoch 98/200 - Train Loss: 42.915015, Val Loss: 42.575755\n",
      "Epoch 99/200 - Train Loss: 42.891355, Val Loss: 42.548520\n",
      "Epoch 100/200 - Train Loss: 42.867424, Val Loss: 42.518049\n",
      "Epoch 101/200 - Train Loss: 42.844838, Val Loss: 42.487482\n",
      "Epoch 102/200 - Train Loss: 42.822897, Val Loss: 42.460446\n",
      "Epoch 103/200 - Train Loss: 42.801021, Val Loss: 42.438290\n",
      "Epoch 104/200 - Train Loss: 42.780475, Val Loss: 42.409535\n",
      "Epoch 105/200 - Train Loss: 42.759731, Val Loss: 42.385151\n",
      "Epoch 106/200 - Train Loss: 42.739897, Val Loss: 42.357657\n",
      "Epoch 107/200 - Train Loss: 42.720590, Val Loss: 42.334302\n",
      "Epoch 108/200 - Train Loss: 42.702491, Val Loss: 42.314398\n",
      "Epoch 109/200 - Train Loss: 42.683837, Val Loss: 42.294934\n",
      "Epoch 110/200 - Train Loss: 42.666984, Val Loss: 42.269824\n",
      "Epoch 111/200 - Train Loss: 42.649841, Val Loss: 42.241859\n",
      "Epoch 112/200 - Train Loss: 42.633011, Val Loss: 42.224782\n",
      "Epoch 113/200 - Train Loss: 42.617401, Val Loss: 42.203174\n",
      "Epoch 114/200 - Train Loss: 42.601749, Val Loss: 42.181566\n",
      "Epoch 115/200 - Train Loss: 42.587202, Val Loss: 42.161505\n",
      "Epoch 116/200 - Train Loss: 42.572541, Val Loss: 42.151657\n",
      "Epoch 117/200 - Train Loss: 42.559336, Val Loss: 42.122729\n",
      "Epoch 118/200 - Train Loss: 42.545380, Val Loss: 42.112680\n",
      "Epoch 119/200 - Train Loss: 42.532472, Val Loss: 42.091658\n",
      "Epoch 120/200 - Train Loss: 42.520088, Val Loss: 42.070632\n",
      "Epoch 121/200 - Train Loss: 42.507750, Val Loss: 42.055488\n",
      "Epoch 122/200 - Train Loss: 42.495983, Val Loss: 42.037362\n",
      "Epoch 123/200 - Train Loss: 42.485162, Val Loss: 42.020610\n",
      "Epoch 124/200 - Train Loss: 42.473206, Val Loss: 42.010493\n",
      "Epoch 125/200 - Train Loss: 42.463213, Val Loss: 41.988903\n",
      "Epoch 126/200 - Train Loss: 42.453274, Val Loss: 41.977476\n",
      "Epoch 127/200 - Train Loss: 42.443303, Val Loss: 41.964915\n",
      "Epoch 128/200 - Train Loss: 42.433251, Val Loss: 41.950388\n",
      "Epoch 129/200 - Train Loss: 42.424352, Val Loss: 41.938700\n",
      "Epoch 130/200 - Train Loss: 42.416275, Val Loss: 41.927893\n",
      "Epoch 131/200 - Train Loss: 42.407648, Val Loss: 41.919204\n",
      "Epoch 132/200 - Train Loss: 42.399219, Val Loss: 41.903204\n",
      "Epoch 133/200 - Train Loss: 42.391644, Val Loss: 41.890347\n",
      "Epoch 134/200 - Train Loss: 42.384201, Val Loss: 41.881531\n",
      "Epoch 135/200 - Train Loss: 42.376567, Val Loss: 41.873485\n",
      "Epoch 136/200 - Train Loss: 42.369589, Val Loss: 41.857882\n",
      "Epoch 137/200 - Train Loss: 42.363424, Val Loss: 41.844337\n",
      "Epoch 138/200 - Train Loss: 42.357094, Val Loss: 41.832552\n",
      "Epoch 139/200 - Train Loss: 42.350929, Val Loss: 41.829221\n",
      "Epoch 140/200 - Train Loss: 42.344451, Val Loss: 41.814707\n",
      "Epoch 141/200 - Train Loss: 42.339069, Val Loss: 41.813286\n",
      "Epoch 142/200 - Train Loss: 42.333758, Val Loss: 41.803019\n",
      "Epoch 143/200 - Train Loss: 42.327865, Val Loss: 41.796363\n",
      "Epoch 144/200 - Train Loss: 42.322877, Val Loss: 41.792374\n",
      "Epoch 145/200 - Train Loss: 42.318666, Val Loss: 41.778735\n",
      "Epoch 146/200 - Train Loss: 42.313723, Val Loss: 41.775623\n",
      "Epoch 147/200 - Train Loss: 42.309473, Val Loss: 41.767258\n",
      "Epoch 148/200 - Train Loss: 42.305121, Val Loss: 41.762195\n",
      "Epoch 149/200 - Train Loss: 42.300959, Val Loss: 41.752548\n",
      "Epoch 150/200 - Train Loss: 42.296698, Val Loss: 41.740637\n",
      "Epoch 151/200 - Train Loss: 42.293528, Val Loss: 41.735396\n",
      "Epoch 152/200 - Train Loss: 42.289968, Val Loss: 41.729886\n",
      "Epoch 153/200 - Train Loss: 42.286236, Val Loss: 41.725945\n",
      "Epoch 154/200 - Train Loss: 42.283821, Val Loss: 41.715884\n",
      "Epoch 155/200 - Train Loss: 42.280155, Val Loss: 41.714149\n",
      "Epoch 156/200 - Train Loss: 42.278136, Val Loss: 41.711994\n",
      "Epoch 157/200 - Train Loss: 42.274350, Val Loss: 41.703207\n",
      "Epoch 158/200 - Train Loss: 42.272441, Val Loss: 41.702727\n",
      "Epoch 159/200 - Train Loss: 42.269579, Val Loss: 41.690911\n",
      "Epoch 160/200 - Train Loss: 42.267872, Val Loss: 41.694500\n",
      "Epoch 161/200 - Train Loss: 42.264675, Val Loss: 41.687157\n",
      "Epoch 162/200 - Train Loss: 42.263129, Val Loss: 41.676573\n",
      "Epoch 163/200 - Train Loss: 42.261014, Val Loss: 41.678973\n",
      "Epoch 164/200 - Train Loss: 42.258696, Val Loss: 41.672096\n",
      "Epoch 165/200 - Train Loss: 42.256619, Val Loss: 41.665692\n",
      "Epoch 166/200 - Train Loss: 42.255017, Val Loss: 41.671971\n",
      "Epoch 167/200 - Train Loss: 42.253607, Val Loss: 41.657566\n",
      "Epoch 168/200 - Train Loss: 42.252294, Val Loss: 41.661133\n",
      "Epoch 169/200 - Train Loss: 42.250684, Val Loss: 41.653678\n",
      "Epoch 170/200 - Train Loss: 42.248916, Val Loss: 41.653835\n",
      "Epoch 171/200 - Train Loss: 42.248194, Val Loss: 41.644176\n",
      "Epoch 172/200 - Train Loss: 42.246420, Val Loss: 41.644135\n",
      "Epoch 173/200 - Train Loss: 42.245433, Val Loss: 41.643400\n",
      "Epoch 174/200 - Train Loss: 42.244200, Val Loss: 41.642430\n",
      "Epoch 175/200 - Train Loss: 42.243193, Val Loss: 41.643801\n",
      "Epoch 176/200 - Train Loss: 42.242195, Val Loss: 41.639499\n",
      "Epoch 177/200 - Train Loss: 42.241426, Val Loss: 41.631868\n",
      "Epoch 178/200 - Train Loss: 42.240862, Val Loss: 41.628935\n",
      "Epoch 179/200 - Train Loss: 42.239406, Val Loss: 41.623999\n",
      "Epoch 180/200 - Train Loss: 42.238880, Val Loss: 41.623736\n",
      "Epoch 181/200 - Train Loss: 42.238075, Val Loss: 41.622591\n",
      "Epoch 182/200 - Train Loss: 42.237616, Val Loss: 41.623344\n",
      "Epoch 183/200 - Train Loss: 42.237139, Val Loss: 41.615227\n",
      "Epoch 184/200 - Train Loss: 42.236580, Val Loss: 41.617507\n",
      "Epoch 185/200 - Train Loss: 42.236188, Val Loss: 41.618723\n",
      "Epoch 186/200 - Train Loss: 42.236251, Val Loss: 41.612398\n",
      "Epoch 187/200 - Train Loss: 42.234806, Val Loss: 41.612972\n",
      "Epoch 188/200 - Train Loss: 42.235467, Val Loss: 41.608890\n",
      "Epoch 189/200 - Train Loss: 42.234585, Val Loss: 41.608835\n",
      "Epoch 190/200 - Train Loss: 42.234046, Val Loss: 41.604328\n",
      "Epoch 191/200 - Train Loss: 42.233566, Val Loss: 41.610534\n",
      "Epoch 192/200 - Train Loss: 42.232990, Val Loss: 41.610148\n",
      "Epoch 193/200 - Train Loss: 42.233562, Val Loss: 41.608098\n",
      "Epoch 194/200 - Train Loss: 42.232901, Val Loss: 41.604529\n",
      "Epoch 195/200 - Train Loss: 42.232803, Val Loss: 41.600924\n",
      "Epoch 196/200 - Train Loss: 42.232256, Val Loss: 41.605445\n",
      "Epoch 197/200 - Train Loss: 42.232425, Val Loss: 41.600892\n",
      "Epoch 198/200 - Train Loss: 42.232442, Val Loss: 41.600240\n",
      "Epoch 199/200 - Train Loss: 42.231566, Val Loss: 41.593857\n",
      "Epoch 200/200 - Train Loss: 42.231941, Val Loss: 41.588790\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 57.156166, Val Loss: 55.318243\n",
      "Epoch 2/200 - Train Loss: 55.039244, Val Loss: 54.138834\n",
      "Epoch 3/200 - Train Loss: 54.091160, Val Loss: 53.424156\n",
      "Epoch 4/200 - Train Loss: 53.423358, Val Loss: 52.859311\n",
      "Epoch 5/200 - Train Loss: 52.876531, Val Loss: 52.388842\n",
      "Epoch 6/200 - Train Loss: 52.408380, Val Loss: 51.974056\n",
      "Epoch 7/200 - Train Loss: 51.991319, Val Loss: 51.618106\n",
      "Epoch 8/200 - Train Loss: 51.618875, Val Loss: 51.287865\n",
      "Epoch 9/200 - Train Loss: 51.280604, Val Loss: 50.997194\n",
      "Epoch 10/200 - Train Loss: 50.969216, Val Loss: 50.718552\n",
      "Epoch 11/200 - Train Loss: 50.681300, Val Loss: 50.464441\n",
      "Epoch 12/200 - Train Loss: 50.411247, Val Loss: 50.229009\n",
      "Epoch 13/200 - Train Loss: 50.157469, Val Loss: 50.011960\n",
      "Epoch 14/200 - Train Loss: 49.920462, Val Loss: 49.804284\n",
      "Epoch 15/200 - Train Loss: 49.694969, Val Loss: 49.605039\n",
      "Epoch 16/200 - Train Loss: 49.480448, Val Loss: 49.420049\n",
      "Epoch 17/200 - Train Loss: 49.275700, Val Loss: 49.244648\n",
      "Epoch 18/200 - Train Loss: 49.080545, Val Loss: 49.069681\n",
      "Epoch 19/200 - Train Loss: 48.894290, Val Loss: 48.911325\n",
      "Epoch 20/200 - Train Loss: 48.715347, Val Loss: 48.754623\n",
      "Epoch 21/200 - Train Loss: 48.542135, Val Loss: 48.602117\n",
      "Epoch 22/200 - Train Loss: 48.375994, Val Loss: 48.454495\n",
      "Epoch 23/200 - Train Loss: 48.215655, Val Loss: 48.310434\n",
      "Epoch 24/200 - Train Loss: 48.061147, Val Loss: 48.177362\n",
      "Epoch 25/200 - Train Loss: 47.913285, Val Loss: 48.048611\n",
      "Epoch 26/200 - Train Loss: 47.766176, Val Loss: 47.918944\n",
      "Epoch 27/200 - Train Loss: 47.626502, Val Loss: 47.790941\n",
      "Epoch 28/200 - Train Loss: 47.488531, Val Loss: 47.664167\n",
      "Epoch 29/200 - Train Loss: 47.356316, Val Loss: 47.549152\n",
      "Epoch 30/200 - Train Loss: 47.228141, Val Loss: 47.430771\n",
      "Epoch 31/200 - Train Loss: 47.101462, Val Loss: 47.323829\n",
      "Epoch 32/200 - Train Loss: 46.979853, Val Loss: 47.198512\n",
      "Epoch 33/200 - Train Loss: 46.860912, Val Loss: 47.093620\n",
      "Epoch 34/200 - Train Loss: 46.743030, Val Loss: 46.982042\n",
      "Epoch 35/200 - Train Loss: 46.629673, Val Loss: 46.874644\n",
      "Epoch 36/200 - Train Loss: 46.517325, Val Loss: 46.768659\n",
      "Epoch 37/200 - Train Loss: 46.409633, Val Loss: 46.663246\n",
      "Epoch 38/200 - Train Loss: 46.301182, Val Loss: 46.557462\n",
      "Epoch 39/200 - Train Loss: 46.198454, Val Loss: 46.462815\n",
      "Epoch 40/200 - Train Loss: 46.097309, Val Loss: 46.366564\n",
      "Epoch 41/200 - Train Loss: 45.996840, Val Loss: 46.261395\n",
      "Epoch 42/200 - Train Loss: 45.899550, Val Loss: 46.164879\n",
      "Epoch 43/200 - Train Loss: 45.802958, Val Loss: 46.081287\n",
      "Epoch 44/200 - Train Loss: 45.710992, Val Loss: 45.978740\n",
      "Epoch 45/200 - Train Loss: 45.619290, Val Loss: 45.889894\n",
      "Epoch 46/200 - Train Loss: 45.530530, Val Loss: 45.803362\n",
      "Epoch 47/200 - Train Loss: 45.442035, Val Loss: 45.717643\n",
      "Epoch 48/200 - Train Loss: 45.356430, Val Loss: 45.635552\n",
      "Epoch 49/200 - Train Loss: 45.272258, Val Loss: 45.547288\n",
      "Epoch 50/200 - Train Loss: 45.189097, Val Loss: 45.467651\n",
      "Epoch 51/200 - Train Loss: 45.108841, Val Loss: 45.390977\n",
      "Epoch 52/200 - Train Loss: 45.030510, Val Loss: 45.304491\n",
      "Epoch 53/200 - Train Loss: 44.953750, Val Loss: 45.229863\n",
      "Epoch 54/200 - Train Loss: 44.878311, Val Loss: 45.156694\n",
      "Epoch 55/200 - Train Loss: 44.803816, Val Loss: 45.079744\n",
      "Epoch 56/200 - Train Loss: 44.731445, Val Loss: 44.998580\n",
      "Epoch 57/200 - Train Loss: 44.661457, Val Loss: 44.933911\n",
      "Epoch 58/200 - Train Loss: 44.592567, Val Loss: 44.878758\n",
      "Epoch 59/200 - Train Loss: 44.524132, Val Loss: 44.792995\n",
      "Epoch 60/200 - Train Loss: 44.458378, Val Loss: 44.729119\n",
      "Epoch 61/200 - Train Loss: 44.393596, Val Loss: 44.663199\n",
      "Epoch 62/200 - Train Loss: 44.330306, Val Loss: 44.598953\n",
      "Epoch 63/200 - Train Loss: 44.268134, Val Loss: 44.545037\n",
      "Epoch 64/200 - Train Loss: 44.207331, Val Loss: 44.477574\n",
      "Epoch 65/200 - Train Loss: 44.148224, Val Loss: 44.411445\n",
      "Epoch 66/200 - Train Loss: 44.090302, Val Loss: 44.358394\n",
      "Epoch 67/200 - Train Loss: 44.033739, Val Loss: 44.294497\n",
      "Epoch 68/200 - Train Loss: 43.978608, Val Loss: 44.243955\n",
      "Epoch 69/200 - Train Loss: 43.924203, Val Loss: 44.181906\n",
      "Epoch 70/200 - Train Loss: 43.871538, Val Loss: 44.134653\n",
      "Epoch 71/200 - Train Loss: 43.820615, Val Loss: 44.079766\n",
      "Epoch 72/200 - Train Loss: 43.769906, Val Loss: 44.022235\n",
      "Epoch 73/200 - Train Loss: 43.721183, Val Loss: 43.976513\n",
      "Epoch 74/200 - Train Loss: 43.673218, Val Loss: 43.925287\n",
      "Epoch 75/200 - Train Loss: 43.625711, Val Loss: 43.879087\n",
      "Epoch 76/200 - Train Loss: 43.580427, Val Loss: 43.838074\n",
      "Epoch 77/200 - Train Loss: 43.535948, Val Loss: 43.785875\n",
      "Epoch 78/200 - Train Loss: 43.492370, Val Loss: 43.734000\n",
      "Epoch 79/200 - Train Loss: 43.450539, Val Loss: 43.695008\n",
      "Epoch 80/200 - Train Loss: 43.409589, Val Loss: 43.653479\n",
      "Epoch 81/200 - Train Loss: 43.368570, Val Loss: 43.597954\n",
      "Epoch 82/200 - Train Loss: 43.329244, Val Loss: 43.563585\n",
      "Epoch 83/200 - Train Loss: 43.290686, Val Loss: 43.525483\n",
      "Epoch 84/200 - Train Loss: 43.253276, Val Loss: 43.483742\n",
      "Epoch 85/200 - Train Loss: 43.216736, Val Loss: 43.442927\n",
      "Epoch 86/200 - Train Loss: 43.181517, Val Loss: 43.409809\n",
      "Epoch 87/200 - Train Loss: 43.147278, Val Loss: 43.373468\n",
      "Epoch 88/200 - Train Loss: 43.113423, Val Loss: 43.337678\n",
      "Epoch 89/200 - Train Loss: 43.080877, Val Loss: 43.301635\n",
      "Epoch 90/200 - Train Loss: 43.049140, Val Loss: 43.261603\n",
      "Epoch 91/200 - Train Loss: 43.017741, Val Loss: 43.230136\n",
      "Epoch 92/200 - Train Loss: 42.988119, Val Loss: 43.193865\n",
      "Epoch 93/200 - Train Loss: 42.957502, Val Loss: 43.168011\n",
      "Epoch 94/200 - Train Loss: 42.929559, Val Loss: 43.137459\n",
      "Epoch 95/200 - Train Loss: 42.901166, Val Loss: 43.107847\n",
      "Epoch 96/200 - Train Loss: 42.874941, Val Loss: 43.072405\n",
      "Epoch 97/200 - Train Loss: 42.848645, Val Loss: 43.046408\n",
      "Epoch 98/200 - Train Loss: 42.822771, Val Loss: 43.011592\n",
      "Epoch 99/200 - Train Loss: 42.798883, Val Loss: 42.988951\n",
      "Epoch 100/200 - Train Loss: 42.774189, Val Loss: 42.963751\n",
      "Epoch 101/200 - Train Loss: 42.750928, Val Loss: 42.930948\n",
      "Epoch 102/200 - Train Loss: 42.728922, Val Loss: 42.907069\n",
      "Epoch 103/200 - Train Loss: 42.706389, Val Loss: 42.882006\n",
      "Epoch 104/200 - Train Loss: 42.685325, Val Loss: 42.853786\n",
      "Epoch 105/200 - Train Loss: 42.664164, Val Loss: 42.828751\n",
      "Epoch 106/200 - Train Loss: 42.643892, Val Loss: 42.803438\n",
      "Epoch 107/200 - Train Loss: 42.624678, Val Loss: 42.786360\n",
      "Epoch 108/200 - Train Loss: 42.605669, Val Loss: 42.768451\n",
      "Epoch 109/200 - Train Loss: 42.586584, Val Loss: 42.742654\n",
      "Epoch 110/200 - Train Loss: 42.569559, Val Loss: 42.718952\n",
      "Epoch 111/200 - Train Loss: 42.552673, Val Loss: 42.696411\n",
      "Epoch 112/200 - Train Loss: 42.535114, Val Loss: 42.678189\n",
      "Epoch 113/200 - Train Loss: 42.519578, Val Loss: 42.657668\n",
      "Epoch 114/200 - Train Loss: 42.503352, Val Loss: 42.637679\n",
      "Epoch 115/200 - Train Loss: 42.488285, Val Loss: 42.618345\n",
      "Epoch 116/200 - Train Loss: 42.474299, Val Loss: 42.611018\n",
      "Epoch 117/200 - Train Loss: 42.460131, Val Loss: 42.585427\n",
      "Epoch 118/200 - Train Loss: 42.446261, Val Loss: 42.567194\n",
      "Epoch 119/200 - Train Loss: 42.432814, Val Loss: 42.549041\n",
      "Epoch 120/200 - Train Loss: 42.420555, Val Loss: 42.530015\n",
      "Epoch 121/200 - Train Loss: 42.408226, Val Loss: 42.514407\n",
      "Epoch 122/200 - Train Loss: 42.396114, Val Loss: 42.500091\n",
      "Epoch 123/200 - Train Loss: 42.385071, Val Loss: 42.484456\n",
      "Epoch 124/200 - Train Loss: 42.373731, Val Loss: 42.472729\n",
      "Epoch 125/200 - Train Loss: 42.363512, Val Loss: 42.453276\n",
      "Epoch 126/200 - Train Loss: 42.353230, Val Loss: 42.438831\n",
      "Epoch 127/200 - Train Loss: 42.343766, Val Loss: 42.426643\n",
      "Epoch 128/200 - Train Loss: 42.333511, Val Loss: 42.414159\n",
      "Epoch 129/200 - Train Loss: 42.324633, Val Loss: 42.398063\n",
      "Epoch 130/200 - Train Loss: 42.316438, Val Loss: 42.386382\n",
      "Epoch 131/200 - Train Loss: 42.307867, Val Loss: 42.376089\n",
      "Epoch 132/200 - Train Loss: 42.299473, Val Loss: 42.363594\n",
      "Epoch 133/200 - Train Loss: 42.291738, Val Loss: 42.351887\n",
      "Epoch 134/200 - Train Loss: 42.284604, Val Loss: 42.337721\n",
      "Epoch 135/200 - Train Loss: 42.277147, Val Loss: 42.331290\n",
      "Epoch 136/200 - Train Loss: 42.270238, Val Loss: 42.312168\n",
      "Epoch 137/200 - Train Loss: 42.263711, Val Loss: 42.301827\n",
      "Epoch 138/200 - Train Loss: 42.257724, Val Loss: 42.294901\n",
      "Epoch 139/200 - Train Loss: 42.251626, Val Loss: 42.286108\n",
      "Epoch 140/200 - Train Loss: 42.245431, Val Loss: 42.272369\n",
      "Epoch 141/200 - Train Loss: 42.240371, Val Loss: 42.263245\n",
      "Epoch 142/200 - Train Loss: 42.234824, Val Loss: 42.258567\n",
      "Epoch 143/200 - Train Loss: 42.229375, Val Loss: 42.242106\n",
      "Epoch 144/200 - Train Loss: 42.224277, Val Loss: 42.245533\n",
      "Epoch 145/200 - Train Loss: 42.220673, Val Loss: 42.229429\n",
      "Epoch 146/200 - Train Loss: 42.215794, Val Loss: 42.225897\n",
      "Epoch 147/200 - Train Loss: 42.211445, Val Loss: 42.214726\n",
      "Epoch 148/200 - Train Loss: 42.207306, Val Loss: 42.206764\n",
      "Epoch 149/200 - Train Loss: 42.203459, Val Loss: 42.199150\n",
      "Epoch 150/200 - Train Loss: 42.199779, Val Loss: 42.182212\n",
      "Epoch 151/200 - Train Loss: 42.196503, Val Loss: 42.183341\n",
      "Epoch 152/200 - Train Loss: 42.193155, Val Loss: 42.179263\n",
      "Epoch 153/200 - Train Loss: 42.189607, Val Loss: 42.174603\n",
      "Epoch 154/200 - Train Loss: 42.187639, Val Loss: 42.158982\n",
      "Epoch 155/200 - Train Loss: 42.184139, Val Loss: 42.159091\n",
      "Epoch 156/200 - Train Loss: 42.181942, Val Loss: 42.151854\n",
      "Epoch 157/200 - Train Loss: 42.178554, Val Loss: 42.145408\n",
      "Epoch 158/200 - Train Loss: 42.176802, Val Loss: 42.142569\n",
      "Epoch 159/200 - Train Loss: 42.173898, Val Loss: 42.132993\n",
      "Epoch 160/200 - Train Loss: 42.172294, Val Loss: 42.135794\n",
      "Epoch 161/200 - Train Loss: 42.169934, Val Loss: 42.125377\n",
      "Epoch 162/200 - Train Loss: 42.167946, Val Loss: 42.115341\n",
      "Epoch 163/200 - Train Loss: 42.165923, Val Loss: 42.120578\n",
      "Epoch 164/200 - Train Loss: 42.163974, Val Loss: 42.110168\n",
      "Epoch 165/200 - Train Loss: 42.162375, Val Loss: 42.103620\n",
      "Epoch 166/200 - Train Loss: 42.160335, Val Loss: 42.102625\n",
      "Epoch 167/200 - Train Loss: 42.159498, Val Loss: 42.093401\n",
      "Epoch 168/200 - Train Loss: 42.158372, Val Loss: 42.093306\n",
      "Epoch 169/200 - Train Loss: 42.157266, Val Loss: 42.083313\n",
      "Epoch 170/200 - Train Loss: 42.155437, Val Loss: 42.090889\n",
      "Epoch 171/200 - Train Loss: 42.154956, Val Loss: 42.076132\n",
      "Epoch 172/200 - Train Loss: 42.153472, Val Loss: 42.074542\n",
      "Epoch 173/200 - Train Loss: 42.152794, Val Loss: 42.076378\n",
      "Epoch 174/200 - Train Loss: 42.151729, Val Loss: 42.075619\n",
      "Epoch 175/200 - Train Loss: 42.151204, Val Loss: 42.066346\n",
      "Epoch 176/200 - Train Loss: 42.149884, Val Loss: 42.071085\n",
      "Epoch 177/200 - Train Loss: 42.149722, Val Loss: 42.062869\n",
      "Epoch 178/200 - Train Loss: 42.148461, Val Loss: 42.057023\n",
      "Epoch 179/200 - Train Loss: 42.147679, Val Loss: 42.055904\n",
      "Epoch 180/200 - Train Loss: 42.147357, Val Loss: 42.050834\n",
      "Epoch 181/200 - Train Loss: 42.146779, Val Loss: 42.048704\n",
      "Epoch 182/200 - Train Loss: 42.146266, Val Loss: 42.053133\n",
      "Epoch 183/200 - Train Loss: 42.145927, Val Loss: 42.044846\n",
      "Epoch 184/200 - Train Loss: 42.145808, Val Loss: 42.047649\n",
      "Epoch 185/200 - Train Loss: 42.145493, Val Loss: 42.044812\n",
      "Epoch 186/200 - Train Loss: 42.144774, Val Loss: 42.039537\n",
      "Epoch 187/200 - Train Loss: 42.144439, Val Loss: 42.037626\n",
      "Epoch 188/200 - Train Loss: 42.145096, Val Loss: 42.036651\n",
      "Epoch 189/200 - Train Loss: 42.143857, Val Loss: 42.031126\n",
      "Epoch 190/200 - Train Loss: 42.143660, Val Loss: 42.028028\n",
      "Epoch 191/200 - Train Loss: 42.143800, Val Loss: 42.035929\n",
      "Epoch 192/200 - Train Loss: 42.143035, Val Loss: 42.027523\n",
      "Epoch 193/200 - Train Loss: 42.143387, Val Loss: 42.036771\n",
      "Epoch 194/200 - Train Loss: 42.143245, Val Loss: 42.025863\n",
      "Epoch 195/200 - Train Loss: 42.143047, Val Loss: 42.019528\n",
      "Epoch 196/200 - Train Loss: 42.142722, Val Loss: 42.026877\n",
      "Epoch 197/200 - Train Loss: 42.142905, Val Loss: 42.024070\n",
      "Epoch 198/200 - Train Loss: 42.142797, Val Loss: 42.026789\n",
      "Epoch 199/200 - Train Loss: 42.142393, Val Loss: 42.020108\n",
      "Epoch 200/200 - Train Loss: 42.142707, Val Loss: 42.015464\n",
      "  Avg Val Score for config {'learning_rate': 0.0001, 'weight_decay': 0}: 42.293211\n",
      "  Testing config: {'learning_rate': 0.0001, 'weight_decay': 1e-05}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 56.852308, Val Loss: 55.908414\n",
      "Epoch 2/200 - Train Loss: 54.814100, Val Loss: 54.655990\n",
      "Epoch 3/200 - Train Loss: 53.894083, Val Loss: 53.905132\n",
      "Epoch 4/200 - Train Loss: 53.248553, Val Loss: 53.320170\n",
      "Epoch 5/200 - Train Loss: 52.724035, Val Loss: 52.830719\n",
      "Epoch 6/200 - Train Loss: 52.275897, Val Loss: 52.408116\n",
      "Epoch 7/200 - Train Loss: 51.877878, Val Loss: 52.017943\n",
      "Epoch 8/200 - Train Loss: 51.521027, Val Loss: 51.680044\n",
      "Epoch 9/200 - Train Loss: 51.197664, Val Loss: 51.362495\n",
      "Epoch 10/200 - Train Loss: 50.899886, Val Loss: 51.070599\n",
      "Epoch 11/200 - Train Loss: 50.624775, Val Loss: 50.807211\n",
      "Epoch 12/200 - Train Loss: 50.366766, Val Loss: 50.547942\n",
      "Epoch 13/200 - Train Loss: 50.123208, Val Loss: 50.308566\n",
      "Epoch 14/200 - Train Loss: 49.895598, Val Loss: 50.089160\n",
      "Epoch 15/200 - Train Loss: 49.677986, Val Loss: 49.872505\n",
      "Epoch 16/200 - Train Loss: 49.469726, Val Loss: 49.667104\n",
      "Epoch 17/200 - Train Loss: 49.271560, Val Loss: 49.471649\n",
      "Epoch 18/200 - Train Loss: 49.081328, Val Loss: 49.285959\n",
      "Epoch 19/200 - Train Loss: 48.899347, Val Loss: 49.114455\n",
      "Epoch 20/200 - Train Loss: 48.722907, Val Loss: 48.943259\n",
      "Epoch 21/200 - Train Loss: 48.551846, Val Loss: 48.773072\n",
      "Epoch 22/200 - Train Loss: 48.387736, Val Loss: 48.625578\n",
      "Epoch 23/200 - Train Loss: 48.228268, Val Loss: 48.473624\n",
      "Epoch 24/200 - Train Loss: 48.073220, Val Loss: 48.321471\n",
      "Epoch 25/200 - Train Loss: 47.924860, Val Loss: 48.168904\n",
      "Epoch 26/200 - Train Loss: 47.775853, Val Loss: 48.038103\n",
      "Epoch 27/200 - Train Loss: 47.634482, Val Loss: 47.905032\n",
      "Epoch 28/200 - Train Loss: 47.495027, Val Loss: 47.763259\n",
      "Epoch 29/200 - Train Loss: 47.359910, Val Loss: 47.636616\n",
      "Epoch 30/200 - Train Loss: 47.229350, Val Loss: 47.524983\n",
      "Epoch 31/200 - Train Loss: 47.099561, Val Loss: 47.403921\n",
      "Epoch 32/200 - Train Loss: 46.974888, Val Loss: 47.293469\n",
      "Epoch 33/200 - Train Loss: 46.852136, Val Loss: 47.168163\n",
      "Epoch 34/200 - Train Loss: 46.731101, Val Loss: 47.052724\n",
      "Epoch 35/200 - Train Loss: 46.613881, Val Loss: 46.947134\n",
      "Epoch 36/200 - Train Loss: 46.498506, Val Loss: 46.839999\n",
      "Epoch 37/200 - Train Loss: 46.386724, Val Loss: 46.744700\n",
      "Epoch 38/200 - Train Loss: 46.274671, Val Loss: 46.629072\n",
      "Epoch 39/200 - Train Loss: 46.167824, Val Loss: 46.536656\n",
      "Epoch 40/200 - Train Loss: 46.062788, Val Loss: 46.439853\n",
      "Epoch 41/200 - Train Loss: 45.959215, Val Loss: 46.345594\n",
      "Epoch 42/200 - Train Loss: 45.857437, Val Loss: 46.266030\n",
      "Epoch 43/200 - Train Loss: 45.757311, Val Loss: 46.152600\n",
      "Epoch 44/200 - Train Loss: 45.660965, Val Loss: 46.091357\n",
      "Epoch 45/200 - Train Loss: 45.565808, Val Loss: 45.988633\n",
      "Epoch 46/200 - Train Loss: 45.473274, Val Loss: 45.910679\n",
      "Epoch 47/200 - Train Loss: 45.380613, Val Loss: 45.828524\n",
      "Epoch 48/200 - Train Loss: 45.290795, Val Loss: 45.747132\n",
      "Epoch 49/200 - Train Loss: 45.202996, Val Loss: 45.672589\n",
      "Epoch 50/200 - Train Loss: 45.115510, Val Loss: 45.581850\n",
      "Epoch 51/200 - Train Loss: 45.030890, Val Loss: 45.511568\n",
      "Epoch 52/200 - Train Loss: 44.948699, Val Loss: 45.450860\n",
      "Epoch 53/200 - Train Loss: 44.867945, Val Loss: 45.372863\n",
      "Epoch 54/200 - Train Loss: 44.788716, Val Loss: 45.303359\n",
      "Epoch 55/200 - Train Loss: 44.709347, Val Loss: 45.239953\n",
      "Epoch 56/200 - Train Loss: 44.633304, Val Loss: 45.191962\n",
      "Epoch 57/200 - Train Loss: 44.557965, Val Loss: 45.130649\n",
      "Epoch 58/200 - Train Loss: 44.485409, Val Loss: 45.027456\n",
      "Epoch 59/200 - Train Loss: 44.412575, Val Loss: 44.983205\n",
      "Epoch 60/200 - Train Loss: 44.342105, Val Loss: 44.933745\n",
      "Epoch 61/200 - Train Loss: 44.272967, Val Loss: 44.860945\n",
      "Epoch 62/200 - Train Loss: 44.204786, Val Loss: 44.806580\n",
      "Epoch 63/200 - Train Loss: 44.138505, Val Loss: 44.745574\n",
      "Epoch 64/200 - Train Loss: 44.072981, Val Loss: 44.698592\n",
      "Epoch 65/200 - Train Loss: 44.009463, Val Loss: 44.653340\n",
      "Epoch 66/200 - Train Loss: 43.946776, Val Loss: 44.606915\n",
      "Epoch 67/200 - Train Loss: 43.885702, Val Loss: 44.561539\n",
      "Epoch 68/200 - Train Loss: 43.825784, Val Loss: 44.501820\n",
      "Epoch 69/200 - Train Loss: 43.766614, Val Loss: 44.450597\n",
      "Epoch 70/200 - Train Loss: 43.709221, Val Loss: 44.400167\n",
      "Epoch 71/200 - Train Loss: 43.653174, Val Loss: 44.355221\n",
      "Epoch 72/200 - Train Loss: 43.597994, Val Loss: 44.320343\n",
      "Epoch 73/200 - Train Loss: 43.544237, Val Loss: 44.277072\n",
      "Epoch 74/200 - Train Loss: 43.491313, Val Loss: 44.239827\n",
      "Epoch 75/200 - Train Loss: 43.439469, Val Loss: 44.192451\n",
      "Epoch 76/200 - Train Loss: 43.388777, Val Loss: 44.145070\n",
      "Epoch 77/200 - Train Loss: 43.339418, Val Loss: 44.122781\n",
      "Epoch 78/200 - Train Loss: 43.290943, Val Loss: 44.097853\n",
      "Epoch 79/200 - Train Loss: 43.244172, Val Loss: 44.057685\n",
      "Epoch 80/200 - Train Loss: 43.198137, Val Loss: 44.009943\n",
      "Epoch 81/200 - Train Loss: 43.152250, Val Loss: 43.983826\n",
      "Epoch 82/200 - Train Loss: 43.107847, Val Loss: 43.944210\n",
      "Epoch 83/200 - Train Loss: 43.064809, Val Loss: 43.910138\n",
      "Epoch 84/200 - Train Loss: 43.021845, Val Loss: 43.880059\n",
      "Epoch 85/200 - Train Loss: 42.980269, Val Loss: 43.857549\n",
      "Epoch 86/200 - Train Loss: 42.939751, Val Loss: 43.823445\n",
      "Epoch 87/200 - Train Loss: 42.900246, Val Loss: 43.789617\n",
      "Epoch 88/200 - Train Loss: 42.861474, Val Loss: 43.766124\n",
      "Epoch 89/200 - Train Loss: 42.823404, Val Loss: 43.745934\n",
      "Epoch 90/200 - Train Loss: 42.787173, Val Loss: 43.716370\n",
      "Epoch 91/200 - Train Loss: 42.750277, Val Loss: 43.689851\n",
      "Epoch 92/200 - Train Loss: 42.715957, Val Loss: 43.677772\n",
      "Epoch 93/200 - Train Loss: 42.679798, Val Loss: 43.654484\n",
      "Epoch 94/200 - Train Loss: 42.646784, Val Loss: 43.622220\n",
      "Epoch 95/200 - Train Loss: 42.613849, Val Loss: 43.615140\n",
      "Epoch 96/200 - Train Loss: 42.581576, Val Loss: 43.594879\n",
      "Epoch 97/200 - Train Loss: 42.550701, Val Loss: 43.571178\n",
      "Epoch 98/200 - Train Loss: 42.519720, Val Loss: 43.558675\n",
      "Epoch 99/200 - Train Loss: 42.489863, Val Loss: 43.527214\n",
      "Epoch 100/200 - Train Loss: 42.460089, Val Loss: 43.512439\n",
      "Epoch 101/200 - Train Loss: 42.431958, Val Loss: 43.512447\n",
      "Epoch 102/200 - Train Loss: 42.405053, Val Loss: 43.489096\n",
      "Epoch 103/200 - Train Loss: 42.376982, Val Loss: 43.468764\n",
      "Epoch 104/200 - Train Loss: 42.351017, Val Loss: 43.461292\n",
      "Epoch 105/200 - Train Loss: 42.324633, Val Loss: 43.449084\n",
      "Epoch 106/200 - Train Loss: 42.299195, Val Loss: 43.430117\n",
      "Epoch 107/200 - Train Loss: 42.274469, Val Loss: 43.428437\n",
      "Epoch 108/200 - Train Loss: 42.251066, Val Loss: 43.399939\n",
      "Epoch 109/200 - Train Loss: 42.227187, Val Loss: 43.395505\n",
      "Epoch 110/200 - Train Loss: 42.204348, Val Loss: 43.383881\n",
      "Epoch 111/200 - Train Loss: 42.182042, Val Loss: 43.378217\n",
      "Epoch 112/200 - Train Loss: 42.160256, Val Loss: 43.371379\n",
      "Epoch 113/200 - Train Loss: 42.139227, Val Loss: 43.357259\n",
      "Epoch 114/200 - Train Loss: 42.118234, Val Loss: 43.354301\n",
      "Epoch 115/200 - Train Loss: 42.098410, Val Loss: 43.337103\n",
      "Epoch 116/200 - Train Loss: 42.078925, Val Loss: 43.321904\n",
      "Epoch 117/200 - Train Loss: 42.060012, Val Loss: 43.334576\n",
      "Epoch 118/200 - Train Loss: 42.041463, Val Loss: 43.322942\n",
      "Epoch 119/200 - Train Loss: 42.022975, Val Loss: 43.312936\n",
      "Epoch 120/200 - Train Loss: 42.006062, Val Loss: 43.319140\n",
      "Epoch 121/200 - Train Loss: 41.988764, Val Loss: 43.303043\n",
      "Epoch 122/200 - Train Loss: 41.972037, Val Loss: 43.298453\n",
      "Epoch 123/200 - Train Loss: 41.956004, Val Loss: 43.308535\n",
      "Epoch 124/200 - Train Loss: 41.939877, Val Loss: 43.281800\n",
      "Epoch 125/200 - Train Loss: 41.924630, Val Loss: 43.297325\n",
      "Epoch 126/200 - Train Loss: 41.909964, Val Loss: 43.289938\n",
      "Epoch 127/200 - Train Loss: 41.895988, Val Loss: 43.279205\n",
      "Epoch 128/200 - Train Loss: 41.881354, Val Loss: 43.292570\n",
      "Epoch 129/200 - Train Loss: 41.868354, Val Loss: 43.292975\n",
      "Epoch 130/200 - Train Loss: 41.855044, Val Loss: 43.280629\n",
      "Epoch 131/200 - Train Loss: 41.842330, Val Loss: 43.276566\n",
      "Epoch 132/200 - Train Loss: 41.829147, Val Loss: 43.285695\n",
      "Epoch 133/200 - Train Loss: 41.817390, Val Loss: 43.276085\n",
      "Epoch 134/200 - Train Loss: 41.805575, Val Loss: 43.283660\n",
      "Epoch 135/200 - Train Loss: 41.794292, Val Loss: 43.275380\n",
      "Epoch 136/200 - Train Loss: 41.782771, Val Loss: 43.287347\n",
      "Epoch 137/200 - Train Loss: 41.772452, Val Loss: 43.280802\n",
      "Epoch 138/200 - Train Loss: 41.762141, Val Loss: 43.290184\n",
      "Epoch 139/200 - Train Loss: 41.752018, Val Loss: 43.284078\n",
      "Epoch 140/200 - Train Loss: 41.741651, Val Loss: 43.301373\n",
      "Epoch 141/200 - Train Loss: 41.732394, Val Loss: 43.291849\n",
      "Epoch 142/200 - Train Loss: 41.723439, Val Loss: 43.296530\n",
      "Epoch 143/200 - Train Loss: 41.713681, Val Loss: 43.296250\n",
      "Epoch 144/200 - Train Loss: 41.705158, Val Loss: 43.293863\n",
      "Epoch 145/200 - Train Loss: 41.697277, Val Loss: 43.294767\n",
      "Epoch 146/200 - Train Loss: 41.688601, Val Loss: 43.299282\n",
      "Epoch 147/200 - Train Loss: 41.681304, Val Loss: 43.295948\n",
      "Epoch 148/200 - Train Loss: 41.673412, Val Loss: 43.301483\n",
      "Epoch 149/200 - Train Loss: 41.665650, Val Loss: 43.307707\n",
      "Epoch 150/200 - Train Loss: 41.658644, Val Loss: 43.328483\n",
      "Early stopping triggered after 150 epochs. Best val loss 43.275380 at epoch 135.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 56.816713, Val Loss: 55.722200\n",
      "Epoch 2/200 - Train Loss: 54.778458, Val Loss: 54.515932\n",
      "Epoch 3/200 - Train Loss: 53.844412, Val Loss: 53.785892\n",
      "Epoch 4/200 - Train Loss: 53.185596, Val Loss: 53.205970\n",
      "Epoch 5/200 - Train Loss: 52.651913, Val Loss: 52.721145\n",
      "Epoch 6/200 - Train Loss: 52.198429, Val Loss: 52.292433\n",
      "Epoch 7/200 - Train Loss: 51.797466, Val Loss: 51.916610\n",
      "Epoch 8/200 - Train Loss: 51.439439, Val Loss: 51.568709\n",
      "Epoch 9/200 - Train Loss: 51.115172, Val Loss: 51.251169\n",
      "Epoch 10/200 - Train Loss: 50.817318, Val Loss: 50.959713\n",
      "Epoch 11/200 - Train Loss: 50.542131, Val Loss: 50.686267\n",
      "Epoch 12/200 - Train Loss: 50.284152, Val Loss: 50.430445\n",
      "Epoch 13/200 - Train Loss: 50.042155, Val Loss: 50.193484\n",
      "Epoch 14/200 - Train Loss: 49.815907, Val Loss: 49.964079\n",
      "Epoch 15/200 - Train Loss: 49.600592, Val Loss: 49.745650\n",
      "Epoch 16/200 - Train Loss: 49.393842, Val Loss: 49.543278\n",
      "Epoch 17/200 - Train Loss: 49.198586, Val Loss: 49.344441\n",
      "Epoch 18/200 - Train Loss: 49.011462, Val Loss: 49.154487\n",
      "Epoch 19/200 - Train Loss: 48.832305, Val Loss: 48.971034\n",
      "Epoch 20/200 - Train Loss: 48.660334, Val Loss: 48.795313\n",
      "Epoch 21/200 - Train Loss: 48.493380, Val Loss: 48.630385\n",
      "Epoch 22/200 - Train Loss: 48.333158, Val Loss: 48.460863\n",
      "Epoch 23/200 - Train Loss: 48.178211, Val Loss: 48.303439\n",
      "Epoch 24/200 - Train Loss: 48.028447, Val Loss: 48.149280\n",
      "Epoch 25/200 - Train Loss: 47.885000, Val Loss: 48.008156\n",
      "Epoch 26/200 - Train Loss: 47.741679, Val Loss: 47.854492\n",
      "Epoch 27/200 - Train Loss: 47.605830, Val Loss: 47.712122\n",
      "Epoch 28/200 - Train Loss: 47.471727, Val Loss: 47.574829\n",
      "Epoch 29/200 - Train Loss: 47.342282, Val Loss: 47.444392\n",
      "Epoch 30/200 - Train Loss: 47.216778, Val Loss: 47.309105\n",
      "Epoch 31/200 - Train Loss: 47.092897, Val Loss: 47.181477\n",
      "Epoch 32/200 - Train Loss: 46.973685, Val Loss: 47.053267\n",
      "Epoch 33/200 - Train Loss: 46.857171, Val Loss: 46.930995\n",
      "Epoch 34/200 - Train Loss: 46.740805, Val Loss: 46.813044\n",
      "Epoch 35/200 - Train Loss: 46.629761, Val Loss: 46.694169\n",
      "Epoch 36/200 - Train Loss: 46.520068, Val Loss: 46.576971\n",
      "Epoch 37/200 - Train Loss: 46.414143, Val Loss: 46.462756\n",
      "Epoch 38/200 - Train Loss: 46.307720, Val Loss: 46.357640\n",
      "Epoch 39/200 - Train Loss: 46.207027, Val Loss: 46.244823\n",
      "Epoch 40/200 - Train Loss: 46.107920, Val Loss: 46.141363\n",
      "Epoch 41/200 - Train Loss: 46.009693, Val Loss: 46.035625\n",
      "Epoch 42/200 - Train Loss: 45.914049, Val Loss: 45.929059\n",
      "Epoch 43/200 - Train Loss: 45.819677, Val Loss: 45.843448\n",
      "Epoch 44/200 - Train Loss: 45.729761, Val Loss: 45.732417\n",
      "Epoch 45/200 - Train Loss: 45.640001, Val Loss: 45.638640\n",
      "Epoch 46/200 - Train Loss: 45.553316, Val Loss: 45.544342\n",
      "Epoch 47/200 - Train Loss: 45.466699, Val Loss: 45.452076\n",
      "Epoch 48/200 - Train Loss: 45.382935, Val Loss: 45.366450\n",
      "Epoch 49/200 - Train Loss: 45.300979, Val Loss: 45.273159\n",
      "Epoch 50/200 - Train Loss: 45.219972, Val Loss: 45.190902\n",
      "Epoch 51/200 - Train Loss: 45.141704, Val Loss: 45.110525\n",
      "Epoch 52/200 - Train Loss: 45.065111, Val Loss: 45.019705\n",
      "Epoch 53/200 - Train Loss: 44.990459, Val Loss: 44.938933\n",
      "Epoch 54/200 - Train Loss: 44.917141, Val Loss: 44.858027\n",
      "Epoch 55/200 - Train Loss: 44.844320, Val Loss: 44.777751\n",
      "Epoch 56/200 - Train Loss: 44.773769, Val Loss: 44.694018\n",
      "Epoch 57/200 - Train Loss: 44.705538, Val Loss: 44.619026\n",
      "Epoch 58/200 - Train Loss: 44.638617, Val Loss: 44.560583\n",
      "Epoch 59/200 - Train Loss: 44.571800, Val Loss: 44.480076\n",
      "Epoch 60/200 - Train Loss: 44.507823, Val Loss: 44.407975\n",
      "Epoch 61/200 - Train Loss: 44.444627, Val Loss: 44.339150\n",
      "Epoch 62/200 - Train Loss: 44.383046, Val Loss: 44.269548\n",
      "Epoch 63/200 - Train Loss: 44.322675, Val Loss: 44.205364\n",
      "Epoch 64/200 - Train Loss: 44.263384, Val Loss: 44.139871\n",
      "Epoch 65/200 - Train Loss: 44.205811, Val Loss: 44.068429\n",
      "Epoch 66/200 - Train Loss: 44.149536, Val Loss: 44.006793\n",
      "Epoch 67/200 - Train Loss: 44.094443, Val Loss: 43.947680\n",
      "Epoch 68/200 - Train Loss: 44.040821, Val Loss: 43.890593\n",
      "Epoch 69/200 - Train Loss: 43.988080, Val Loss: 43.827661\n",
      "Epoch 70/200 - Train Loss: 43.936914, Val Loss: 43.776064\n",
      "Epoch 71/200 - Train Loss: 43.887187, Val Loss: 43.718666\n",
      "Epoch 72/200 - Train Loss: 43.837759, Val Loss: 43.657900\n",
      "Epoch 73/200 - Train Loss: 43.790186, Val Loss: 43.603423\n",
      "Epoch 74/200 - Train Loss: 43.743697, Val Loss: 43.547816\n",
      "Epoch 75/200 - Train Loss: 43.697767, Val Loss: 43.499998\n",
      "Epoch 76/200 - Train Loss: 43.653712, Val Loss: 43.457450\n",
      "Epoch 77/200 - Train Loss: 43.610214, Val Loss: 43.399586\n",
      "Epoch 78/200 - Train Loss: 43.567583, Val Loss: 43.345982\n",
      "Epoch 79/200 - Train Loss: 43.527417, Val Loss: 43.303953\n",
      "Epoch 80/200 - Train Loss: 43.487435, Val Loss: 43.256583\n",
      "Epoch 81/200 - Train Loss: 43.447476, Val Loss: 43.206938\n",
      "Epoch 82/200 - Train Loss: 43.409008, Val Loss: 43.168044\n",
      "Epoch 83/200 - Train Loss: 43.371604, Val Loss: 43.127455\n",
      "Epoch 84/200 - Train Loss: 43.334893, Val Loss: 43.078816\n",
      "Epoch 85/200 - Train Loss: 43.299394, Val Loss: 43.039351\n",
      "Epoch 86/200 - Train Loss: 43.265165, Val Loss: 43.002300\n",
      "Epoch 87/200 - Train Loss: 43.231645, Val Loss: 42.960284\n",
      "Epoch 88/200 - Train Loss: 43.198837, Val Loss: 42.924769\n",
      "Epoch 89/200 - Train Loss: 43.166846, Val Loss: 42.879923\n",
      "Epoch 90/200 - Train Loss: 43.136113, Val Loss: 42.841650\n",
      "Epoch 91/200 - Train Loss: 43.105140, Val Loss: 42.809165\n",
      "Epoch 92/200 - Train Loss: 43.076913, Val Loss: 42.771259\n",
      "Epoch 93/200 - Train Loss: 43.046756, Val Loss: 42.737830\n",
      "Epoch 94/200 - Train Loss: 43.019503, Val Loss: 42.704192\n",
      "Epoch 95/200 - Train Loss: 42.991941, Val Loss: 42.673385\n",
      "Epoch 96/200 - Train Loss: 42.965914, Val Loss: 42.637383\n",
      "Epoch 97/200 - Train Loss: 42.940131, Val Loss: 42.610519\n",
      "Epoch 98/200 - Train Loss: 42.915015, Val Loss: 42.575755\n",
      "Epoch 99/200 - Train Loss: 42.891355, Val Loss: 42.548520\n",
      "Epoch 100/200 - Train Loss: 42.867424, Val Loss: 42.518049\n",
      "Epoch 101/200 - Train Loss: 42.844838, Val Loss: 42.487482\n",
      "Epoch 102/200 - Train Loss: 42.822897, Val Loss: 42.460446\n",
      "Epoch 103/200 - Train Loss: 42.801021, Val Loss: 42.438290\n",
      "Epoch 104/200 - Train Loss: 42.780475, Val Loss: 42.409535\n",
      "Epoch 105/200 - Train Loss: 42.759731, Val Loss: 42.385151\n",
      "Epoch 106/200 - Train Loss: 42.739897, Val Loss: 42.357657\n",
      "Epoch 107/200 - Train Loss: 42.720590, Val Loss: 42.334302\n",
      "Epoch 108/200 - Train Loss: 42.702491, Val Loss: 42.314398\n",
      "Epoch 109/200 - Train Loss: 42.683837, Val Loss: 42.294934\n",
      "Epoch 110/200 - Train Loss: 42.666984, Val Loss: 42.269824\n",
      "Epoch 111/200 - Train Loss: 42.649841, Val Loss: 42.241859\n",
      "Epoch 112/200 - Train Loss: 42.633011, Val Loss: 42.224782\n",
      "Epoch 113/200 - Train Loss: 42.617401, Val Loss: 42.203174\n",
      "Epoch 114/200 - Train Loss: 42.601749, Val Loss: 42.181566\n",
      "Epoch 115/200 - Train Loss: 42.587202, Val Loss: 42.161505\n",
      "Epoch 116/200 - Train Loss: 42.572541, Val Loss: 42.151657\n",
      "Epoch 117/200 - Train Loss: 42.559336, Val Loss: 42.122729\n",
      "Epoch 118/200 - Train Loss: 42.545380, Val Loss: 42.112680\n",
      "Epoch 119/200 - Train Loss: 42.532472, Val Loss: 42.091658\n",
      "Epoch 120/200 - Train Loss: 42.520088, Val Loss: 42.070632\n",
      "Epoch 121/200 - Train Loss: 42.507750, Val Loss: 42.055488\n",
      "Epoch 122/200 - Train Loss: 42.495983, Val Loss: 42.037362\n",
      "Epoch 123/200 - Train Loss: 42.485162, Val Loss: 42.020610\n",
      "Epoch 124/200 - Train Loss: 42.473206, Val Loss: 42.010493\n",
      "Epoch 125/200 - Train Loss: 42.463213, Val Loss: 41.988903\n",
      "Epoch 126/200 - Train Loss: 42.453274, Val Loss: 41.977476\n",
      "Epoch 127/200 - Train Loss: 42.443303, Val Loss: 41.964915\n",
      "Epoch 128/200 - Train Loss: 42.433251, Val Loss: 41.950388\n",
      "Epoch 129/200 - Train Loss: 42.424352, Val Loss: 41.938700\n",
      "Epoch 130/200 - Train Loss: 42.416275, Val Loss: 41.927893\n",
      "Epoch 131/200 - Train Loss: 42.407648, Val Loss: 41.919204\n",
      "Epoch 132/200 - Train Loss: 42.399219, Val Loss: 41.903204\n",
      "Epoch 133/200 - Train Loss: 42.391644, Val Loss: 41.890347\n",
      "Epoch 134/200 - Train Loss: 42.384201, Val Loss: 41.881531\n",
      "Epoch 135/200 - Train Loss: 42.376567, Val Loss: 41.873485\n",
      "Epoch 136/200 - Train Loss: 42.369589, Val Loss: 41.857882\n",
      "Epoch 137/200 - Train Loss: 42.363424, Val Loss: 41.844337\n",
      "Epoch 138/200 - Train Loss: 42.357094, Val Loss: 41.832552\n",
      "Epoch 139/200 - Train Loss: 42.350929, Val Loss: 41.829221\n",
      "Epoch 140/200 - Train Loss: 42.344451, Val Loss: 41.814707\n",
      "Epoch 141/200 - Train Loss: 42.339069, Val Loss: 41.813286\n",
      "Epoch 142/200 - Train Loss: 42.333758, Val Loss: 41.803019\n",
      "Epoch 143/200 - Train Loss: 42.327865, Val Loss: 41.796363\n",
      "Epoch 144/200 - Train Loss: 42.322877, Val Loss: 41.792374\n",
      "Epoch 145/200 - Train Loss: 42.318666, Val Loss: 41.778735\n",
      "Epoch 146/200 - Train Loss: 42.313723, Val Loss: 41.775623\n",
      "Epoch 147/200 - Train Loss: 42.309473, Val Loss: 41.767258\n",
      "Epoch 148/200 - Train Loss: 42.305121, Val Loss: 41.762195\n",
      "Epoch 149/200 - Train Loss: 42.300959, Val Loss: 41.752548\n",
      "Epoch 150/200 - Train Loss: 42.296698, Val Loss: 41.740637\n",
      "Epoch 151/200 - Train Loss: 42.293528, Val Loss: 41.735396\n",
      "Epoch 152/200 - Train Loss: 42.289968, Val Loss: 41.729886\n",
      "Epoch 153/200 - Train Loss: 42.286236, Val Loss: 41.725945\n",
      "Epoch 154/200 - Train Loss: 42.283821, Val Loss: 41.715884\n",
      "Epoch 155/200 - Train Loss: 42.280155, Val Loss: 41.714149\n",
      "Epoch 156/200 - Train Loss: 42.278136, Val Loss: 41.711994\n",
      "Epoch 157/200 - Train Loss: 42.274350, Val Loss: 41.703207\n",
      "Epoch 158/200 - Train Loss: 42.272441, Val Loss: 41.702727\n",
      "Epoch 159/200 - Train Loss: 42.269579, Val Loss: 41.690911\n",
      "Epoch 160/200 - Train Loss: 42.267872, Val Loss: 41.694500\n",
      "Epoch 161/200 - Train Loss: 42.264675, Val Loss: 41.687157\n",
      "Epoch 162/200 - Train Loss: 42.263129, Val Loss: 41.676573\n",
      "Epoch 163/200 - Train Loss: 42.261014, Val Loss: 41.678973\n",
      "Epoch 164/200 - Train Loss: 42.258696, Val Loss: 41.672096\n",
      "Epoch 165/200 - Train Loss: 42.256619, Val Loss: 41.665692\n",
      "Epoch 166/200 - Train Loss: 42.255017, Val Loss: 41.671971\n",
      "Epoch 167/200 - Train Loss: 42.253607, Val Loss: 41.657566\n",
      "Epoch 168/200 - Train Loss: 42.252294, Val Loss: 41.661133\n",
      "Epoch 169/200 - Train Loss: 42.250684, Val Loss: 41.653678\n",
      "Epoch 170/200 - Train Loss: 42.248916, Val Loss: 41.653835\n",
      "Epoch 171/200 - Train Loss: 42.248194, Val Loss: 41.644176\n",
      "Epoch 172/200 - Train Loss: 42.246420, Val Loss: 41.644135\n",
      "Epoch 173/200 - Train Loss: 42.245433, Val Loss: 41.643400\n",
      "Epoch 174/200 - Train Loss: 42.244200, Val Loss: 41.642430\n",
      "Epoch 175/200 - Train Loss: 42.243193, Val Loss: 41.643801\n",
      "Epoch 176/200 - Train Loss: 42.242195, Val Loss: 41.639499\n",
      "Epoch 177/200 - Train Loss: 42.241426, Val Loss: 41.631868\n",
      "Epoch 178/200 - Train Loss: 42.240862, Val Loss: 41.628935\n",
      "Epoch 179/200 - Train Loss: 42.239406, Val Loss: 41.623999\n",
      "Epoch 180/200 - Train Loss: 42.238880, Val Loss: 41.623736\n",
      "Epoch 181/200 - Train Loss: 42.238075, Val Loss: 41.622591\n",
      "Epoch 182/200 - Train Loss: 42.237616, Val Loss: 41.623344\n",
      "Epoch 183/200 - Train Loss: 42.237139, Val Loss: 41.615227\n",
      "Epoch 184/200 - Train Loss: 42.236580, Val Loss: 41.617507\n",
      "Epoch 185/200 - Train Loss: 42.236188, Val Loss: 41.618723\n",
      "Epoch 186/200 - Train Loss: 42.236251, Val Loss: 41.612398\n",
      "Epoch 187/200 - Train Loss: 42.234806, Val Loss: 41.612972\n",
      "Epoch 188/200 - Train Loss: 42.235467, Val Loss: 41.608890\n",
      "Epoch 189/200 - Train Loss: 42.234585, Val Loss: 41.608835\n",
      "Epoch 190/200 - Train Loss: 42.234046, Val Loss: 41.604328\n",
      "Epoch 191/200 - Train Loss: 42.233566, Val Loss: 41.610534\n",
      "Epoch 192/200 - Train Loss: 42.232990, Val Loss: 41.610148\n",
      "Epoch 193/200 - Train Loss: 42.233562, Val Loss: 41.608098\n",
      "Epoch 194/200 - Train Loss: 42.232901, Val Loss: 41.604529\n",
      "Epoch 195/200 - Train Loss: 42.232803, Val Loss: 41.600924\n",
      "Epoch 196/200 - Train Loss: 42.232256, Val Loss: 41.605445\n",
      "Epoch 197/200 - Train Loss: 42.232425, Val Loss: 41.600892\n",
      "Epoch 198/200 - Train Loss: 42.232442, Val Loss: 41.600240\n",
      "Epoch 199/200 - Train Loss: 42.231566, Val Loss: 41.593857\n",
      "Epoch 200/200 - Train Loss: 42.231941, Val Loss: 41.588790\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 57.156166, Val Loss: 55.318243\n",
      "Epoch 2/200 - Train Loss: 55.039244, Val Loss: 54.138834\n",
      "Epoch 3/200 - Train Loss: 54.091160, Val Loss: 53.424156\n",
      "Epoch 4/200 - Train Loss: 53.423358, Val Loss: 52.859311\n",
      "Epoch 5/200 - Train Loss: 52.876531, Val Loss: 52.388842\n",
      "Epoch 6/200 - Train Loss: 52.408380, Val Loss: 51.974056\n",
      "Epoch 7/200 - Train Loss: 51.991319, Val Loss: 51.618106\n",
      "Epoch 8/200 - Train Loss: 51.618875, Val Loss: 51.287865\n",
      "Epoch 9/200 - Train Loss: 51.280604, Val Loss: 50.997194\n",
      "Epoch 10/200 - Train Loss: 50.969216, Val Loss: 50.718552\n",
      "Epoch 11/200 - Train Loss: 50.681300, Val Loss: 50.464441\n",
      "Epoch 12/200 - Train Loss: 50.411247, Val Loss: 50.229009\n",
      "Epoch 13/200 - Train Loss: 50.157469, Val Loss: 50.011960\n",
      "Epoch 14/200 - Train Loss: 49.920462, Val Loss: 49.804284\n",
      "Epoch 15/200 - Train Loss: 49.694969, Val Loss: 49.605039\n",
      "Epoch 16/200 - Train Loss: 49.480448, Val Loss: 49.420049\n",
      "Epoch 17/200 - Train Loss: 49.275700, Val Loss: 49.244648\n",
      "Epoch 18/200 - Train Loss: 49.080545, Val Loss: 49.069681\n",
      "Epoch 19/200 - Train Loss: 48.894290, Val Loss: 48.911325\n",
      "Epoch 20/200 - Train Loss: 48.715347, Val Loss: 48.754623\n",
      "Epoch 21/200 - Train Loss: 48.542135, Val Loss: 48.602117\n",
      "Epoch 22/200 - Train Loss: 48.375994, Val Loss: 48.454495\n",
      "Epoch 23/200 - Train Loss: 48.215655, Val Loss: 48.310434\n",
      "Epoch 24/200 - Train Loss: 48.061147, Val Loss: 48.177362\n",
      "Epoch 25/200 - Train Loss: 47.913285, Val Loss: 48.048611\n",
      "Epoch 26/200 - Train Loss: 47.766176, Val Loss: 47.918944\n",
      "Epoch 27/200 - Train Loss: 47.626502, Val Loss: 47.790941\n",
      "Epoch 28/200 - Train Loss: 47.488531, Val Loss: 47.664167\n",
      "Epoch 29/200 - Train Loss: 47.356316, Val Loss: 47.549152\n",
      "Epoch 30/200 - Train Loss: 47.228141, Val Loss: 47.430771\n",
      "Epoch 31/200 - Train Loss: 47.101462, Val Loss: 47.323829\n",
      "Epoch 32/200 - Train Loss: 46.979853, Val Loss: 47.198512\n",
      "Epoch 33/200 - Train Loss: 46.860912, Val Loss: 47.093620\n",
      "Epoch 34/200 - Train Loss: 46.743030, Val Loss: 46.982042\n",
      "Epoch 35/200 - Train Loss: 46.629673, Val Loss: 46.874644\n",
      "Epoch 36/200 - Train Loss: 46.517325, Val Loss: 46.768659\n",
      "Epoch 37/200 - Train Loss: 46.409633, Val Loss: 46.663246\n",
      "Epoch 38/200 - Train Loss: 46.301182, Val Loss: 46.557462\n",
      "Epoch 39/200 - Train Loss: 46.198454, Val Loss: 46.462815\n",
      "Epoch 40/200 - Train Loss: 46.097309, Val Loss: 46.366564\n",
      "Epoch 41/200 - Train Loss: 45.996840, Val Loss: 46.261395\n",
      "Epoch 42/200 - Train Loss: 45.899550, Val Loss: 46.164879\n",
      "Epoch 43/200 - Train Loss: 45.802958, Val Loss: 46.081287\n",
      "Epoch 44/200 - Train Loss: 45.710992, Val Loss: 45.978740\n",
      "Epoch 45/200 - Train Loss: 45.619290, Val Loss: 45.889894\n",
      "Epoch 46/200 - Train Loss: 45.530530, Val Loss: 45.803362\n",
      "Epoch 47/200 - Train Loss: 45.442035, Val Loss: 45.717643\n",
      "Epoch 48/200 - Train Loss: 45.356430, Val Loss: 45.635552\n",
      "Epoch 49/200 - Train Loss: 45.272258, Val Loss: 45.547288\n",
      "Epoch 50/200 - Train Loss: 45.189097, Val Loss: 45.467651\n",
      "Epoch 51/200 - Train Loss: 45.108841, Val Loss: 45.390977\n",
      "Epoch 52/200 - Train Loss: 45.030510, Val Loss: 45.304491\n",
      "Epoch 53/200 - Train Loss: 44.953750, Val Loss: 45.229863\n",
      "Epoch 54/200 - Train Loss: 44.878311, Val Loss: 45.156694\n",
      "Epoch 55/200 - Train Loss: 44.803816, Val Loss: 45.079744\n",
      "Epoch 56/200 - Train Loss: 44.731445, Val Loss: 44.998580\n",
      "Epoch 57/200 - Train Loss: 44.661457, Val Loss: 44.933911\n",
      "Epoch 58/200 - Train Loss: 44.592567, Val Loss: 44.878758\n",
      "Epoch 59/200 - Train Loss: 44.524132, Val Loss: 44.792995\n",
      "Epoch 60/200 - Train Loss: 44.458378, Val Loss: 44.729119\n",
      "Epoch 61/200 - Train Loss: 44.393596, Val Loss: 44.663199\n",
      "Epoch 62/200 - Train Loss: 44.330306, Val Loss: 44.598953\n",
      "Epoch 63/200 - Train Loss: 44.268134, Val Loss: 44.545037\n",
      "Epoch 64/200 - Train Loss: 44.207331, Val Loss: 44.477574\n",
      "Epoch 65/200 - Train Loss: 44.148224, Val Loss: 44.411445\n",
      "Epoch 66/200 - Train Loss: 44.090302, Val Loss: 44.358394\n",
      "Epoch 67/200 - Train Loss: 44.033739, Val Loss: 44.294497\n",
      "Epoch 68/200 - Train Loss: 43.978608, Val Loss: 44.243955\n",
      "Epoch 69/200 - Train Loss: 43.924203, Val Loss: 44.181906\n",
      "Epoch 70/200 - Train Loss: 43.871538, Val Loss: 44.134653\n",
      "Epoch 71/200 - Train Loss: 43.820615, Val Loss: 44.079766\n",
      "Epoch 72/200 - Train Loss: 43.769906, Val Loss: 44.022235\n",
      "Epoch 73/200 - Train Loss: 43.721183, Val Loss: 43.976513\n",
      "Epoch 74/200 - Train Loss: 43.673218, Val Loss: 43.925287\n",
      "Epoch 75/200 - Train Loss: 43.625711, Val Loss: 43.879087\n",
      "Epoch 76/200 - Train Loss: 43.580427, Val Loss: 43.838074\n",
      "Epoch 77/200 - Train Loss: 43.535948, Val Loss: 43.785875\n",
      "Epoch 78/200 - Train Loss: 43.492370, Val Loss: 43.734000\n",
      "Epoch 79/200 - Train Loss: 43.450539, Val Loss: 43.695008\n",
      "Epoch 80/200 - Train Loss: 43.409589, Val Loss: 43.653479\n",
      "Epoch 81/200 - Train Loss: 43.368570, Val Loss: 43.597954\n",
      "Epoch 82/200 - Train Loss: 43.329244, Val Loss: 43.563585\n",
      "Epoch 83/200 - Train Loss: 43.290686, Val Loss: 43.525483\n",
      "Epoch 84/200 - Train Loss: 43.253276, Val Loss: 43.483742\n",
      "Epoch 85/200 - Train Loss: 43.216736, Val Loss: 43.442927\n",
      "Epoch 86/200 - Train Loss: 43.181517, Val Loss: 43.409809\n",
      "Epoch 87/200 - Train Loss: 43.147278, Val Loss: 43.373468\n",
      "Epoch 88/200 - Train Loss: 43.113423, Val Loss: 43.337678\n",
      "Epoch 89/200 - Train Loss: 43.080877, Val Loss: 43.301635\n",
      "Epoch 90/200 - Train Loss: 43.049140, Val Loss: 43.261603\n",
      "Epoch 91/200 - Train Loss: 43.017741, Val Loss: 43.230136\n",
      "Epoch 92/200 - Train Loss: 42.988119, Val Loss: 43.193865\n",
      "Epoch 93/200 - Train Loss: 42.957502, Val Loss: 43.168011\n",
      "Epoch 94/200 - Train Loss: 42.929559, Val Loss: 43.137459\n",
      "Epoch 95/200 - Train Loss: 42.901166, Val Loss: 43.107847\n",
      "Epoch 96/200 - Train Loss: 42.874941, Val Loss: 43.072405\n",
      "Epoch 97/200 - Train Loss: 42.848645, Val Loss: 43.046408\n",
      "Epoch 98/200 - Train Loss: 42.822771, Val Loss: 43.011592\n",
      "Epoch 99/200 - Train Loss: 42.798883, Val Loss: 42.988951\n",
      "Epoch 100/200 - Train Loss: 42.774189, Val Loss: 42.963751\n",
      "Epoch 101/200 - Train Loss: 42.750928, Val Loss: 42.930948\n",
      "Epoch 102/200 - Train Loss: 42.728922, Val Loss: 42.907069\n",
      "Epoch 103/200 - Train Loss: 42.706389, Val Loss: 42.882006\n",
      "Epoch 104/200 - Train Loss: 42.685325, Val Loss: 42.853786\n",
      "Epoch 105/200 - Train Loss: 42.664164, Val Loss: 42.828751\n",
      "Epoch 106/200 - Train Loss: 42.643892, Val Loss: 42.803438\n",
      "Epoch 107/200 - Train Loss: 42.624678, Val Loss: 42.786360\n",
      "Epoch 108/200 - Train Loss: 42.605669, Val Loss: 42.768451\n",
      "Epoch 109/200 - Train Loss: 42.586584, Val Loss: 42.742654\n",
      "Epoch 110/200 - Train Loss: 42.569559, Val Loss: 42.718952\n",
      "Epoch 111/200 - Train Loss: 42.552673, Val Loss: 42.696411\n",
      "Epoch 112/200 - Train Loss: 42.535114, Val Loss: 42.678189\n",
      "Epoch 113/200 - Train Loss: 42.519578, Val Loss: 42.657668\n",
      "Epoch 114/200 - Train Loss: 42.503352, Val Loss: 42.637679\n",
      "Epoch 115/200 - Train Loss: 42.488285, Val Loss: 42.618345\n",
      "Epoch 116/200 - Train Loss: 42.474299, Val Loss: 42.611018\n",
      "Epoch 117/200 - Train Loss: 42.460131, Val Loss: 42.585427\n",
      "Epoch 118/200 - Train Loss: 42.446261, Val Loss: 42.567194\n",
      "Epoch 119/200 - Train Loss: 42.432814, Val Loss: 42.549041\n",
      "Epoch 120/200 - Train Loss: 42.420555, Val Loss: 42.530015\n",
      "Epoch 121/200 - Train Loss: 42.408226, Val Loss: 42.514407\n",
      "Epoch 122/200 - Train Loss: 42.396114, Val Loss: 42.500091\n",
      "Epoch 123/200 - Train Loss: 42.385071, Val Loss: 42.484456\n",
      "Epoch 124/200 - Train Loss: 42.373731, Val Loss: 42.472729\n",
      "Epoch 125/200 - Train Loss: 42.363512, Val Loss: 42.453276\n",
      "Epoch 126/200 - Train Loss: 42.353230, Val Loss: 42.438831\n",
      "Epoch 127/200 - Train Loss: 42.343766, Val Loss: 42.426643\n",
      "Epoch 128/200 - Train Loss: 42.333511, Val Loss: 42.414159\n",
      "Epoch 129/200 - Train Loss: 42.324633, Val Loss: 42.398063\n",
      "Epoch 130/200 - Train Loss: 42.316438, Val Loss: 42.386382\n",
      "Epoch 131/200 - Train Loss: 42.307867, Val Loss: 42.376089\n",
      "Epoch 132/200 - Train Loss: 42.299473, Val Loss: 42.363594\n",
      "Epoch 133/200 - Train Loss: 42.291738, Val Loss: 42.351887\n",
      "Epoch 134/200 - Train Loss: 42.284604, Val Loss: 42.337721\n",
      "Epoch 135/200 - Train Loss: 42.277147, Val Loss: 42.331290\n",
      "Epoch 136/200 - Train Loss: 42.270238, Val Loss: 42.312168\n",
      "Epoch 137/200 - Train Loss: 42.263711, Val Loss: 42.301827\n",
      "Epoch 138/200 - Train Loss: 42.257724, Val Loss: 42.294901\n",
      "Epoch 139/200 - Train Loss: 42.251626, Val Loss: 42.286108\n",
      "Epoch 140/200 - Train Loss: 42.245431, Val Loss: 42.272369\n",
      "Epoch 141/200 - Train Loss: 42.240371, Val Loss: 42.263245\n",
      "Epoch 142/200 - Train Loss: 42.234824, Val Loss: 42.258567\n",
      "Epoch 143/200 - Train Loss: 42.229375, Val Loss: 42.242106\n",
      "Epoch 144/200 - Train Loss: 42.224277, Val Loss: 42.245533\n",
      "Epoch 145/200 - Train Loss: 42.220673, Val Loss: 42.229429\n",
      "Epoch 146/200 - Train Loss: 42.215794, Val Loss: 42.225897\n",
      "Epoch 147/200 - Train Loss: 42.211445, Val Loss: 42.214726\n",
      "Epoch 148/200 - Train Loss: 42.207306, Val Loss: 42.206764\n",
      "Epoch 149/200 - Train Loss: 42.203459, Val Loss: 42.199150\n",
      "Epoch 150/200 - Train Loss: 42.199779, Val Loss: 42.182212\n",
      "Epoch 151/200 - Train Loss: 42.196503, Val Loss: 42.183341\n",
      "Epoch 152/200 - Train Loss: 42.193155, Val Loss: 42.179263\n",
      "Epoch 153/200 - Train Loss: 42.189607, Val Loss: 42.174603\n",
      "Epoch 154/200 - Train Loss: 42.187639, Val Loss: 42.158982\n",
      "Epoch 155/200 - Train Loss: 42.184139, Val Loss: 42.159091\n",
      "Epoch 156/200 - Train Loss: 42.181942, Val Loss: 42.151854\n",
      "Epoch 157/200 - Train Loss: 42.178554, Val Loss: 42.145408\n",
      "Epoch 158/200 - Train Loss: 42.176802, Val Loss: 42.142569\n",
      "Epoch 159/200 - Train Loss: 42.173898, Val Loss: 42.132993\n",
      "Epoch 160/200 - Train Loss: 42.172294, Val Loss: 42.135794\n",
      "Epoch 161/200 - Train Loss: 42.169934, Val Loss: 42.125377\n",
      "Epoch 162/200 - Train Loss: 42.167946, Val Loss: 42.115341\n",
      "Epoch 163/200 - Train Loss: 42.165923, Val Loss: 42.120578\n",
      "Epoch 164/200 - Train Loss: 42.163974, Val Loss: 42.110168\n",
      "Epoch 165/200 - Train Loss: 42.162375, Val Loss: 42.103620\n",
      "Epoch 166/200 - Train Loss: 42.160335, Val Loss: 42.102625\n",
      "Epoch 167/200 - Train Loss: 42.159498, Val Loss: 42.093401\n",
      "Epoch 168/200 - Train Loss: 42.158372, Val Loss: 42.093306\n",
      "Epoch 169/200 - Train Loss: 42.157266, Val Loss: 42.083313\n",
      "Epoch 170/200 - Train Loss: 42.155437, Val Loss: 42.090889\n",
      "Epoch 171/200 - Train Loss: 42.154956, Val Loss: 42.076132\n",
      "Epoch 172/200 - Train Loss: 42.153472, Val Loss: 42.074542\n",
      "Epoch 173/200 - Train Loss: 42.152794, Val Loss: 42.076378\n",
      "Epoch 174/200 - Train Loss: 42.151729, Val Loss: 42.075619\n",
      "Epoch 175/200 - Train Loss: 42.151204, Val Loss: 42.066346\n",
      "Epoch 176/200 - Train Loss: 42.149884, Val Loss: 42.071085\n",
      "Epoch 177/200 - Train Loss: 42.149722, Val Loss: 42.062869\n",
      "Epoch 178/200 - Train Loss: 42.148461, Val Loss: 42.057023\n",
      "Epoch 179/200 - Train Loss: 42.147679, Val Loss: 42.055904\n",
      "Epoch 180/200 - Train Loss: 42.147357, Val Loss: 42.050834\n",
      "Epoch 181/200 - Train Loss: 42.146779, Val Loss: 42.048704\n",
      "Epoch 182/200 - Train Loss: 42.146266, Val Loss: 42.053133\n",
      "Epoch 183/200 - Train Loss: 42.145927, Val Loss: 42.044846\n",
      "Epoch 184/200 - Train Loss: 42.145808, Val Loss: 42.047649\n",
      "Epoch 185/200 - Train Loss: 42.145493, Val Loss: 42.044812\n",
      "Epoch 186/200 - Train Loss: 42.144774, Val Loss: 42.039537\n",
      "Epoch 187/200 - Train Loss: 42.144439, Val Loss: 42.037626\n",
      "Epoch 188/200 - Train Loss: 42.145096, Val Loss: 42.036651\n",
      "Epoch 189/200 - Train Loss: 42.143857, Val Loss: 42.031126\n",
      "Epoch 190/200 - Train Loss: 42.143660, Val Loss: 42.028028\n",
      "Epoch 191/200 - Train Loss: 42.143800, Val Loss: 42.035929\n",
      "Epoch 192/200 - Train Loss: 42.143035, Val Loss: 42.027523\n",
      "Epoch 193/200 - Train Loss: 42.143387, Val Loss: 42.036771\n",
      "Epoch 194/200 - Train Loss: 42.143245, Val Loss: 42.025863\n",
      "Epoch 195/200 - Train Loss: 42.143047, Val Loss: 42.019528\n",
      "Epoch 196/200 - Train Loss: 42.142722, Val Loss: 42.026877\n",
      "Epoch 197/200 - Train Loss: 42.142905, Val Loss: 42.024070\n",
      "Epoch 198/200 - Train Loss: 42.142797, Val Loss: 42.026789\n",
      "Epoch 199/200 - Train Loss: 42.142393, Val Loss: 42.020108\n",
      "Epoch 200/200 - Train Loss: 42.142707, Val Loss: 42.015464\n",
      "  Avg Val Score for config {'learning_rate': 0.0001, 'weight_decay': 1e-05}: 42.293211\n",
      "  Testing config: {'learning_rate': 0.0001, 'weight_decay': 0.0001}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 56.852308, Val Loss: 55.908414\n",
      "Epoch 2/200 - Train Loss: 54.814100, Val Loss: 54.655990\n",
      "Epoch 3/200 - Train Loss: 53.894083, Val Loss: 53.905132\n",
      "Epoch 4/200 - Train Loss: 53.248553, Val Loss: 53.320170\n",
      "Epoch 5/200 - Train Loss: 52.724035, Val Loss: 52.830719\n",
      "Epoch 6/200 - Train Loss: 52.275897, Val Loss: 52.408116\n",
      "Epoch 7/200 - Train Loss: 51.877878, Val Loss: 52.017943\n",
      "Epoch 8/200 - Train Loss: 51.521027, Val Loss: 51.680044\n",
      "Epoch 9/200 - Train Loss: 51.197664, Val Loss: 51.362495\n",
      "Epoch 10/200 - Train Loss: 50.899886, Val Loss: 51.070599\n",
      "Epoch 11/200 - Train Loss: 50.624775, Val Loss: 50.807211\n",
      "Epoch 12/200 - Train Loss: 50.366766, Val Loss: 50.547942\n",
      "Epoch 13/200 - Train Loss: 50.123208, Val Loss: 50.308566\n",
      "Epoch 14/200 - Train Loss: 49.895598, Val Loss: 50.089160\n",
      "Epoch 15/200 - Train Loss: 49.677986, Val Loss: 49.872505\n",
      "Epoch 16/200 - Train Loss: 49.469726, Val Loss: 49.667104\n",
      "Epoch 17/200 - Train Loss: 49.271560, Val Loss: 49.471649\n",
      "Epoch 18/200 - Train Loss: 49.081328, Val Loss: 49.285959\n",
      "Epoch 19/200 - Train Loss: 48.899347, Val Loss: 49.114455\n",
      "Epoch 20/200 - Train Loss: 48.722907, Val Loss: 48.943259\n",
      "Epoch 21/200 - Train Loss: 48.551846, Val Loss: 48.773072\n",
      "Epoch 22/200 - Train Loss: 48.387736, Val Loss: 48.625578\n",
      "Epoch 23/200 - Train Loss: 48.228268, Val Loss: 48.473624\n",
      "Epoch 24/200 - Train Loss: 48.073220, Val Loss: 48.321471\n",
      "Epoch 25/200 - Train Loss: 47.924860, Val Loss: 48.168904\n",
      "Epoch 26/200 - Train Loss: 47.775853, Val Loss: 48.038103\n",
      "Epoch 27/200 - Train Loss: 47.634482, Val Loss: 47.905032\n",
      "Epoch 28/200 - Train Loss: 47.495027, Val Loss: 47.763259\n",
      "Epoch 29/200 - Train Loss: 47.359910, Val Loss: 47.636616\n",
      "Epoch 30/200 - Train Loss: 47.229350, Val Loss: 47.524983\n",
      "Epoch 31/200 - Train Loss: 47.099561, Val Loss: 47.403921\n",
      "Epoch 32/200 - Train Loss: 46.974888, Val Loss: 47.293469\n",
      "Epoch 33/200 - Train Loss: 46.852136, Val Loss: 47.168163\n",
      "Epoch 34/200 - Train Loss: 46.731101, Val Loss: 47.052724\n",
      "Epoch 35/200 - Train Loss: 46.613881, Val Loss: 46.947134\n",
      "Epoch 36/200 - Train Loss: 46.498506, Val Loss: 46.839999\n",
      "Epoch 37/200 - Train Loss: 46.386724, Val Loss: 46.744700\n",
      "Epoch 38/200 - Train Loss: 46.274671, Val Loss: 46.629072\n",
      "Epoch 39/200 - Train Loss: 46.167824, Val Loss: 46.536656\n",
      "Epoch 40/200 - Train Loss: 46.062788, Val Loss: 46.439853\n",
      "Epoch 41/200 - Train Loss: 45.959215, Val Loss: 46.345594\n",
      "Epoch 42/200 - Train Loss: 45.857437, Val Loss: 46.266030\n",
      "Epoch 43/200 - Train Loss: 45.757311, Val Loss: 46.152600\n",
      "Epoch 44/200 - Train Loss: 45.660965, Val Loss: 46.091357\n",
      "Epoch 45/200 - Train Loss: 45.565808, Val Loss: 45.988633\n",
      "Epoch 46/200 - Train Loss: 45.473274, Val Loss: 45.910679\n",
      "Epoch 47/200 - Train Loss: 45.380613, Val Loss: 45.828524\n",
      "Epoch 48/200 - Train Loss: 45.290795, Val Loss: 45.747132\n",
      "Epoch 49/200 - Train Loss: 45.202996, Val Loss: 45.672589\n",
      "Epoch 50/200 - Train Loss: 45.115510, Val Loss: 45.581850\n",
      "Epoch 51/200 - Train Loss: 45.030890, Val Loss: 45.511568\n",
      "Epoch 52/200 - Train Loss: 44.948699, Val Loss: 45.450860\n",
      "Epoch 53/200 - Train Loss: 44.867945, Val Loss: 45.372863\n",
      "Epoch 54/200 - Train Loss: 44.788716, Val Loss: 45.303359\n",
      "Epoch 55/200 - Train Loss: 44.709347, Val Loss: 45.239953\n",
      "Epoch 56/200 - Train Loss: 44.633304, Val Loss: 45.191962\n",
      "Epoch 57/200 - Train Loss: 44.557965, Val Loss: 45.130649\n",
      "Epoch 58/200 - Train Loss: 44.485409, Val Loss: 45.027456\n",
      "Epoch 59/200 - Train Loss: 44.412575, Val Loss: 44.983205\n",
      "Epoch 60/200 - Train Loss: 44.342105, Val Loss: 44.933745\n",
      "Epoch 61/200 - Train Loss: 44.272967, Val Loss: 44.860945\n",
      "Epoch 62/200 - Train Loss: 44.204786, Val Loss: 44.806580\n",
      "Epoch 63/200 - Train Loss: 44.138505, Val Loss: 44.745574\n",
      "Epoch 64/200 - Train Loss: 44.072981, Val Loss: 44.698592\n",
      "Epoch 65/200 - Train Loss: 44.009463, Val Loss: 44.653340\n",
      "Epoch 66/200 - Train Loss: 43.946776, Val Loss: 44.606915\n",
      "Epoch 67/200 - Train Loss: 43.885702, Val Loss: 44.561539\n",
      "Epoch 68/200 - Train Loss: 43.825784, Val Loss: 44.501820\n",
      "Epoch 69/200 - Train Loss: 43.766614, Val Loss: 44.450597\n",
      "Epoch 70/200 - Train Loss: 43.709221, Val Loss: 44.400167\n",
      "Epoch 71/200 - Train Loss: 43.653174, Val Loss: 44.355221\n",
      "Epoch 72/200 - Train Loss: 43.597994, Val Loss: 44.320343\n",
      "Epoch 73/200 - Train Loss: 43.544237, Val Loss: 44.277072\n",
      "Epoch 74/200 - Train Loss: 43.491313, Val Loss: 44.239827\n",
      "Epoch 75/200 - Train Loss: 43.439469, Val Loss: 44.192451\n",
      "Epoch 76/200 - Train Loss: 43.388777, Val Loss: 44.145070\n",
      "Epoch 77/200 - Train Loss: 43.339418, Val Loss: 44.122781\n",
      "Epoch 78/200 - Train Loss: 43.290943, Val Loss: 44.097853\n",
      "Epoch 79/200 - Train Loss: 43.244172, Val Loss: 44.057685\n",
      "Epoch 80/200 - Train Loss: 43.198137, Val Loss: 44.009943\n",
      "Epoch 81/200 - Train Loss: 43.152250, Val Loss: 43.983826\n",
      "Epoch 82/200 - Train Loss: 43.107847, Val Loss: 43.944210\n",
      "Epoch 83/200 - Train Loss: 43.064809, Val Loss: 43.910138\n",
      "Epoch 84/200 - Train Loss: 43.021845, Val Loss: 43.880059\n",
      "Epoch 85/200 - Train Loss: 42.980269, Val Loss: 43.857549\n",
      "Epoch 86/200 - Train Loss: 42.939751, Val Loss: 43.823445\n",
      "Epoch 87/200 - Train Loss: 42.900246, Val Loss: 43.789617\n",
      "Epoch 88/200 - Train Loss: 42.861474, Val Loss: 43.766124\n",
      "Epoch 89/200 - Train Loss: 42.823404, Val Loss: 43.745934\n",
      "Epoch 90/200 - Train Loss: 42.787173, Val Loss: 43.716370\n",
      "Epoch 91/200 - Train Loss: 42.750277, Val Loss: 43.689851\n",
      "Epoch 92/200 - Train Loss: 42.715957, Val Loss: 43.677772\n",
      "Epoch 93/200 - Train Loss: 42.679798, Val Loss: 43.654484\n",
      "Epoch 94/200 - Train Loss: 42.646784, Val Loss: 43.622220\n",
      "Epoch 95/200 - Train Loss: 42.613849, Val Loss: 43.615140\n",
      "Epoch 96/200 - Train Loss: 42.581576, Val Loss: 43.594879\n",
      "Epoch 97/200 - Train Loss: 42.550701, Val Loss: 43.571178\n",
      "Epoch 98/200 - Train Loss: 42.519720, Val Loss: 43.558675\n",
      "Epoch 99/200 - Train Loss: 42.489863, Val Loss: 43.527214\n",
      "Epoch 100/200 - Train Loss: 42.460089, Val Loss: 43.512439\n",
      "Epoch 101/200 - Train Loss: 42.431958, Val Loss: 43.512447\n",
      "Epoch 102/200 - Train Loss: 42.405053, Val Loss: 43.489096\n",
      "Epoch 103/200 - Train Loss: 42.376982, Val Loss: 43.468764\n",
      "Epoch 104/200 - Train Loss: 42.351017, Val Loss: 43.461292\n",
      "Epoch 105/200 - Train Loss: 42.324633, Val Loss: 43.449084\n",
      "Epoch 106/200 - Train Loss: 42.299195, Val Loss: 43.430117\n",
      "Epoch 107/200 - Train Loss: 42.274469, Val Loss: 43.428437\n",
      "Epoch 108/200 - Train Loss: 42.251066, Val Loss: 43.399939\n",
      "Epoch 109/200 - Train Loss: 42.227187, Val Loss: 43.395505\n",
      "Epoch 110/200 - Train Loss: 42.204348, Val Loss: 43.383881\n",
      "Epoch 111/200 - Train Loss: 42.182042, Val Loss: 43.378217\n",
      "Epoch 112/200 - Train Loss: 42.160256, Val Loss: 43.371379\n",
      "Epoch 113/200 - Train Loss: 42.139227, Val Loss: 43.357259\n",
      "Epoch 114/200 - Train Loss: 42.118234, Val Loss: 43.354301\n",
      "Epoch 115/200 - Train Loss: 42.098410, Val Loss: 43.337103\n",
      "Epoch 116/200 - Train Loss: 42.078925, Val Loss: 43.321904\n",
      "Epoch 117/200 - Train Loss: 42.060012, Val Loss: 43.334576\n",
      "Epoch 118/200 - Train Loss: 42.041463, Val Loss: 43.322942\n",
      "Epoch 119/200 - Train Loss: 42.022975, Val Loss: 43.312936\n",
      "Epoch 120/200 - Train Loss: 42.006062, Val Loss: 43.319140\n",
      "Epoch 121/200 - Train Loss: 41.988764, Val Loss: 43.303043\n",
      "Epoch 122/200 - Train Loss: 41.972037, Val Loss: 43.298453\n",
      "Epoch 123/200 - Train Loss: 41.956004, Val Loss: 43.308535\n",
      "Epoch 124/200 - Train Loss: 41.939877, Val Loss: 43.281800\n",
      "Epoch 125/200 - Train Loss: 41.924630, Val Loss: 43.297325\n",
      "Epoch 126/200 - Train Loss: 41.909964, Val Loss: 43.289938\n",
      "Epoch 127/200 - Train Loss: 41.895988, Val Loss: 43.279205\n",
      "Epoch 128/200 - Train Loss: 41.881354, Val Loss: 43.292570\n",
      "Epoch 129/200 - Train Loss: 41.868354, Val Loss: 43.292975\n",
      "Epoch 130/200 - Train Loss: 41.855044, Val Loss: 43.280629\n",
      "Epoch 131/200 - Train Loss: 41.842330, Val Loss: 43.276566\n",
      "Epoch 132/200 - Train Loss: 41.829147, Val Loss: 43.285695\n",
      "Epoch 133/200 - Train Loss: 41.817390, Val Loss: 43.276085\n",
      "Epoch 134/200 - Train Loss: 41.805575, Val Loss: 43.283660\n",
      "Epoch 135/200 - Train Loss: 41.794292, Val Loss: 43.275380\n",
      "Epoch 136/200 - Train Loss: 41.782771, Val Loss: 43.287347\n",
      "Epoch 137/200 - Train Loss: 41.772452, Val Loss: 43.280802\n",
      "Epoch 138/200 - Train Loss: 41.762141, Val Loss: 43.290184\n",
      "Epoch 139/200 - Train Loss: 41.752018, Val Loss: 43.284078\n",
      "Epoch 140/200 - Train Loss: 41.741651, Val Loss: 43.301373\n",
      "Epoch 141/200 - Train Loss: 41.732394, Val Loss: 43.291849\n",
      "Epoch 142/200 - Train Loss: 41.723439, Val Loss: 43.296530\n",
      "Epoch 143/200 - Train Loss: 41.713681, Val Loss: 43.296250\n",
      "Epoch 144/200 - Train Loss: 41.705158, Val Loss: 43.293863\n",
      "Epoch 145/200 - Train Loss: 41.697277, Val Loss: 43.294767\n",
      "Epoch 146/200 - Train Loss: 41.688601, Val Loss: 43.299282\n",
      "Epoch 147/200 - Train Loss: 41.681304, Val Loss: 43.295948\n",
      "Epoch 148/200 - Train Loss: 41.673412, Val Loss: 43.301483\n",
      "Epoch 149/200 - Train Loss: 41.665650, Val Loss: 43.307707\n",
      "Epoch 150/200 - Train Loss: 41.658644, Val Loss: 43.328483\n",
      "Early stopping triggered after 150 epochs. Best val loss 43.275380 at epoch 135.\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 56.816713, Val Loss: 55.722200\n",
      "Epoch 2/200 - Train Loss: 54.778458, Val Loss: 54.515932\n",
      "Epoch 3/200 - Train Loss: 53.844412, Val Loss: 53.785892\n",
      "Epoch 4/200 - Train Loss: 53.185596, Val Loss: 53.205970\n",
      "Epoch 5/200 - Train Loss: 52.651913, Val Loss: 52.721145\n",
      "Epoch 6/200 - Train Loss: 52.198429, Val Loss: 52.292433\n",
      "Epoch 7/200 - Train Loss: 51.797466, Val Loss: 51.916610\n",
      "Epoch 8/200 - Train Loss: 51.439439, Val Loss: 51.568709\n",
      "Epoch 9/200 - Train Loss: 51.115172, Val Loss: 51.251169\n",
      "Epoch 10/200 - Train Loss: 50.817318, Val Loss: 50.959713\n",
      "Epoch 11/200 - Train Loss: 50.542131, Val Loss: 50.686267\n",
      "Epoch 12/200 - Train Loss: 50.284152, Val Loss: 50.430445\n",
      "Epoch 13/200 - Train Loss: 50.042155, Val Loss: 50.193484\n",
      "Epoch 14/200 - Train Loss: 49.815907, Val Loss: 49.964079\n",
      "Epoch 15/200 - Train Loss: 49.600592, Val Loss: 49.745650\n",
      "Epoch 16/200 - Train Loss: 49.393842, Val Loss: 49.543278\n",
      "Epoch 17/200 - Train Loss: 49.198586, Val Loss: 49.344441\n",
      "Epoch 18/200 - Train Loss: 49.011462, Val Loss: 49.154487\n",
      "Epoch 19/200 - Train Loss: 48.832305, Val Loss: 48.971034\n",
      "Epoch 20/200 - Train Loss: 48.660334, Val Loss: 48.795313\n",
      "Epoch 21/200 - Train Loss: 48.493380, Val Loss: 48.630385\n",
      "Epoch 22/200 - Train Loss: 48.333158, Val Loss: 48.460863\n",
      "Epoch 23/200 - Train Loss: 48.178211, Val Loss: 48.303439\n",
      "Epoch 24/200 - Train Loss: 48.028447, Val Loss: 48.149280\n",
      "Epoch 25/200 - Train Loss: 47.885000, Val Loss: 48.008156\n",
      "Epoch 26/200 - Train Loss: 47.741679, Val Loss: 47.854492\n",
      "Epoch 27/200 - Train Loss: 47.605830, Val Loss: 47.712122\n",
      "Epoch 28/200 - Train Loss: 47.471727, Val Loss: 47.574829\n",
      "Epoch 29/200 - Train Loss: 47.342282, Val Loss: 47.444392\n",
      "Epoch 30/200 - Train Loss: 47.216778, Val Loss: 47.309105\n",
      "Epoch 31/200 - Train Loss: 47.092897, Val Loss: 47.181477\n",
      "Epoch 32/200 - Train Loss: 46.973685, Val Loss: 47.053267\n",
      "Epoch 33/200 - Train Loss: 46.857171, Val Loss: 46.930995\n",
      "Epoch 34/200 - Train Loss: 46.740805, Val Loss: 46.813044\n",
      "Epoch 35/200 - Train Loss: 46.629761, Val Loss: 46.694169\n",
      "Epoch 36/200 - Train Loss: 46.520068, Val Loss: 46.576971\n",
      "Epoch 37/200 - Train Loss: 46.414143, Val Loss: 46.462756\n",
      "Epoch 38/200 - Train Loss: 46.307720, Val Loss: 46.357640\n",
      "Epoch 39/200 - Train Loss: 46.207027, Val Loss: 46.244823\n",
      "Epoch 40/200 - Train Loss: 46.107920, Val Loss: 46.141363\n",
      "Epoch 41/200 - Train Loss: 46.009693, Val Loss: 46.035625\n",
      "Epoch 42/200 - Train Loss: 45.914049, Val Loss: 45.929059\n",
      "Epoch 43/200 - Train Loss: 45.819677, Val Loss: 45.843448\n",
      "Epoch 44/200 - Train Loss: 45.729761, Val Loss: 45.732417\n",
      "Epoch 45/200 - Train Loss: 45.640001, Val Loss: 45.638640\n",
      "Epoch 46/200 - Train Loss: 45.553316, Val Loss: 45.544342\n",
      "Epoch 47/200 - Train Loss: 45.466699, Val Loss: 45.452076\n",
      "Epoch 48/200 - Train Loss: 45.382935, Val Loss: 45.366450\n",
      "Epoch 49/200 - Train Loss: 45.300979, Val Loss: 45.273159\n",
      "Epoch 50/200 - Train Loss: 45.219972, Val Loss: 45.190902\n",
      "Epoch 51/200 - Train Loss: 45.141704, Val Loss: 45.110525\n",
      "Epoch 52/200 - Train Loss: 45.065111, Val Loss: 45.019705\n",
      "Epoch 53/200 - Train Loss: 44.990459, Val Loss: 44.938933\n",
      "Epoch 54/200 - Train Loss: 44.917141, Val Loss: 44.858027\n",
      "Epoch 55/200 - Train Loss: 44.844320, Val Loss: 44.777751\n",
      "Epoch 56/200 - Train Loss: 44.773769, Val Loss: 44.694018\n",
      "Epoch 57/200 - Train Loss: 44.705538, Val Loss: 44.619026\n",
      "Epoch 58/200 - Train Loss: 44.638617, Val Loss: 44.560583\n",
      "Epoch 59/200 - Train Loss: 44.571800, Val Loss: 44.480076\n",
      "Epoch 60/200 - Train Loss: 44.507823, Val Loss: 44.407975\n",
      "Epoch 61/200 - Train Loss: 44.444627, Val Loss: 44.339150\n",
      "Epoch 62/200 - Train Loss: 44.383046, Val Loss: 44.269548\n",
      "Epoch 63/200 - Train Loss: 44.322675, Val Loss: 44.205364\n",
      "Epoch 64/200 - Train Loss: 44.263384, Val Loss: 44.139871\n",
      "Epoch 65/200 - Train Loss: 44.205811, Val Loss: 44.068429\n",
      "Epoch 66/200 - Train Loss: 44.149536, Val Loss: 44.006793\n",
      "Epoch 67/200 - Train Loss: 44.094443, Val Loss: 43.947680\n",
      "Epoch 68/200 - Train Loss: 44.040821, Val Loss: 43.890593\n",
      "Epoch 69/200 - Train Loss: 43.988080, Val Loss: 43.827661\n",
      "Epoch 70/200 - Train Loss: 43.936914, Val Loss: 43.776064\n",
      "Epoch 71/200 - Train Loss: 43.887187, Val Loss: 43.718666\n",
      "Epoch 72/200 - Train Loss: 43.837759, Val Loss: 43.657900\n",
      "Epoch 73/200 - Train Loss: 43.790186, Val Loss: 43.603423\n",
      "Epoch 74/200 - Train Loss: 43.743697, Val Loss: 43.547816\n",
      "Epoch 75/200 - Train Loss: 43.697767, Val Loss: 43.499998\n",
      "Epoch 76/200 - Train Loss: 43.653712, Val Loss: 43.457450\n",
      "Epoch 77/200 - Train Loss: 43.610214, Val Loss: 43.399586\n",
      "Epoch 78/200 - Train Loss: 43.567583, Val Loss: 43.345982\n",
      "Epoch 79/200 - Train Loss: 43.527417, Val Loss: 43.303953\n",
      "Epoch 80/200 - Train Loss: 43.487435, Val Loss: 43.256583\n",
      "Epoch 81/200 - Train Loss: 43.447476, Val Loss: 43.206938\n",
      "Epoch 82/200 - Train Loss: 43.409008, Val Loss: 43.168044\n",
      "Epoch 83/200 - Train Loss: 43.371604, Val Loss: 43.127455\n",
      "Epoch 84/200 - Train Loss: 43.334893, Val Loss: 43.078816\n",
      "Epoch 85/200 - Train Loss: 43.299394, Val Loss: 43.039351\n",
      "Epoch 86/200 - Train Loss: 43.265165, Val Loss: 43.002300\n",
      "Epoch 87/200 - Train Loss: 43.231645, Val Loss: 42.960284\n",
      "Epoch 88/200 - Train Loss: 43.198837, Val Loss: 42.924769\n",
      "Epoch 89/200 - Train Loss: 43.166846, Val Loss: 42.879923\n",
      "Epoch 90/200 - Train Loss: 43.136113, Val Loss: 42.841650\n",
      "Epoch 91/200 - Train Loss: 43.105140, Val Loss: 42.809165\n",
      "Epoch 92/200 - Train Loss: 43.076913, Val Loss: 42.771259\n",
      "Epoch 93/200 - Train Loss: 43.046756, Val Loss: 42.737830\n",
      "Epoch 94/200 - Train Loss: 43.019503, Val Loss: 42.704192\n",
      "Epoch 95/200 - Train Loss: 42.991941, Val Loss: 42.673385\n",
      "Epoch 96/200 - Train Loss: 42.965914, Val Loss: 42.637383\n",
      "Epoch 97/200 - Train Loss: 42.940131, Val Loss: 42.610519\n",
      "Epoch 98/200 - Train Loss: 42.915015, Val Loss: 42.575755\n",
      "Epoch 99/200 - Train Loss: 42.891355, Val Loss: 42.548520\n",
      "Epoch 100/200 - Train Loss: 42.867424, Val Loss: 42.518049\n",
      "Epoch 101/200 - Train Loss: 42.844838, Val Loss: 42.487482\n",
      "Epoch 102/200 - Train Loss: 42.822897, Val Loss: 42.460446\n",
      "Epoch 103/200 - Train Loss: 42.801021, Val Loss: 42.438290\n",
      "Epoch 104/200 - Train Loss: 42.780475, Val Loss: 42.409535\n",
      "Epoch 105/200 - Train Loss: 42.759731, Val Loss: 42.385151\n",
      "Epoch 106/200 - Train Loss: 42.739897, Val Loss: 42.357657\n",
      "Epoch 107/200 - Train Loss: 42.720590, Val Loss: 42.334302\n",
      "Epoch 108/200 - Train Loss: 42.702491, Val Loss: 42.314398\n",
      "Epoch 109/200 - Train Loss: 42.683837, Val Loss: 42.294934\n",
      "Epoch 110/200 - Train Loss: 42.666984, Val Loss: 42.269824\n",
      "Epoch 111/200 - Train Loss: 42.649841, Val Loss: 42.241859\n",
      "Epoch 112/200 - Train Loss: 42.633011, Val Loss: 42.224782\n",
      "Epoch 113/200 - Train Loss: 42.617401, Val Loss: 42.203174\n",
      "Epoch 114/200 - Train Loss: 42.601749, Val Loss: 42.181566\n",
      "Epoch 115/200 - Train Loss: 42.587202, Val Loss: 42.161505\n",
      "Epoch 116/200 - Train Loss: 42.572541, Val Loss: 42.151657\n",
      "Epoch 117/200 - Train Loss: 42.559336, Val Loss: 42.122729\n",
      "Epoch 118/200 - Train Loss: 42.545380, Val Loss: 42.112680\n",
      "Epoch 119/200 - Train Loss: 42.532472, Val Loss: 42.091658\n",
      "Epoch 120/200 - Train Loss: 42.520088, Val Loss: 42.070632\n",
      "Epoch 121/200 - Train Loss: 42.507750, Val Loss: 42.055488\n",
      "Epoch 122/200 - Train Loss: 42.495983, Val Loss: 42.037362\n",
      "Epoch 123/200 - Train Loss: 42.485162, Val Loss: 42.020610\n",
      "Epoch 124/200 - Train Loss: 42.473206, Val Loss: 42.010493\n",
      "Epoch 125/200 - Train Loss: 42.463213, Val Loss: 41.988903\n",
      "Epoch 126/200 - Train Loss: 42.453274, Val Loss: 41.977476\n",
      "Epoch 127/200 - Train Loss: 42.443303, Val Loss: 41.964915\n",
      "Epoch 128/200 - Train Loss: 42.433251, Val Loss: 41.950388\n",
      "Epoch 129/200 - Train Loss: 42.424352, Val Loss: 41.938700\n",
      "Epoch 130/200 - Train Loss: 42.416275, Val Loss: 41.927893\n",
      "Epoch 131/200 - Train Loss: 42.407648, Val Loss: 41.919204\n",
      "Epoch 132/200 - Train Loss: 42.399219, Val Loss: 41.903204\n",
      "Epoch 133/200 - Train Loss: 42.391644, Val Loss: 41.890347\n",
      "Epoch 134/200 - Train Loss: 42.384201, Val Loss: 41.881531\n",
      "Epoch 135/200 - Train Loss: 42.376567, Val Loss: 41.873485\n",
      "Epoch 136/200 - Train Loss: 42.369589, Val Loss: 41.857882\n",
      "Epoch 137/200 - Train Loss: 42.363424, Val Loss: 41.844337\n",
      "Epoch 138/200 - Train Loss: 42.357094, Val Loss: 41.832552\n",
      "Epoch 139/200 - Train Loss: 42.350929, Val Loss: 41.829221\n",
      "Epoch 140/200 - Train Loss: 42.344451, Val Loss: 41.814707\n",
      "Epoch 141/200 - Train Loss: 42.339069, Val Loss: 41.813286\n",
      "Epoch 142/200 - Train Loss: 42.333758, Val Loss: 41.803019\n",
      "Epoch 143/200 - Train Loss: 42.327865, Val Loss: 41.796363\n",
      "Epoch 144/200 - Train Loss: 42.322877, Val Loss: 41.792374\n",
      "Epoch 145/200 - Train Loss: 42.318666, Val Loss: 41.778735\n",
      "Epoch 146/200 - Train Loss: 42.313723, Val Loss: 41.775623\n",
      "Epoch 147/200 - Train Loss: 42.309473, Val Loss: 41.767258\n",
      "Epoch 148/200 - Train Loss: 42.305121, Val Loss: 41.762195\n",
      "Epoch 149/200 - Train Loss: 42.300959, Val Loss: 41.752548\n",
      "Epoch 150/200 - Train Loss: 42.296698, Val Loss: 41.740637\n",
      "Epoch 151/200 - Train Loss: 42.293528, Val Loss: 41.735396\n",
      "Epoch 152/200 - Train Loss: 42.289968, Val Loss: 41.729886\n",
      "Epoch 153/200 - Train Loss: 42.286236, Val Loss: 41.725945\n",
      "Epoch 154/200 - Train Loss: 42.283821, Val Loss: 41.715884\n",
      "Epoch 155/200 - Train Loss: 42.280155, Val Loss: 41.714149\n",
      "Epoch 156/200 - Train Loss: 42.278136, Val Loss: 41.711994\n",
      "Epoch 157/200 - Train Loss: 42.274350, Val Loss: 41.703207\n",
      "Epoch 158/200 - Train Loss: 42.272441, Val Loss: 41.702727\n",
      "Epoch 159/200 - Train Loss: 42.269579, Val Loss: 41.690911\n",
      "Epoch 160/200 - Train Loss: 42.267872, Val Loss: 41.694500\n",
      "Epoch 161/200 - Train Loss: 42.264675, Val Loss: 41.687157\n",
      "Epoch 162/200 - Train Loss: 42.263129, Val Loss: 41.676573\n",
      "Epoch 163/200 - Train Loss: 42.261014, Val Loss: 41.678973\n",
      "Epoch 164/200 - Train Loss: 42.258696, Val Loss: 41.672096\n",
      "Epoch 165/200 - Train Loss: 42.256619, Val Loss: 41.665692\n",
      "Epoch 166/200 - Train Loss: 42.255017, Val Loss: 41.671971\n",
      "Epoch 167/200 - Train Loss: 42.253607, Val Loss: 41.657566\n",
      "Epoch 168/200 - Train Loss: 42.252294, Val Loss: 41.661133\n",
      "Epoch 169/200 - Train Loss: 42.250684, Val Loss: 41.653678\n",
      "Epoch 170/200 - Train Loss: 42.248916, Val Loss: 41.653835\n",
      "Epoch 171/200 - Train Loss: 42.248194, Val Loss: 41.644176\n",
      "Epoch 172/200 - Train Loss: 42.246420, Val Loss: 41.644135\n",
      "Epoch 173/200 - Train Loss: 42.245433, Val Loss: 41.643400\n",
      "Epoch 174/200 - Train Loss: 42.244200, Val Loss: 41.642430\n",
      "Epoch 175/200 - Train Loss: 42.243193, Val Loss: 41.643801\n",
      "Epoch 176/200 - Train Loss: 42.242195, Val Loss: 41.639499\n",
      "Epoch 177/200 - Train Loss: 42.241426, Val Loss: 41.631868\n",
      "Epoch 178/200 - Train Loss: 42.240862, Val Loss: 41.628935\n",
      "Epoch 179/200 - Train Loss: 42.239406, Val Loss: 41.623999\n",
      "Epoch 180/200 - Train Loss: 42.238880, Val Loss: 41.623736\n",
      "Epoch 181/200 - Train Loss: 42.238075, Val Loss: 41.622591\n",
      "Epoch 182/200 - Train Loss: 42.237616, Val Loss: 41.623344\n",
      "Epoch 183/200 - Train Loss: 42.237139, Val Loss: 41.615227\n",
      "Epoch 184/200 - Train Loss: 42.236580, Val Loss: 41.617507\n",
      "Epoch 185/200 - Train Loss: 42.236188, Val Loss: 41.618723\n",
      "Epoch 186/200 - Train Loss: 42.236251, Val Loss: 41.612398\n",
      "Epoch 187/200 - Train Loss: 42.234806, Val Loss: 41.612972\n",
      "Epoch 188/200 - Train Loss: 42.235467, Val Loss: 41.608890\n",
      "Epoch 189/200 - Train Loss: 42.234585, Val Loss: 41.608835\n",
      "Epoch 190/200 - Train Loss: 42.234046, Val Loss: 41.604328\n",
      "Epoch 191/200 - Train Loss: 42.233566, Val Loss: 41.610534\n",
      "Epoch 192/200 - Train Loss: 42.232990, Val Loss: 41.610148\n",
      "Epoch 193/200 - Train Loss: 42.233562, Val Loss: 41.608098\n",
      "Epoch 194/200 - Train Loss: 42.232901, Val Loss: 41.604529\n",
      "Epoch 195/200 - Train Loss: 42.232803, Val Loss: 41.600924\n",
      "Epoch 196/200 - Train Loss: 42.232256, Val Loss: 41.605445\n",
      "Epoch 197/200 - Train Loss: 42.232425, Val Loss: 41.600892\n",
      "Epoch 198/200 - Train Loss: 42.232442, Val Loss: 41.600240\n",
      "Epoch 199/200 - Train Loss: 42.231566, Val Loss: 41.593857\n",
      "Epoch 200/200 - Train Loss: 42.231941, Val Loss: 41.588790\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Epoch 1/200 - Train Loss: 57.156166, Val Loss: 55.318243\n",
      "Epoch 2/200 - Train Loss: 55.039244, Val Loss: 54.138834\n",
      "Epoch 3/200 - Train Loss: 54.091160, Val Loss: 53.424156\n",
      "Epoch 4/200 - Train Loss: 53.423358, Val Loss: 52.859311\n",
      "Epoch 5/200 - Train Loss: 52.876531, Val Loss: 52.388842\n",
      "Epoch 6/200 - Train Loss: 52.408380, Val Loss: 51.974056\n",
      "Epoch 7/200 - Train Loss: 51.991319, Val Loss: 51.618106\n",
      "Epoch 8/200 - Train Loss: 51.618875, Val Loss: 51.287865\n",
      "Epoch 9/200 - Train Loss: 51.280604, Val Loss: 50.997194\n",
      "Epoch 10/200 - Train Loss: 50.969216, Val Loss: 50.718552\n",
      "Epoch 11/200 - Train Loss: 50.681300, Val Loss: 50.464441\n",
      "Epoch 12/200 - Train Loss: 50.411247, Val Loss: 50.229009\n",
      "Epoch 13/200 - Train Loss: 50.157469, Val Loss: 50.011960\n",
      "Epoch 14/200 - Train Loss: 49.920462, Val Loss: 49.804284\n",
      "Epoch 15/200 - Train Loss: 49.694969, Val Loss: 49.605039\n",
      "Epoch 16/200 - Train Loss: 49.480448, Val Loss: 49.420049\n",
      "Epoch 17/200 - Train Loss: 49.275700, Val Loss: 49.244648\n",
      "Epoch 18/200 - Train Loss: 49.080545, Val Loss: 49.069681\n",
      "Epoch 19/200 - Train Loss: 48.894290, Val Loss: 48.911325\n",
      "Epoch 20/200 - Train Loss: 48.715347, Val Loss: 48.754623\n",
      "Epoch 21/200 - Train Loss: 48.542135, Val Loss: 48.602117\n",
      "Epoch 22/200 - Train Loss: 48.375994, Val Loss: 48.454495\n",
      "Epoch 23/200 - Train Loss: 48.215655, Val Loss: 48.310434\n",
      "Epoch 24/200 - Train Loss: 48.061147, Val Loss: 48.177362\n",
      "Epoch 25/200 - Train Loss: 47.913285, Val Loss: 48.048611\n",
      "Epoch 26/200 - Train Loss: 47.766176, Val Loss: 47.918944\n",
      "Epoch 27/200 - Train Loss: 47.626502, Val Loss: 47.790941\n",
      "Epoch 28/200 - Train Loss: 47.488531, Val Loss: 47.664167\n",
      "Epoch 29/200 - Train Loss: 47.356316, Val Loss: 47.549152\n",
      "Epoch 30/200 - Train Loss: 47.228141, Val Loss: 47.430771\n",
      "Epoch 31/200 - Train Loss: 47.101462, Val Loss: 47.323829\n",
      "Epoch 32/200 - Train Loss: 46.979853, Val Loss: 47.198512\n",
      "Epoch 33/200 - Train Loss: 46.860912, Val Loss: 47.093620\n",
      "Epoch 34/200 - Train Loss: 46.743030, Val Loss: 46.982042\n",
      "Epoch 35/200 - Train Loss: 46.629673, Val Loss: 46.874644\n",
      "Epoch 36/200 - Train Loss: 46.517325, Val Loss: 46.768659\n",
      "Epoch 37/200 - Train Loss: 46.409633, Val Loss: 46.663246\n",
      "Epoch 38/200 - Train Loss: 46.301182, Val Loss: 46.557462\n",
      "Epoch 39/200 - Train Loss: 46.198454, Val Loss: 46.462815\n",
      "Epoch 40/200 - Train Loss: 46.097309, Val Loss: 46.366564\n",
      "Epoch 41/200 - Train Loss: 45.996840, Val Loss: 46.261395\n",
      "Epoch 42/200 - Train Loss: 45.899550, Val Loss: 46.164879\n",
      "Epoch 43/200 - Train Loss: 45.802958, Val Loss: 46.081287\n",
      "Epoch 44/200 - Train Loss: 45.710992, Val Loss: 45.978740\n",
      "Epoch 45/200 - Train Loss: 45.619290, Val Loss: 45.889894\n",
      "Epoch 46/200 - Train Loss: 45.530530, Val Loss: 45.803362\n",
      "Epoch 47/200 - Train Loss: 45.442035, Val Loss: 45.717643\n",
      "Epoch 48/200 - Train Loss: 45.356430, Val Loss: 45.635552\n",
      "Epoch 49/200 - Train Loss: 45.272258, Val Loss: 45.547288\n",
      "Epoch 50/200 - Train Loss: 45.189097, Val Loss: 45.467651\n",
      "Epoch 51/200 - Train Loss: 45.108841, Val Loss: 45.390977\n",
      "Epoch 52/200 - Train Loss: 45.030510, Val Loss: 45.304491\n",
      "Epoch 53/200 - Train Loss: 44.953750, Val Loss: 45.229863\n",
      "Epoch 54/200 - Train Loss: 44.878311, Val Loss: 45.156694\n",
      "Epoch 55/200 - Train Loss: 44.803816, Val Loss: 45.079744\n",
      "Epoch 56/200 - Train Loss: 44.731445, Val Loss: 44.998580\n",
      "Epoch 57/200 - Train Loss: 44.661457, Val Loss: 44.933911\n",
      "Epoch 58/200 - Train Loss: 44.592567, Val Loss: 44.878758\n",
      "Epoch 59/200 - Train Loss: 44.524132, Val Loss: 44.792995\n",
      "Epoch 60/200 - Train Loss: 44.458378, Val Loss: 44.729119\n",
      "Epoch 61/200 - Train Loss: 44.393596, Val Loss: 44.663199\n",
      "Epoch 62/200 - Train Loss: 44.330306, Val Loss: 44.598953\n",
      "Epoch 63/200 - Train Loss: 44.268134, Val Loss: 44.545037\n",
      "Epoch 64/200 - Train Loss: 44.207331, Val Loss: 44.477574\n",
      "Epoch 65/200 - Train Loss: 44.148224, Val Loss: 44.411445\n",
      "Epoch 66/200 - Train Loss: 44.090302, Val Loss: 44.358394\n",
      "Epoch 67/200 - Train Loss: 44.033739, Val Loss: 44.294497\n",
      "Epoch 68/200 - Train Loss: 43.978608, Val Loss: 44.243955\n",
      "Epoch 69/200 - Train Loss: 43.924203, Val Loss: 44.181906\n",
      "Epoch 70/200 - Train Loss: 43.871538, Val Loss: 44.134653\n",
      "Epoch 71/200 - Train Loss: 43.820615, Val Loss: 44.079766\n",
      "Epoch 72/200 - Train Loss: 43.769906, Val Loss: 44.022235\n",
      "Epoch 73/200 - Train Loss: 43.721183, Val Loss: 43.976513\n",
      "Epoch 74/200 - Train Loss: 43.673218, Val Loss: 43.925287\n",
      "Epoch 75/200 - Train Loss: 43.625711, Val Loss: 43.879087\n",
      "Epoch 76/200 - Train Loss: 43.580427, Val Loss: 43.838074\n",
      "Epoch 77/200 - Train Loss: 43.535948, Val Loss: 43.785875\n",
      "Epoch 78/200 - Train Loss: 43.492370, Val Loss: 43.734000\n",
      "Epoch 79/200 - Train Loss: 43.450539, Val Loss: 43.695008\n",
      "Epoch 80/200 - Train Loss: 43.409589, Val Loss: 43.653479\n",
      "Epoch 81/200 - Train Loss: 43.368570, Val Loss: 43.597954\n",
      "Epoch 82/200 - Train Loss: 43.329244, Val Loss: 43.563585\n",
      "Epoch 83/200 - Train Loss: 43.290686, Val Loss: 43.525483\n",
      "Epoch 84/200 - Train Loss: 43.253276, Val Loss: 43.483742\n",
      "Epoch 85/200 - Train Loss: 43.216736, Val Loss: 43.442927\n",
      "Epoch 86/200 - Train Loss: 43.181517, Val Loss: 43.409809\n",
      "Epoch 87/200 - Train Loss: 43.147278, Val Loss: 43.373468\n",
      "Epoch 88/200 - Train Loss: 43.113423, Val Loss: 43.337678\n",
      "Epoch 89/200 - Train Loss: 43.080877, Val Loss: 43.301635\n",
      "Epoch 90/200 - Train Loss: 43.049140, Val Loss: 43.261603\n",
      "Epoch 91/200 - Train Loss: 43.017741, Val Loss: 43.230136\n",
      "Epoch 92/200 - Train Loss: 42.988119, Val Loss: 43.193865\n",
      "Epoch 93/200 - Train Loss: 42.957502, Val Loss: 43.168011\n",
      "Epoch 94/200 - Train Loss: 42.929559, Val Loss: 43.137459\n",
      "Epoch 95/200 - Train Loss: 42.901166, Val Loss: 43.107847\n",
      "Epoch 96/200 - Train Loss: 42.874941, Val Loss: 43.072405\n",
      "Epoch 97/200 - Train Loss: 42.848645, Val Loss: 43.046408\n",
      "Epoch 98/200 - Train Loss: 42.822771, Val Loss: 43.011592\n",
      "Epoch 99/200 - Train Loss: 42.798883, Val Loss: 42.988951\n",
      "Epoch 100/200 - Train Loss: 42.774189, Val Loss: 42.963751\n",
      "Epoch 101/200 - Train Loss: 42.750928, Val Loss: 42.930948\n",
      "Epoch 102/200 - Train Loss: 42.728922, Val Loss: 42.907069\n",
      "Epoch 103/200 - Train Loss: 42.706389, Val Loss: 42.882006\n",
      "Epoch 104/200 - Train Loss: 42.685325, Val Loss: 42.853786\n",
      "Epoch 105/200 - Train Loss: 42.664164, Val Loss: 42.828751\n",
      "Epoch 106/200 - Train Loss: 42.643892, Val Loss: 42.803438\n",
      "Epoch 107/200 - Train Loss: 42.624678, Val Loss: 42.786360\n",
      "Epoch 108/200 - Train Loss: 42.605669, Val Loss: 42.768451\n",
      "Epoch 109/200 - Train Loss: 42.586584, Val Loss: 42.742654\n",
      "Epoch 110/200 - Train Loss: 42.569559, Val Loss: 42.718952\n",
      "Epoch 111/200 - Train Loss: 42.552673, Val Loss: 42.696411\n",
      "Epoch 112/200 - Train Loss: 42.535114, Val Loss: 42.678189\n",
      "Epoch 113/200 - Train Loss: 42.519578, Val Loss: 42.657668\n",
      "Epoch 114/200 - Train Loss: 42.503352, Val Loss: 42.637679\n",
      "Epoch 115/200 - Train Loss: 42.488285, Val Loss: 42.618345\n",
      "Epoch 116/200 - Train Loss: 42.474299, Val Loss: 42.611018\n",
      "Epoch 117/200 - Train Loss: 42.460131, Val Loss: 42.585427\n",
      "Epoch 118/200 - Train Loss: 42.446261, Val Loss: 42.567194\n",
      "Epoch 119/200 - Train Loss: 42.432814, Val Loss: 42.549041\n",
      "Epoch 120/200 - Train Loss: 42.420555, Val Loss: 42.530015\n",
      "Epoch 121/200 - Train Loss: 42.408226, Val Loss: 42.514407\n",
      "Epoch 122/200 - Train Loss: 42.396114, Val Loss: 42.500091\n",
      "Epoch 123/200 - Train Loss: 42.385071, Val Loss: 42.484456\n",
      "Epoch 124/200 - Train Loss: 42.373731, Val Loss: 42.472729\n",
      "Epoch 125/200 - Train Loss: 42.363512, Val Loss: 42.453276\n",
      "Epoch 126/200 - Train Loss: 42.353230, Val Loss: 42.438831\n",
      "Epoch 127/200 - Train Loss: 42.343766, Val Loss: 42.426643\n",
      "Epoch 128/200 - Train Loss: 42.333511, Val Loss: 42.414159\n",
      "Epoch 129/200 - Train Loss: 42.324633, Val Loss: 42.398063\n",
      "Epoch 130/200 - Train Loss: 42.316438, Val Loss: 42.386382\n",
      "Epoch 131/200 - Train Loss: 42.307867, Val Loss: 42.376089\n",
      "Epoch 132/200 - Train Loss: 42.299473, Val Loss: 42.363594\n",
      "Epoch 133/200 - Train Loss: 42.291738, Val Loss: 42.351887\n",
      "Epoch 134/200 - Train Loss: 42.284604, Val Loss: 42.337721\n",
      "Epoch 135/200 - Train Loss: 42.277147, Val Loss: 42.331290\n",
      "Epoch 136/200 - Train Loss: 42.270238, Val Loss: 42.312168\n",
      "Epoch 137/200 - Train Loss: 42.263711, Val Loss: 42.301827\n",
      "Epoch 138/200 - Train Loss: 42.257724, Val Loss: 42.294901\n",
      "Epoch 139/200 - Train Loss: 42.251626, Val Loss: 42.286108\n",
      "Epoch 140/200 - Train Loss: 42.245431, Val Loss: 42.272369\n",
      "Epoch 141/200 - Train Loss: 42.240371, Val Loss: 42.263245\n",
      "Epoch 142/200 - Train Loss: 42.234824, Val Loss: 42.258567\n",
      "Epoch 143/200 - Train Loss: 42.229375, Val Loss: 42.242106\n",
      "Epoch 144/200 - Train Loss: 42.224277, Val Loss: 42.245533\n",
      "Epoch 145/200 - Train Loss: 42.220673, Val Loss: 42.229429\n",
      "Epoch 146/200 - Train Loss: 42.215794, Val Loss: 42.225897\n",
      "Epoch 147/200 - Train Loss: 42.211445, Val Loss: 42.214726\n",
      "Epoch 148/200 - Train Loss: 42.207306, Val Loss: 42.206764\n",
      "Epoch 149/200 - Train Loss: 42.203459, Val Loss: 42.199150\n",
      "Epoch 150/200 - Train Loss: 42.199779, Val Loss: 42.182212\n",
      "Epoch 151/200 - Train Loss: 42.196503, Val Loss: 42.183341\n",
      "Epoch 152/200 - Train Loss: 42.193155, Val Loss: 42.179263\n",
      "Epoch 153/200 - Train Loss: 42.189607, Val Loss: 42.174603\n",
      "Epoch 154/200 - Train Loss: 42.187639, Val Loss: 42.158982\n",
      "Epoch 155/200 - Train Loss: 42.184139, Val Loss: 42.159091\n",
      "Epoch 156/200 - Train Loss: 42.181942, Val Loss: 42.151854\n",
      "Epoch 157/200 - Train Loss: 42.178554, Val Loss: 42.145408\n",
      "Epoch 158/200 - Train Loss: 42.176802, Val Loss: 42.142569\n",
      "Epoch 159/200 - Train Loss: 42.173898, Val Loss: 42.132993\n",
      "Epoch 160/200 - Train Loss: 42.172294, Val Loss: 42.135794\n",
      "Epoch 161/200 - Train Loss: 42.169934, Val Loss: 42.125377\n",
      "Epoch 162/200 - Train Loss: 42.167946, Val Loss: 42.115341\n",
      "Epoch 163/200 - Train Loss: 42.165923, Val Loss: 42.120578\n",
      "Epoch 164/200 - Train Loss: 42.163974, Val Loss: 42.110168\n",
      "Epoch 165/200 - Train Loss: 42.162375, Val Loss: 42.103620\n",
      "Epoch 166/200 - Train Loss: 42.160335, Val Loss: 42.102625\n",
      "Epoch 167/200 - Train Loss: 42.159498, Val Loss: 42.093401\n",
      "Epoch 168/200 - Train Loss: 42.158372, Val Loss: 42.093306\n",
      "Epoch 169/200 - Train Loss: 42.157266, Val Loss: 42.083313\n",
      "Epoch 170/200 - Train Loss: 42.155437, Val Loss: 42.090889\n",
      "Epoch 171/200 - Train Loss: 42.154956, Val Loss: 42.076132\n",
      "Epoch 172/200 - Train Loss: 42.153472, Val Loss: 42.074542\n",
      "Epoch 173/200 - Train Loss: 42.152794, Val Loss: 42.076378\n",
      "Epoch 174/200 - Train Loss: 42.151729, Val Loss: 42.075619\n",
      "Epoch 175/200 - Train Loss: 42.151204, Val Loss: 42.066346\n",
      "Epoch 176/200 - Train Loss: 42.149884, Val Loss: 42.071085\n",
      "Epoch 177/200 - Train Loss: 42.149722, Val Loss: 42.062869\n",
      "Epoch 178/200 - Train Loss: 42.148461, Val Loss: 42.057023\n",
      "Epoch 179/200 - Train Loss: 42.147679, Val Loss: 42.055904\n",
      "Epoch 180/200 - Train Loss: 42.147357, Val Loss: 42.050834\n",
      "Epoch 181/200 - Train Loss: 42.146779, Val Loss: 42.048704\n",
      "Epoch 182/200 - Train Loss: 42.146266, Val Loss: 42.053133\n",
      "Epoch 183/200 - Train Loss: 42.145927, Val Loss: 42.044846\n",
      "Epoch 184/200 - Train Loss: 42.145808, Val Loss: 42.047649\n",
      "Epoch 185/200 - Train Loss: 42.145493, Val Loss: 42.044812\n",
      "Epoch 186/200 - Train Loss: 42.144774, Val Loss: 42.039537\n",
      "Epoch 187/200 - Train Loss: 42.144439, Val Loss: 42.037626\n",
      "Epoch 188/200 - Train Loss: 42.145096, Val Loss: 42.036651\n",
      "Epoch 189/200 - Train Loss: 42.143857, Val Loss: 42.031126\n",
      "Epoch 190/200 - Train Loss: 42.143660, Val Loss: 42.028028\n",
      "Epoch 191/200 - Train Loss: 42.143800, Val Loss: 42.035929\n",
      "Epoch 192/200 - Train Loss: 42.143035, Val Loss: 42.027523\n",
      "Epoch 193/200 - Train Loss: 42.143387, Val Loss: 42.036771\n",
      "Epoch 194/200 - Train Loss: 42.143245, Val Loss: 42.025863\n",
      "Epoch 195/200 - Train Loss: 42.143047, Val Loss: 42.019528\n",
      "Epoch 196/200 - Train Loss: 42.142722, Val Loss: 42.026877\n",
      "Epoch 197/200 - Train Loss: 42.142905, Val Loss: 42.024070\n",
      "Epoch 198/200 - Train Loss: 42.142797, Val Loss: 42.026789\n",
      "Epoch 199/200 - Train Loss: 42.142393, Val Loss: 42.020108\n",
      "Epoch 200/200 - Train Loss: 42.142707, Val Loss: 42.015464\n",
      "  Avg Val Score for config {'learning_rate': 0.0001, 'weight_decay': 0.0001}: 42.293211\n",
      "\n",
      "Best Softmax config found: {'learning_rate': 0.0001, 'weight_decay': 0} with score 42.293211\n",
      "--- Finished Cross-Validation for Softmax Regression ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>mean_cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>42.293211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>42.293211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>42.293211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>42.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>42.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>42.299922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>42.520074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>42.520079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>42.520112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  weight_decay  mean_cv_score\n",
       "6         0.0001       0.00000      42.293211\n",
       "7         0.0001       0.00001      42.293211\n",
       "8         0.0001       0.00010      42.293211\n",
       "3         0.0010       0.00000      42.299900\n",
       "4         0.0010       0.00001      42.299900\n",
       "5         0.0010       0.00010      42.299922\n",
       "0         0.0100       0.00000      42.520074\n",
       "1         0.0100       0.00001      42.520079\n",
       "2         0.0100       0.00010      42.520112"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 2. Softmax Regression Model Builder ---\n",
    "\n",
    "def build_softmax_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Builds a Softmax Regression model (Linear layer + Softmax).\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        output_dim (int): Number of output classes (probabilities).\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: An instance of the SoftmaxRegression model.\n",
    "    \"\"\"\n",
    "    class SoftmaxRegression(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "            # Apply Softmax to get probability outputs.\n",
    "            # Note: If your custom 'weighted_cross_entropy_loss' expects raw logits\n",
    "            # (like nn.CrossEntropyLoss), remove the Softmax layer here and\n",
    "            # apply it separately if needed after prediction.\n",
    "            self.softmax = nn.Softmax(dim=1) # Apply softmax across the class dimension\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward pass.\"\"\"\n",
    "            logits = self.linear(x)\n",
    "            probabilities = self.softmax(logits)\n",
    "            return probabilities\n",
    "\n",
    "    model = SoftmaxRegression(input_dim, output_dim)\n",
    "    return model\n",
    "\n",
    "# --- 2. Cross-Validation Function for Softmax Regression (Modified) ---\n",
    "\n",
    "def cross_val_softmax(fold_definitions=FOLD_DEFINITIONS,\n",
    "                       batch_size=BATCH_SIZE, \n",
    "                       device=DEVICE,\n",
    "                       max_epochs=MAX_EPOCHS, \n",
    "                       patience=PATIENCE):\n",
    "    \"\"\"\n",
    "    Performs 3-fold CV hyperparameter tuning for Softmax Regression (PyTorch).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: Results DF sorted by score.\n",
    "            - torch.nn.Module: Untrained Softmax model instance with the best structural params (dims).\n",
    "                                Training hyperparams (LR, WD) are not part of the structure.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Cross-Validation for Softmax Regression ---\")\n",
    "    param_grid = {\n",
    "        'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "        'weight_decay': [0, 1e-5, 1e-4]\n",
    "    }\n",
    "    results_list = []\n",
    "    best_score = float('inf')\n",
    "    best_config = None # Store the best config dict\n",
    "    optimizer_choice = optim.AdamW\n",
    "\n",
    "    # --- Get Input/Output Dimensions Once ---\n",
    "    # Load data from the first fold just to determine dimensions\n",
    "    temp_X, temp_y, _ = load_data(fold_definitions[0]['train_suffix'])\n",
    "    input_dim = temp_X.shape[1]\n",
    "    output_dim = temp_y.shape[1]\n",
    "    del temp_X, temp_y # Free memory\n",
    "    # ---\n",
    "\n",
    "    for config in ParameterGrid(param_grid):\n",
    "        print(f\"  Testing config: {config}\")\n",
    "        fold_validation_scores = []\n",
    "\n",
    "        for i, fold in enumerate(fold_definitions):\n",
    "            X_train_pd, y_train_pd, wts_train_pd = load_data(fold['train_suffix'])\n",
    "            X_val_pd, y_val_pd, wts_val_pd = load_data(fold['val_suffix'])\n",
    "\n",
    "            train_loader, val_loader = create_pytorch_datasets_loaders(X_train_pd, \n",
    "                                                                       y_train_pd, \n",
    "                                                                       wts_train_pd, \n",
    "                                                                       X_val_pd, \n",
    "                                                                       y_val_pd, \n",
    "                                                                       wts_val_pd,\n",
    "                                                                       batch_size)\n",
    "\n",
    "            model = build_softmax_model(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "            optimizer = optimizer_choice(model.parameters(), \n",
    "                                         lr=config['learning_rate'], \n",
    "                                         weight_decay=config['weight_decay'])\n",
    "\n",
    "            validation_score = train_pytorch_model(model, \n",
    "                                                   train_loader, \n",
    "                                                   val_loader, \n",
    "                                                   weighted_cross_entropy_loss, \n",
    "                                                   optimizer,\n",
    "                                                   device, \n",
    "                                                   max_epochs, \n",
    "                                                   patience, \n",
    "                                                   verbose=True)\n",
    "            fold_validation_scores.append(validation_score)\n",
    "\n",
    "        avg_validation_score = np.mean(fold_validation_scores)\n",
    "        print(f\"  Avg Val Score for config {config}: {avg_validation_score:.6f}\")\n",
    "        current_result = {**config, 'mean_cv_score': avg_validation_score}\n",
    "        results_list.append(current_result)\n",
    "\n",
    "        # Track best configuration (dictionary)\n",
    "        if avg_validation_score < best_score:\n",
    "            best_score = avg_validation_score\n",
    "            best_config = config # Keep the config dict\n",
    "\n",
    "    # Compile results\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    results_df = results_df.sort_values(by='mean_cv_score', ascending=True)\n",
    "\n",
    "    # Build the best model instance (untrained)\n",
    "    # Note: Softmax structure only depends on dims, not LR or WD.\n",
    "    print(f\"\\nBest Softmax config found: {best_config} with score {best_score:.6f}\")\n",
    "    best_model_instance = build_softmax_model(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "    print(\"--- Finished Cross-Validation for Softmax Regression ---\")\n",
    "    return results_df, best_model_instance\n",
    "\n",
    "# get results and model for softmax cross-validation\n",
    "results['softmax'], models['softmax'] = cross_val_softmax()\n",
    "\n",
    "# display softmax results\n",
    "results['softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "beb7d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. 1-Hidden-Layer MLP Model Builder ---\n",
    "\n",
    "def build_nn1_model(input_dim, \n",
    "                    output_dim, \n",
    "                    n_hidden, \n",
    "                    dropout_rate):\n",
    "    \"\"\"\n",
    "    Builds a 1-Hidden-Layer MLP with ReLU activation and Dropout.\n",
    "\n",
    "    Architecture: Linear -> ReLU -> Dropout -> Linear -> Softmax\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        output_dim (int): Number of output classes (probabilities).\n",
    "        n_hidden (int): Number of neurons in the hidden layer.\n",
    "        dropout_rate (float): Dropout probability.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: An instance of the NN1Layer model.\n",
    "    \"\"\"\n",
    "    class NN1Layer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, n_hidden, dropout_rate):\n",
    "            super().__init__()\n",
    "            self.layer_1 = nn.Linear(input_dim, n_hidden)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.layer_2 = nn.Linear(n_hidden, output_dim)\n",
    "            # Apply Softmax to get probability outputs (see note in build_softmax_model)\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward pass.\"\"\"\n",
    "            x = self.layer_1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x) # Apply dropout after activation\n",
    "            logits = self.layer_2(x)\n",
    "            probabilities = self.softmax(logits)\n",
    "            return probabilities\n",
    "\n",
    "    model = NN1Layer(input_dim, output_dim, n_hidden, dropout_rate)\n",
    "    return model\n",
    "\n",
    "# --- 3. Cross-Validation Function for 1-Hidden-Layer MLP (Modified) ---\n",
    "\n",
    "def cross_val_mlp1(fold_definitions=FOLD_DEFINITIONS,\n",
    "                   batch_size=BATCH_SIZE, \n",
    "                   device=DEVICE,\n",
    "                   max_epochs=MAX_EPOCHS, \n",
    "                   patience=PATIENCE):\n",
    "    \"\"\"\n",
    "    Performs 3-fold CV hyperparameter tuning for a 1-Hidden-Layer MLP (PyTorch).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: Results DF sorted by score.\n",
    "            - torch.nn.Module: Untrained MLP-1 model instance with the best hyperparameters.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Cross-Validation for 1-Layer MLP ---\")\n",
    "    param_grid = {\n",
    "        'n_hidden': [16, 32, 64, 128],\n",
    "        'dropout_rate': [0.1, 0.3, 0.5],\n",
    "        'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "    }\n",
    "    results_list = []\n",
    "    best_score = float('inf')\n",
    "    best_config = None\n",
    "    optimizer_choice = optim.AdamW\n",
    "\n",
    "    # --- Get Input/Output Dimensions Once ---\n",
    "    temp_X, temp_y, _ = load_data(fold_definitions[0]['train_suffix'])\n",
    "    input_dim = temp_X.shape[1]\n",
    "    output_dim = temp_y.shape[1]\n",
    "    del temp_X, temp_y\n",
    "    # ---\n",
    "\n",
    "    for config in ParameterGrid(param_grid):\n",
    "        print(f\"  Testing config: {config}\")\n",
    "        fold_validation_scores = []\n",
    "\n",
    "        for i, fold in enumerate(fold_definitions):\n",
    "            X_train_pd, y_train_pd, wts_train_pd = load_data(fold['train_suffix'])\n",
    "            X_val_pd, y_val_pd, wts_val_pd = load_data(fold['val_suffix'])\n",
    "            train_loader, val_loader = create_pytorch_datasets_loaders(X_train_pd, \n",
    "                                                                       y_train_pd, \n",
    "                                                                       wts_train_pd, \n",
    "                                                                       X_val_pd, \n",
    "                                                                       y_val_pd, \n",
    "                                                                       wts_val_pd,\n",
    "                                                                       batch_size)\n",
    "\n",
    "            model = build_nn1_model(input_dim=input_dim, \n",
    "                                    output_dim=output_dim,\n",
    "                                    n_hidden=config['n_hidden'], \n",
    "                                    dropout_rate=config['dropout_rate']\n",
    "            ).to(device)\n",
    "            optimizer = optimizer_choice(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "            validation_score = train_pytorch_model(model, \n",
    "                                                   train_loader, \n",
    "                                                   val_loader, \n",
    "                                                   weighted_cross_entropy_loss, \n",
    "                                                   optimizer,\n",
    "                                                   device, \n",
    "                                                   max_epochs, \n",
    "                                                   patience, \n",
    "                                                   verbose=False\n",
    "            )\n",
    "            fold_validation_scores.append(validation_score)\n",
    "\n",
    "        avg_validation_score = np.mean(fold_validation_scores)\n",
    "        print(f\"  Avg Val Score for config {config}: {avg_validation_score:.6f}\")\n",
    "        current_result = {**config, 'mean_cv_score': avg_validation_score}\n",
    "        results_list.append(current_result)\n",
    "\n",
    "        if avg_validation_score < best_score:\n",
    "            best_score = avg_validation_score\n",
    "            best_config = config\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    results_df = results_df.sort_values(by='mean_cv_score', ascending=True)\n",
    "\n",
    "    # Build the best model instance (untrained)\n",
    "    print(f\"\\nBest 1-Layer MLP config found: {best_config} with score {best_score:.6f}\")\n",
    "    best_model_instance = build_nn1_model(\n",
    "        input_dim=input_dim, output_dim=output_dim,\n",
    "        n_hidden=best_config['n_hidden'], dropout_rate=best_config['dropout_rate']\n",
    "    )\n",
    "\n",
    "    print(\"--- Finished Cross-Validation for 1-Layer MLP ---\")\n",
    "    return results_df, best_model_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "abfa206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Cross-Validation for 1-Layer MLP ---\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.01, 'n_hidden': 16}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.01, 'n_hidden': 16}: 42.318940\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.01, 'n_hidden': 32}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.01, 'n_hidden': 32}: 42.303454\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.01, 'n_hidden': 64}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.01, 'n_hidden': 64}: 42.340736\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.01, 'n_hidden': 128}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.01, 'n_hidden': 128}: 42.351274\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.001, 'n_hidden': 16}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.001, 'n_hidden': 16}: 42.329686\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.001, 'n_hidden': 32}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.001, 'n_hidden': 32}: 42.300045\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.001, 'n_hidden': 64}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.001, 'n_hidden': 64}: 42.275062\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.001, 'n_hidden': 128}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.001, 'n_hidden': 128}: 42.277826\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'n_hidden': 16}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'n_hidden': 16}: 42.370447\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'n_hidden': 32}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'n_hidden': 32}: 42.342159\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'n_hidden': 64}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'n_hidden': 64}: 42.311978\n",
      "  Testing config: {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'n_hidden': 128}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.1, 'learning_rate': 0.0001, 'n_hidden': 128}: 42.312494\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.01, 'n_hidden': 16}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.01, 'n_hidden': 16}: 42.352476\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.01, 'n_hidden': 32}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.01, 'n_hidden': 32}: 42.301870\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.01, 'n_hidden': 64}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.01, 'n_hidden': 64}: 42.397140\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.01, 'n_hidden': 128}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.01, 'n_hidden': 128}: 42.411923\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.001, 'n_hidden': 16}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.001, 'n_hidden': 16}: 42.367052\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.001, 'n_hidden': 32}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.001, 'n_hidden': 32}: 42.321677\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.001, 'n_hidden': 64}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.001, 'n_hidden': 64}: 42.289017\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.001, 'n_hidden': 128}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.001, 'n_hidden': 128}: 42.281945\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'n_hidden': 16}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'n_hidden': 16}: 42.454775\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'n_hidden': 32}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'n_hidden': 32}: 42.373348\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'n_hidden': 64}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'n_hidden': 64}: 42.335819\n",
      "  Testing config: {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'n_hidden': 128}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.3, 'learning_rate': 0.0001, 'n_hidden': 128}: 42.311532\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.01, 'n_hidden': 16}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.01, 'n_hidden': 16}: 42.385064\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.01, 'n_hidden': 32}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.01, 'n_hidden': 32}: 42.340452\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.01, 'n_hidden': 64}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.01, 'n_hidden': 64}: 42.409156\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.01, 'n_hidden': 128}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.01, 'n_hidden': 128}: 42.449603\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'n_hidden': 16}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.001, 'n_hidden': 16}: 42.476625\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'n_hidden': 32}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.001, 'n_hidden': 32}: 42.355376\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'n_hidden': 64}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.001, 'n_hidden': 64}: 42.317538\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.001, 'n_hidden': 128}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.001, 'n_hidden': 128}: 42.293218\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'n_hidden': 16}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'n_hidden': 16}: 42.491051\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'n_hidden': 32}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'n_hidden': 32}: 42.515563\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'n_hidden': 64}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'n_hidden': 64}: 42.447943\n",
      "  Testing config: {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'n_hidden': 128}\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "Created DataLoaders - Train batches: 124, Val batches: 62\n",
      "  Avg Val Score for config {'dropout_rate': 0.5, 'learning_rate': 0.0001, 'n_hidden': 128}: 42.396469\n",
      "\n",
      "Best 1-Layer MLP config found: {'dropout_rate': 0.1, 'learning_rate': 0.001, 'n_hidden': 64} with score 42.275062\n",
      "--- Finished Cross-Validation for 1-Layer MLP ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_hidden</th>\n",
       "      <th>mean_cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>42.275062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>42.277826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>42.281945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>42.289017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>42.293218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>42.300045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>42.301870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>42.303454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>42.311532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>42.311978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>42.312494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>42.317538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>16</td>\n",
       "      <td>42.318940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>42.321677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>42.329686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>42.335819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>42.340452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>42.340736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>42.342159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>42.351274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>16</td>\n",
       "      <td>42.352476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>42.355376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>42.367052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>42.370447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>42.373348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>16</td>\n",
       "      <td>42.385064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>128</td>\n",
       "      <td>42.396469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>42.397140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>42.409156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>42.411923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>42.447943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>128</td>\n",
       "      <td>42.449603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>42.454775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>42.476625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>42.491051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>42.515563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dropout_rate  learning_rate  n_hidden  mean_cv_score\n",
       "6            0.1         0.0010        64      42.275062\n",
       "7            0.1         0.0010       128      42.277826\n",
       "19           0.3         0.0010       128      42.281945\n",
       "18           0.3         0.0010        64      42.289017\n",
       "31           0.5         0.0010       128      42.293218\n",
       "5            0.1         0.0010        32      42.300045\n",
       "13           0.3         0.0100        32      42.301870\n",
       "1            0.1         0.0100        32      42.303454\n",
       "23           0.3         0.0001       128      42.311532\n",
       "10           0.1         0.0001        64      42.311978\n",
       "11           0.1         0.0001       128      42.312494\n",
       "30           0.5         0.0010        64      42.317538\n",
       "0            0.1         0.0100        16      42.318940\n",
       "17           0.3         0.0010        32      42.321677\n",
       "4            0.1         0.0010        16      42.329686\n",
       "22           0.3         0.0001        64      42.335819\n",
       "25           0.5         0.0100        32      42.340452\n",
       "2            0.1         0.0100        64      42.340736\n",
       "9            0.1         0.0001        32      42.342159\n",
       "3            0.1         0.0100       128      42.351274\n",
       "12           0.3         0.0100        16      42.352476\n",
       "29           0.5         0.0010        32      42.355376\n",
       "16           0.3         0.0010        16      42.367052\n",
       "8            0.1         0.0001        16      42.370447\n",
       "21           0.3         0.0001        32      42.373348\n",
       "24           0.5         0.0100        16      42.385064\n",
       "35           0.5         0.0001       128      42.396469\n",
       "14           0.3         0.0100        64      42.397140\n",
       "26           0.5         0.0100        64      42.409156\n",
       "15           0.3         0.0100       128      42.411923\n",
       "34           0.5         0.0001        64      42.447943\n",
       "27           0.5         0.0100       128      42.449603\n",
       "20           0.3         0.0001        16      42.454775\n",
       "28           0.5         0.0010        16      42.476625\n",
       "32           0.5         0.0001        16      42.491051\n",
       "33           0.5         0.0001        32      42.515563"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mlp1'], models['mlp1'] = cross_val_mlp1()\n",
    "results['mlp1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c80d5daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN1Layer(\n",
       "  (layer_1): Linear(in_features=115, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layer_2): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['mlp1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b5761dca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'build_nn1_model.<locals>.NN1Layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp1\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muntrained_mlp1.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/math392/lib/python3.11/site-packages/torch/serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m    853\u001b[0m             pickle_module,\n\u001b[1;32m    854\u001b[0m             pickle_protocol,\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/math392/lib/python3.11/site-packages/torch/serialization.py:1088\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1086\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m   1087\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m-> 1088\u001b[0m pickler\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[1;32m   1089\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m   1090\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'build_nn1_model.<locals>.NN1Layer'"
     ]
    }
   ],
   "source": [
    "torch.save(models['mlp1'],'untrained_mlp1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "716853de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer_1.weight',\n",
       "              tensor([[-0.0038, -0.0037,  0.0708,  ..., -0.0315,  0.0550,  0.0361],\n",
       "                      [ 0.0793, -0.0667,  0.0253,  ..., -0.0650,  0.0144,  0.0389],\n",
       "                      [ 0.0157,  0.0540, -0.0340,  ...,  0.0567, -0.0621, -0.0442],\n",
       "                      ...,\n",
       "                      [ 0.0810, -0.0652, -0.0578,  ...,  0.0658, -0.0269,  0.0615],\n",
       "                      [ 0.0535, -0.0166, -0.0897,  ...,  0.0227,  0.0661,  0.0740],\n",
       "                      [-0.0154, -0.0583,  0.0861,  ...,  0.0496,  0.0276,  0.0243]])),\n",
       "             ('layer_1.bias',\n",
       "              tensor([-0.0345, -0.0910,  0.0128, -0.0161, -0.0071,  0.0089, -0.0545, -0.0581,\n",
       "                      -0.0271,  0.0362, -0.0584,  0.0869, -0.0918, -0.0579,  0.0267, -0.0907,\n",
       "                      -0.0518,  0.0481, -0.0632,  0.0299, -0.0497, -0.0441,  0.0162,  0.0090,\n",
       "                       0.0846,  0.0194,  0.0162, -0.0484,  0.0810, -0.0012,  0.0558, -0.0746,\n",
       "                       0.0488,  0.0758,  0.0604,  0.0779, -0.0424, -0.0669,  0.0336,  0.0404,\n",
       "                      -0.0567, -0.0299, -0.0012, -0.0078,  0.0446,  0.0662,  0.0270, -0.0369,\n",
       "                      -0.0582,  0.0805,  0.0104, -0.0488, -0.0696, -0.0634,  0.0871, -0.0587,\n",
       "                      -0.0908, -0.0699, -0.0443, -0.0028, -0.0639,  0.0411, -0.0153, -0.0388])),\n",
       "             ('layer_2.weight',\n",
       "              tensor([[-0.0758, -0.0396,  0.1214,  0.0063,  0.0352, -0.1211,  0.0763,  0.0197,\n",
       "                       -0.0138,  0.0023, -0.0578, -0.0970, -0.0626,  0.0392,  0.1056, -0.0870,\n",
       "                        0.0575, -0.0608,  0.0942,  0.0153, -0.1136, -0.1078,  0.0301,  0.0698,\n",
       "                        0.0369,  0.0344, -0.0196, -0.0624,  0.0216, -0.0991,  0.0624, -0.0476,\n",
       "                       -0.1121,  0.0136,  0.0481, -0.0954, -0.0789, -0.0224, -0.0517,  0.1013,\n",
       "                        0.0190, -0.0390, -0.0974, -0.0924,  0.0428, -0.0299,  0.0176,  0.0226,\n",
       "                        0.0871,  0.0009,  0.0558, -0.1172,  0.0881, -0.1068, -0.0358, -0.1246,\n",
       "                        0.0638, -0.0330, -0.1228, -0.0691,  0.0685,  0.0842,  0.0876, -0.1191],\n",
       "                      [-0.0514,  0.0202, -0.0078, -0.0635,  0.1217, -0.0777,  0.0816,  0.0804,\n",
       "                       -0.1073, -0.0828, -0.0840,  0.0913, -0.1096,  0.0015, -0.0382, -0.0290,\n",
       "                       -0.0739, -0.1078, -0.1074, -0.0133,  0.0071,  0.0324,  0.0410, -0.0529,\n",
       "                        0.0684,  0.0736, -0.0988,  0.1068, -0.0135, -0.0068, -0.1189,  0.0664,\n",
       "                        0.0505,  0.0317,  0.0550, -0.1099,  0.1242,  0.0836, -0.0368,  0.0658,\n",
       "                       -0.0446, -0.0397, -0.0076,  0.0772,  0.0687, -0.0431, -0.1034,  0.0015,\n",
       "                       -0.0531, -0.0081,  0.0635,  0.0834,  0.0066,  0.0512,  0.0229, -0.0569,\n",
       "                        0.0743, -0.0877, -0.0094, -0.0543,  0.0085,  0.1037,  0.0396, -0.0082],\n",
       "                      [-0.1165, -0.0891, -0.0259, -0.0601,  0.0246, -0.1233,  0.1164, -0.0380,\n",
       "                        0.0745,  0.0897,  0.1022, -0.0207,  0.0043,  0.0638, -0.0931, -0.0089,\n",
       "                        0.0526, -0.0028, -0.0890,  0.0186, -0.0257,  0.0767, -0.0871,  0.0104,\n",
       "                       -0.0170, -0.0023,  0.1131,  0.0010, -0.0049, -0.1185, -0.0604, -0.1008,\n",
       "                        0.0086, -0.0272,  0.0367,  0.1015,  0.0270, -0.0888, -0.0760, -0.0566,\n",
       "                       -0.0698,  0.0208, -0.1198, -0.0958, -0.0218, -0.0856,  0.0788,  0.0411,\n",
       "                        0.0970,  0.0575, -0.0333, -0.0186,  0.0586, -0.1136,  0.0302, -0.1142,\n",
       "                        0.0535, -0.0406,  0.0413,  0.0482,  0.0049,  0.0156, -0.0458,  0.0041],\n",
       "                      [ 0.0721,  0.1028, -0.0352,  0.0289,  0.0300,  0.0501,  0.0432,  0.0415,\n",
       "                       -0.0906, -0.0213, -0.0623,  0.1036, -0.0477,  0.0546,  0.0332,  0.0442,\n",
       "                        0.0300, -0.0113, -0.0406,  0.1225,  0.0299,  0.0936,  0.0410,  0.1212,\n",
       "                        0.0343, -0.0416,  0.0209, -0.0920,  0.0743, -0.0888,  0.0684,  0.0438,\n",
       "                       -0.1242, -0.0430, -0.0263, -0.0447,  0.0335, -0.0651,  0.1069, -0.0207,\n",
       "                        0.0617, -0.0570, -0.0731,  0.0987, -0.1142, -0.0051, -0.0645, -0.0507,\n",
       "                       -0.0094,  0.0367, -0.0133,  0.0179,  0.1128,  0.0222,  0.0802, -0.0330,\n",
       "                       -0.0291, -0.1015, -0.0651, -0.0457,  0.0276, -0.0277, -0.1101, -0.0427]])),\n",
       "             ('layer_2.bias', tensor([ 0.1103, -0.0539, -0.0901,  0.0573]))])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['mlp1'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "57df3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib # For saving scikit-learn models\n",
    "import os\n",
    "\n",
    "# --- Assume necessary helper functions and configs ---\n",
    "# build_ridge_model, build_softmax_model, build_nn1_model, build_nn2_model\n",
    "# DEVICE (needed for loading PyTorch models)\n",
    "# --- End Assume ---\n",
    "\n",
    "\n",
    "# --- 1. Save Untrained Model Object Function ---\n",
    "\n",
    "def save_untrained_model(model, filename):\n",
    "    \"\"\"\n",
    "    Saves the entire UNTRAINED model object (including architecture).\n",
    "    Uses torch.save for PyTorch models and joblib.dump for scikit-learn models.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module or sklearn estimator): The untrained model instance\n",
    "                 (e.g., returned by the cross_val_* functions).\n",
    "        filename (str): The name for the saved file (e.g., 'untrained_best_mlp1.pkl').\n",
    "                        Using '.pkl' extension is common for pickled objects.\n",
    "    \"\"\"\n",
    "    save_path = os.path.join(\".\", filename)\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        print(f\"Saving entire PyTorch model object to: {save_path}\")\n",
    "        # Note: This saves the whole object via pickle. Less portable than state_dict.\n",
    "        torch.save(model, save_path)\n",
    "        print(\"PyTorch model object saved successfully.\")\n",
    "    elif hasattr(model, 'predict'): # Basic check for sklearn-like model\n",
    "        print(f\"Saving scikit-learn model object using joblib to: {save_path}\")\n",
    "        joblib.dump(model, save_path)\n",
    "        print(\"Scikit-learn model object saved successfully.\")\n",
    "    else:\n",
    "        print(f\"Warning: Model type not recognized for saving: {type(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8ba2c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving entire PyTorch model object to: ./untrained_best_mlp1.pkl\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'build_nn1_model.<locals>.NN1Layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_untrained_model(models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muntrained_best_mlp1.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[164], line 28\u001b[0m, in \u001b[0;36msave_untrained_model\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving entire PyTorch model object to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Note: This saves the whole object via pickle. Less portable than state_dict.\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model, save_path)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch model object saved successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;66;03m# Basic check for sklearn-like model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/math392/lib/python3.11/site-packages/torch/serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[1;32m    853\u001b[0m             pickle_module,\n\u001b[1;32m    854\u001b[0m             pickle_protocol,\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/math392/lib/python3.11/site-packages/torch/serialization.py:1088\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1086\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m   1087\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m-> 1088\u001b[0m pickler\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[1;32m   1089\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m   1090\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'build_nn1_model.<locals>.NN1Layer'"
     ]
    }
   ],
   "source": [
    "save_untrained_model(models['mlp1'], 'untrained_best_mlp1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4414b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- 2. Load Untrained Model Object Function ---\n",
    "\n",
    "def load_untrained_model(filename, device):\n",
    "    \"\"\"\n",
    "    Loads an entire UNTRAINED model object saved using save_untrained_model.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The file containing the saved model object.\n",
    "        device (torch.device): The device to load PyTorch models onto ('cpu' or 'cuda').\n",
    "                               Ignored for scikit-learn models.\n",
    "\n",
    "    Returns:\n",
    "        The loaded untrained model instance (torch.nn.Module or sklearn estimator).\n",
    "    \"\"\"\n",
    "    load_path = os.path.join(\".\", filename)\n",
    "    if not os.path.exists(load_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at: {load_path}\")\n",
    "\n",
    "    print(f\"Loading model object from: {load_path}\")\n",
    "    # Try loading with torch first, assume PyTorch model\n",
    "    try:\n",
    "        # map_location moves the model's tensors to the specified device during loading\n",
    "        model = torch.load(load_path, map_location=device)\n",
    "        if isinstance(model, torch.nn.Module):\n",
    "             # Ensure model is fully on device (sometimes needed for internal buffers)\n",
    "            model.to(device)\n",
    "            print(f\"PyTorch model loaded successfully onto {device}.\")\n",
    "            return model\n",
    "        else:\n",
    "            # If torch.load worked but it's not an nn.Module, maybe it was sklearn saved with torch? Unlikely.\n",
    "            # Try joblib next.\n",
    "            pass\n",
    "    except Exception as torch_exception:\n",
    "        # If torch.load fails, assume it might be a scikit-learn model saved with joblib\n",
    "        print(f\"torch.load failed ({torch_exception}), attempting joblib.load...\")\n",
    "        try:\n",
    "            model = joblib.load(load_path)\n",
    "            if hasattr(model, 'predict'):\n",
    "                print(\"Scikit-learn model loaded successfully.\")\n",
    "                return model\n",
    "            else:\n",
    "                raise TypeError(\"Loaded object is not a recognized model type.\")\n",
    "        except Exception as joblib_exception:\n",
    "            print(f\"joblib.load also failed ({joblib_exception}).\")\n",
    "            raise IOError(f\"Could not load model from {load_path} using torch or joblib.\") from joblib_exception\n",
    "\n",
    "    # If we somehow loaded with torch but it wasn't nn.Module, and joblib wasn't tried/failed\n",
    "    raise TypeError(\"Could not determine model type during loading.\")\n",
    "\n",
    "\n",
    "# --- Workflow Integration ---\n",
    "\n",
    "# 1. After running CV:\n",
    "#    ridge_cv_results_df, best_ridge_instance = cross_val_ridge()\n",
    "#    mlp1_cv_results_df, best_mlp1_instance = cross_val_mlp1()\n",
    "#    # ... etc.\n",
    "\n",
    "# 2. Determine the overall best model type and instance\n",
    "#    (e.g., best_overall_model_instance = best_mlp1_instance; best_overall_model_type = 'MLP1')\n",
    "\n",
    "# 3. Save the UNTRAINED best model instance\n",
    "#    save_untrained_model(best_overall_model_instance, f\"UNTRAINED_best_{best_overall_model_type}.pkl\")\n",
    "\n",
    "# --- Student Workflow (in a separate script/notebook) ---\n",
    "\n",
    "# 1. Load the provided untrained model file\n",
    "#    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#    loaded_untrained_model = load_untrained_model(\"UNTRAINED_best_MLP1.pkl\", DEVICE)\n",
    "#    best_overall_model_type = 'MLP1' # Student needs to know this\n",
    "\n",
    "# 2. Get the corresponding best *training* hyperparameters (students would need these, maybe from the CV results df)\n",
    "#    # Example: Assuming students have access to the best row of the results df\n",
    "#    best_hyperparams_row = {'learning_rate': 1e-3, 'dropout_rate': 0.3, 'n_hidden': 64, 'mean_cv_score': 0.123} # Load this row\n",
    "#    training_config = {\n",
    "#        'learning_rate': best_hyperparams_row['learning_rate'],\n",
    "#        'weight_decay': best_hyperparams_row.get('weight_decay', 0) # Handle potential absence\n",
    "#    }\n",
    "\n",
    "# 3. Define final training epochs\n",
    "#    FINAL_EPOCHS = 100\n",
    "\n",
    "# 4. Call the final training function (using the function from the previous response)\n",
    "#    trained_model = train_final_model(\n",
    "#        model_instance=loaded_untrained_model, # Pass the loaded untrained model\n",
    "#        model_type=best_overall_model_type,\n",
    "#        best_training_config=training_config,\n",
    "#        data_dir=\".\", # Students need data directory\n",
    "#        device=DEVICE,\n",
    "#        batch_size=BATCH_SIZE, # Students need batch size\n",
    "#        final_train_epochs=FINAL_EPOCHS\n",
    "#    )\n",
    "\n",
    "# 5. Students can now use the 'trained_model' for testing or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state dictionary to: ./best_softmax.pth\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# save the softmax model to file\n",
    "save_untrained_model(models['softmax'], 'untrained_softmax.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f56eeeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state dictionary to: ./best_mlp1.pth\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# save the mlp1 model to file\n",
    "save_model(models['mlp1'], 'best_mlp1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the softmax model from file\n",
    "softmax_loaded = load_model(build_softmax_model, \n",
    "                             input_dim, \n",
    "                             output_dim, \n",
    "                             {}, # No structural params needed for softmax\n",
    "                             'best_softmax.pth', \n",
    "                             device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- Assume necessary helper functions and configs are defined ---\n",
    "# load_data, create_pytorch_datasets_loaders, weighted_cross_entropy_loss\n",
    "# build_softmax_model, build_nn1_model, build_nn2_model\n",
    "# BATCH_SIZE, DEVICE\n",
    "# --- End Assume ---\n",
    "\n",
    "# --- 1. Save Model Function ---\n",
    "\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Saves the state dictionary of a PyTorch model to a file.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to save.\n",
    "        filename (str): The name of the file to save the model to (e.g., 'best_mlp1.pth').\n",
    "                        It will be saved in the current working directory.\n",
    "    \"\"\"\n",
    "    save_path = os.path.join(\".\", filename) # Save in the current directory\n",
    "    print(f\"Saving model state dictionary to: {save_path}\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(\"Model saved successfully.\")\n",
    "\n",
    "\n",
    "# --- 2. Load Model Function ---\n",
    "\n",
    "def load_model(model_builder, input_dim, output_dim, structural_config, filename, device):\n",
    "    \"\"\"\n",
    "    Loads a model's state dictionary into a newly built model instance.\n",
    "\n",
    "    Args:\n",
    "        model_builder (callable): The function used to build the model\n",
    "                                  (e.g., build_nn1_model).\n",
    "        input_dim (int): The number of input features for the model.\n",
    "        output_dim (int): The number of output classes for the model.\n",
    "        structural_config (dict): Dictionary containing the structural hyperparameters\n",
    "                                 needed by the model_builder (e.g., {'n_hidden': 64, 'dropout_rate': 0.3}).\n",
    "                                 Should NOT include training params like learning_rate.\n",
    "        filename (str): The name of the file containing the saved state dictionary.\n",
    "        device (torch.device): The device to load the model onto ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The model instance with loaded weights.\n",
    "    \"\"\"\n",
    "    load_path = os.path.join(\".\", filename)\n",
    "    if not os.path.exists(load_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at: {load_path}\")\n",
    "\n",
    "    print(f\"Building model structure using: {structural_config}\")\n",
    "    # Build the model structure first, passing necessary dimensions and structural params\n",
    "    model = model_builder(input_dim=input_dim, output_dim=output_dim, **structural_config)\n",
    "\n",
    "    print(f\"Loading model state dictionary from: {load_path}\")\n",
    "    # Load the saved state dictionary. map_location ensures it loads to the specified device.\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Move the model to the specified device (redundant if map_location worked, but good practice)\n",
    "    model.to(device)\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- 3. Train Final Model Function ---\n",
    "\n",
    "def train_final_model(model, training_config, data_dir, device,\n",
    "                      batch_size, final_train_epochs,\n",
    "                      train_data_prefix=\"2008_2012_2016\"):\n",
    "    \"\"\"\n",
    "    Trains a given model instance on the combined training+validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model instance to train (should be already built,\n",
    "                                 possibly loaded).\n",
    "        training_config (dict): Dictionary containing the optimal *training* hyperparameters\n",
    "                                (e.g., {'learning_rate': 1e-3, 'weight_decay': 1e-5}).\n",
    "                                Should NOT include structural params like n_hidden.\n",
    "        data_dir (str): Directory containing the data CSV files.\n",
    "        device (torch.device): The device to train on.\n",
    "        batch_size (int): Batch size for training.\n",
    "        final_train_epochs (int): The fixed number of epochs to train the final model for.\n",
    "                                  (Could be based on median epochs from CV, or a fixed value).\n",
    "        train_data_prefix (str): The prefix for the combined training data files.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The trained model instance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Final Model Training ---\")\n",
    "    print(f\"Training configuration: {training_config}\")\n",
    "    print(f\"Training for {final_train_epochs} epochs.\")\n",
    "\n",
    "    # 1. Load Combined Training Data\n",
    "    X_train_pd, y_train_pd, wts_train_pd = load_data(train_data_prefix, data_dir)\n",
    "\n",
    "    # 2. Create DataLoader for the combined data\n",
    "    # We only need a training loader here. Using create_pytorch_datasets_loaders structure\n",
    "    # but only using the training part.\n",
    "    X_train_tensor = torch.tensor(X_train_pd.values, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train_pd.values, dtype=torch.float32).to(device)\n",
    "    wts_train_tensor = torch.tensor(wts_train_pd[['P(C)']].values, dtype=torch.float32).to(device)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor, wts_train_tensor)\n",
    "    final_train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    print(f\"Loaded final training data: {len(train_dataset)} samples, {len(final_train_loader)} batches.\")\n",
    "\n",
    "    # 3. Setup Optimizer\n",
    "    # Assuming AdamW for NNs based on previous choices. Ridge is handled separately.\n",
    "    # Softmax might use AdamW too if weight decay was chosen.\n",
    "    optimizer_choice = optim.AdamW\n",
    "    optimizer = optimizer_choice(\n",
    "        model.parameters(),\n",
    "        lr=training_config['learning_rate'],\n",
    "        weight_decay=training_config.get('weight_decay', 0) # Use weight_decay if present\n",
    "    )\n",
    "\n",
    "    # 4. Training Loop (Fixed Epochs, No Validation/Early Stopping)\n",
    "    model.to(device) # Ensure model is on the right device\n",
    "    model.train()    # Set model to training mode\n",
    "\n",
    "    for epoch in range(final_train_epochs):\n",
    "        train_loss_epoch = 0.0\n",
    "        for batch_idx, (features, targets, weights) in enumerate(final_train_loader):\n",
    "            # Data is already on the correct device\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            # Calculate loss (using the same weighted loss as in CV)\n",
    "            loss = weighted_cross_entropy_loss(outputs, targets, weights)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_epoch += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss_epoch / len(final_train_loader)\n",
    "        # Print progress periodically\n",
    "        if (epoch + 1) % 10 == 0 or epoch == final_train_epochs - 1:\n",
    "             print(f\"Epoch {epoch+1}/{final_train_epochs} - Training Loss: {avg_train_loss:.6f}\")\n",
    "\n",
    "    print(\"--- Finished Final Model Training ---\")\n",
    "    return model # Return the trained model\n",
    "\n",
    "# --- Example Workflow Integration (Conceptual) ---\n",
    "\n",
    "# # Assuming you have run CV and obtained:\n",
    "# # best_mlp1_config = {'n_hidden': 64, 'dropout_rate': 0.3, 'learning_rate': 0.001}\n",
    "# # best_mlp1_model_structure = build_nn1_model(input_dim=115, output_dim=4, n_hidden=64, dropout_rate=0.3)\n",
    "\n",
    "# # 1. Save the best untrained structure (optional, but good practice)\n",
    "# save_model(best_mlp1_model_structure, \"untrained_best_mlp1.pth\")\n",
    "\n",
    "# # 2. Prepare configs for loading and training\n",
    "# structural_params = {k: v for k, v in best_mlp1_config.items() if k in ['n_hidden', 'dropout_rate']} # Or shared_hidden_size\n",
    "# training_params = {k: v for k, v in best_mlp1_config.items() if k in ['learning_rate', 'weight_decay']}\n",
    "\n",
    "# # Determine input/output dims (e.g., from final training data)\n",
    "# final_input_dim = 115 # Replace with actual value\n",
    "# final_output_dim = 4  # Replace with actual value\n",
    "\n",
    "# # 3. Load the model structure\n",
    "# loaded_model = load_model(\n",
    "#     model_builder=build_nn1_model, # Pass the correct builder function\n",
    "#     input_dim=final_input_dim,\n",
    "#     output_dim=final_output_dim,\n",
    "#     structural_config=structural_params,\n",
    "#     filename=\"untrained_best_mlp1.pth\", # Name used in save_model\n",
    "#     device=DEVICE\n",
    "# )\n",
    "\n",
    "# # 4. Train the loaded model on the combined data\n",
    "# # Choose number of epochs (e.g., a fixed value like 100, or based on CV results)\n",
    "# FINAL_EPOCHS = 100\n",
    "# trained_final_model = train_final_model(\n",
    "#     model=loaded_model,\n",
    "#     training_config=training_params,\n",
    "#     data_dir=\".\", # Your data directory\n",
    "#     device=DEVICE,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     final_train_epochs=FINAL_EPOCHS\n",
    "# )\n",
    "\n",
    "# # 5. Save the *trained* final model before testing\n",
    "# save_model(trained_final_model, \"TRAINED_final_best_mlp1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03815fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. 2-Hidden-Layer MLP Model Builder ---\n",
    "\n",
    "def build_nn2_model(input_dim, output_dim, shared_hidden_size, dropout_rate):\n",
    "    \"\"\"\n",
    "    Builds a 2-Hidden-Layer MLP with ReLU activation and Dropout.\n",
    "    Uses the same size for both hidden layers.\n",
    "\n",
    "    Architecture: Linear -> ReLU -> Dropout -> Linear -> ReLU -> Dropout -> Linear -> Softmax\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        output_dim (int): Number of output classes (probabilities).\n",
    "        shared_hidden_size (int): Number of neurons in *each* hidden layer.\n",
    "        dropout_rate (float): Dropout probability (applied after each ReLU).\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: An instance of the NN2Layer model.\n",
    "    \"\"\"\n",
    "    class NN2Layer(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim, shared_hidden_size, dropout_rate):\n",
    "            super().__init__()\n",
    "            self.layer_1 = nn.Linear(input_dim, shared_hidden_size)\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.dropout1 = nn.Dropout(dropout_rate)\n",
    "            self.layer_2 = nn.Linear(shared_hidden_size, shared_hidden_size)\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.dropout2 = nn.Dropout(dropout_rate)\n",
    "            self.layer_3 = nn.Linear(shared_hidden_size, output_dim)\n",
    "            # Apply Softmax to get probability outputs (see note in build_softmax_model)\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Forward pass.\"\"\"\n",
    "            x = self.layer_1(x)\n",
    "            x = self.relu1(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.layer_2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.dropout2(x)\n",
    "            logits = self.layer_3(x)\n",
    "            probabilities = self.softmax(logits)\n",
    "            return probabilities\n",
    "\n",
    "    model = NN2Layer(input_dim, output_dim, shared_hidden_size, dropout_rate)\n",
    "    return model\n",
    "\n",
    "# --- Example Instantiation (for checking) ---\n",
    "# input_features = 115\n",
    "# output_classes = 4\n",
    "#\n",
    "# softmax_model = build_softmax_model(input_features, output_classes)\n",
    "# print(\"Softmax Model:\\n\", softmax_model)\n",
    "#\n",
    "# nn1_model = build_nn1_model(input_features, output_classes, n_hidden=64, dropout_rate=0.3)\n",
    "# print(\"\\nNN 1-Layer Model:\\n\", nn1_model)\n",
    "#\n",
    "# nn2_model = build_nn2_model(input_features, output_classes, shared_hidden_size=32, dropout_rate=0.5)\n",
    "# print(\"\\nNN 2-Layer Model:\\n\", nn2_model)\n",
    "\n",
    "# --- 4. Cross-Validation Function for 2-Hidden-Layer MLP (Modified) ---\n",
    "\n",
    "def cross_val_mlp2(fold_definitions=FOLD_DEFINITIONS,\n",
    "                   batch_size=BATCH_SIZE, device=DEVICE,\n",
    "                   max_epochs=MAX_EPOCHS, patience=PATIENCE):\n",
    "    \"\"\"\n",
    "    Performs 3-fold CV hyperparameter tuning for a 2-Hidden-Layer MLP (PyTorch).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: Results DF sorted by score.\n",
    "            - torch.nn.Module: Untrained MLP-2 model instance with the best hyperparameters.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Cross-Validation for 2-Layer MLP ---\")\n",
    "    param_grid = {\n",
    "        'shared_hidden_size': [16, 32, 64],\n",
    "        'dropout_rate': [0.1, 0.3, 0.5],\n",
    "        'learning_rate': [1e-2, 1e-3, 1e-4]\n",
    "    }\n",
    "    results_list = []\n",
    "    best_score = float('inf')\n",
    "    best_config = None\n",
    "    optimizer_choice = optim.AdamW\n",
    "\n",
    "    # --- Get Input/Output Dimensions Once ---\n",
    "    temp_X, temp_y, _ = load_data(fold_definitions[0]['train_suffix'])\n",
    "    input_dim = temp_X.shape[1]\n",
    "    output_dim = temp_y.shape[1]\n",
    "    del temp_X, temp_y\n",
    "    # ---\n",
    "\n",
    "    for config in ParameterGrid(param_grid):\n",
    "        print(f\"  Testing config: {config}\")\n",
    "        fold_validation_scores = []\n",
    "\n",
    "        for i, fold in enumerate(fold_definitions):\n",
    "            X_train_pd, y_train_pd, wts_train_pd = load_data(fold['train_suffix'])\n",
    "            X_val_pd, y_val_pd, wts_val_pd = load_data(fold['val_suffix'])\n",
    "            train_loader, val_loader = create_pytorch_datasets_loaders(\n",
    "                X_train_pd, y_train_pd, wts_train_pd, X_val_pd, y_val_pd, wts_val_pd,\n",
    "                batch_size\n",
    "            )\n",
    "\n",
    "            model = build_nn2_model(\n",
    "                input_dim=input_dim, output_dim=output_dim,\n",
    "                shared_hidden_size=config['shared_hidden_size'], dropout_rate=config['dropout_rate']\n",
    "            ).to(device)\n",
    "            optimizer = optimizer_choice(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "            validation_score = train_pytorch_model(\n",
    "                model, train_loader, val_loader, weighted_cross_entropy_loss, optimizer,\n",
    "                device, max_epochs, patience, verbose=False\n",
    "            )\n",
    "            fold_validation_scores.append(validation_score)\n",
    "\n",
    "        avg_validation_score = np.mean(fold_validation_scores)\n",
    "        print(f\"  Avg Val Score for config {config}: {avg_validation_score:.6f}\")\n",
    "        current_result = {**config, 'mean_cv_score': avg_validation_score}\n",
    "        results_list.append(current_result)\n",
    "\n",
    "        if avg_validation_score < best_score:\n",
    "            best_score = avg_validation_score\n",
    "            best_config = config\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    results_df = results_df.sort_values(by='mean_cv_score', ascending=True)\n",
    "\n",
    "    # Build the best model instance (untrained)\n",
    "    print(f\"\\nBest 2-Layer MLP config found: {best_config} with score {best_score:.6f}\")\n",
    "    best_model_instance = build_nn2_model(\n",
    "        input_dim=input_dim, output_dim=output_dim,\n",
    "        shared_hidden_size=best_config['shared_hidden_size'], dropout_rate=best_config['dropout_rate']\n",
    "    )\n",
    "\n",
    "    print(\"--- Finished Cross-Validation for 2-Layer MLP ---\")\n",
    "    return results_df, best_model_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d3fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a2597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb7cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d696a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Assume these are defined elsewhere ---\n",
    "# FOLD_DEFINITIONS = [...]\n",
    "# BATCH_SIZE = 50\n",
    "# DEVICE = torch.device(...)\n",
    "# MAX_EPOCHS = 300\n",
    "# PATIENCE = 15\n",
    "# def load_data(prefix): ...\n",
    "# def create_pytorch_datasets_loaders(...): ...\n",
    "# def train_pytorch_model(...): ...\n",
    "# def weighted_cross_entropy_loss(...): ...\n",
    "# def build_softmax_model(**config): ...\n",
    "# def build_nn1_model(**config): ...\n",
    "# def build_nn2_model(**config): ...\n",
    "# --- End Assume ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
