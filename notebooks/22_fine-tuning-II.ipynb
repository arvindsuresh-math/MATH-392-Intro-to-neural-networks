{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf68033",
   "metadata": {},
   "source": [
    "## Lecture 2: Data Prep, Model Adaptation & Feature Extraction\n",
    "\n",
    "**Recap from Lecture 1:**\n",
    "* We introduced the final project: Comparing fine-tuned models (MobileNetV3 vs. ResNet18) for Pet Breed Classification.\n",
    "* We introduced the Oxford-IIIT Pet dataset (37 breeds).\n",
    "* We showed basic code to load the dataset using `torchvision` with `Resize` and `ToTensor` transforms.\n",
    "\n",
    "**Goals for Today (Lecture 2):**\n",
    "1.  **Complete Data Preprocessing:** Add ImageNet normalization to our transforms and create `DataLoader`s.\n",
    "2.  **Load & Adapt Models:** Load pre-trained MobileNetV3 and ResNet18 from `torchvision` and modify their final classification layers for our 37 pet breeds.\n",
    "3.  **Implement Feature Extraction:** Freeze the pre-trained layers and train *only* the new classification head for one model as a baseline.\n",
    "4.  **Set up Training:** Define loss function, optimizer, and basic training/validation loops.\n",
    "5.  **Run Demo:** Train the classifier head for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44495258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Complete Transforms (including Normalization) ---\n",
    "\n",
    "# Pre-trained models expect input images normalized in the same way as the ImageNet dataset.\n",
    "# We use the standard ImageNet mean and standard deviation values.\n",
    "# Normalization formula: input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define transforms for training (we'll add augmentation in L3) and validation/testing\n",
    "# Validation/Test transforms typically don't include random augmentations.\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),       # Resize images\n",
    "        transforms.ToTensor(),               # Convert image to PyTorch Tensor\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std) # Normalize with ImageNet stats\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "    ]),\n",
    "}\n",
    "print(\"Data transforms defined (including ImageNet normalization).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the Oxford-IIIT Pet Dataset using the new transforms ---\n",
    "data_root = './data'\n",
    "print(f\"\\nLoading datasets from {data_root}...\")\n",
    "\n",
    "# Use 'trainval' for training, 'test' for validation in this setup\n",
    "# Note: A separate validation split is often preferred, but for simplicity here we use 'test' as validation\n",
    "train_dataset = torchvision.datasets.OxfordIIITPet(\n",
    "    root=data_root,\n",
    "    split='trainval',\n",
    "    download=True, # Should already be downloaded from L1\n",
    "    transform=data_transforms['train'] # Apply training transforms\n",
    ")\n",
    "\n",
    "val_dataset = torchvision.datasets.OxfordIIITPet(\n",
    "    root=data_root,\n",
    "    split='test',     # Using the test split as our validation set here\n",
    "    download=True,\n",
    "    transform=data_transforms['val'] # Apply validation transforms\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ef039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create DataLoaders ---\n",
    "# DataLoaders handle batching, shuffling, and loading data in parallel.\n",
    "batch_size = 32 # Adjust based on your system's memory\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2), # num_workers > 0 for parallel loading\n",
    "    'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "\n",
    "print(f\"\\nDataLoaders created with batch size {batch_size}.\")\n",
    "print(\"Ready for model loading and adaptation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ebcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get one batch of training images and labels\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "print(f\"\\nSample batch input shape: {inputs.shape}\") # e.g., [32, 3, 224, 224]\n",
    "print(f\"Sample batch labels shape: {classes.shape}\") # e.g., [32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f45fa9",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Models & Adapting the Classifier Head\n",
    "\n",
    "Now that our data is ready, we'll load the pre-trained models from `torchvision.models`:\n",
    "\n",
    "1.  First, we run `torchvision.models.<model_name>(weights=...)` to get a model pre-trained on ImageNet. We'll use the recommended modern way of specifying weights.\n",
    "2.  We initially want to train *only* the final classification layer (Feature Extraction). To do this, we freeze the parameters of all other layers by setting `parameter.requires_grad = False`.\n",
    "3.  The pre-trained models have a final layer designed for ImageNet's 1000 classes. We need to replace this with a new layer suited for our 37 pet breeds. We must ensure the `in_features` of the new layer matches the `out_features` of the preceding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edb0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Adapt MobileNetV3 ---\n",
    "\n",
    "print(\"Loading pre-trained MobileNetV3 Large...\")\n",
    "# Load MobileNetV3 Large with the best available ImageNet weights\n",
    "mobilenet_v3 = torchvision.models.mobilenet_v3_large(weights=torchvision.models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Display the model architecture\n",
    "print(mobilenet_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de55504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Freeze all parameters in the base model ---\n",
    "print(\"Freezing base model parameters...\")\n",
    "for param in mobilenet_v3.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- Replace the classifier head ---\n",
    "# MobileNetV3's classifier is typically the last element in the `classifier` Sequential block.\n",
    "# We need to find the number of input features to this layer.\n",
    "num_ftrs_mobilenet = mobilenet_v3.classifier[-1].in_features\n",
    "print(f\"Original MobileNetV3 classifier input features: {num_ftrs_mobilenet}\")\n",
    "\n",
    "# Create a new nn.Linear layer for our 37 classes\n",
    "# The parameters of this new layer have requires_grad=True by default.\n",
    "mobilenet_v3.classifier[-1] = nn.Linear(num_ftrs_mobilenet, num_classes)\n",
    "print(\"Replaced MobileNetV3 classifier head.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verify which parameters are trainable ---\n",
    "print(\"\\nTrainable parameters in modified MobileNetV3:\")\n",
    "total_params_mobilenet = 0\n",
    "trainable_params_mobilenet = 0\n",
    "for name, param in mobilenet_v3.named_parameters():\n",
    "    total_params_mobilenet += param.numel()\n",
    "    if param.requires_grad:\n",
    "        print(f\"  - {name} (Size: {param.shape}, Params: {param.numel()})\")\n",
    "        trainable_params_mobilenet += param.numel()\n",
    "\n",
    "print(f\"\\nTotal parameters in MobileNetV3: {total_params_mobilenet:,}\")\n",
    "print(f\"Trainable parameters (classifier head only): {trainable_params_mobilenet:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Adapt ResNet18 ---\n",
    "\n",
    "print(\"\\nLoading pre-trained ResNet18...\")\n",
    "# Load ResNet18 with the best available ImageNet weights\n",
    "resnet18 = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Display the model architecture\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040792f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Freeze all parameters in the base model ---\n",
    "print(\"Freezing base model parameters...\")\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# --- Replace the classifier head ---\n",
    "# ResNet's classifier is typically the `fc` (fully connected) layer.\n",
    "num_ftrs_resnet = resnet18.fc.in_features\n",
    "print(f\"Original ResNet18 fc layer input features: {num_ftrs_resnet}\")\n",
    "\n",
    "# Create a new nn.Linear layer for our 37 classes\n",
    "# Parameters have requires_grad=True by default.\n",
    "resnet18.fc = nn.Linear(num_ftrs_resnet, num_classes)\n",
    "print(\"Replaced ResNet18 classifier head (fc layer).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verify which parameters are trainable ---\n",
    "print(\"\\nTrainable parameters in modified ResNet18:\")\n",
    "total_params_resnet = 0\n",
    "trainable_params_resnet = 0\n",
    "for name, param in resnet18.named_parameters():\n",
    "    total_params_resnet += param.numel()\n",
    "    if param.requires_grad:\n",
    "        print(f\"  - {name} (Size: {param.shape}, Params: {param.numel()})\")\n",
    "        trainable_params_resnet += param.numel()\n",
    "\n",
    "print(f\"\\nTotal parameters in ResNet18: {total_params_resnet:,}\")\n",
    "print(f\"Trainable parameters (classifier head only): {trainable_params_resnet:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d948f8",
   "metadata": {},
   "source": [
    "### Feature Extraction Training Strategy\n",
    "\n",
    "We now have two models (MobileNetV3 and ResNet18) adapted for our task, with only their final classification layers being trainable. As we discussed last class, this process is called \"feature extraction\", because we rely on the pre-training model to extract some useful information or features (the output that is fed into the final classification layer). Note that:\n",
    "* It's fast, as we only compute gradients and update weights for the small classifier layer.\n",
    "* It quickly establishes a baseline performance by leveraging the powerful, pre-trained features from the frozen base model.\n",
    "* It initializes the weights of our new classifier head reasonably well before we potentially unfreeze more layers later for fine-tuning.\n",
    "\n",
    "**Training Setup:**\n",
    "* *Loss Function:* `nn.CrossEntropyLoss` is standard for multi-class classification. It combines LogSoftmax and Negative Log Likelihood Loss.\n",
    "* *Optimizer:* We'll use `optim.Adam`. Crucially, we must configure it to *only* update the parameters of the newly added classification layer (those with `requires_grad=True`).\n",
    "* *Device:* We'll use a GPU (`cuda` or `mps`) if available, otherwise fallback to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb10216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if MPS is available and PyTorch version is compatible\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Fallback to CUDA then CPU\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf844fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training and Validation Helper Functions ---\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Performs one epoch of training.\"\"\"\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        # Move data to the correct device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1) # Get the index of the max logit/score\n",
    "\n",
    "        # Backward pass + optimize only if in training phase\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Performs one epoch of validation.\"\"\"\n",
    "    model.eval() # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # No gradients needed for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"Training and validation helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7041d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Extraction Training Run ---\n",
    "\n",
    "# Let's use MobileNetV3 for the first run.\n",
    "model_to_train = mobilenet_v3\n",
    "model_name = \"MobileNetV3_FeatureExtract\"\n",
    "\n",
    "# Move the model to the chosen device\n",
    "model_to_train = model_to_train.to(device)\n",
    "print(f\"Training model: {model_name} on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11fd38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Loss and Optimizer ---\n",
    "# Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam\n",
    "# IMPORTANT: We only want to optimize the parameters of the NEW classifier head.\n",
    "# We can filter parameters based on requires_grad == True.\n",
    "params_to_update = []\n",
    "print(\"\\nParameters to be optimized:\")\n",
    "for name, param in model_to_train.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        params_to_update.append(param)\n",
    "        print(f\"  - {name}\")\n",
    "\n",
    "optimizer = optim.Adam(params_to_update, lr=0.001) # Common learning rate for Adam\n",
    "print(\"\\nOptimizer created (Adam, lr=0.001) targeting only the classifier head.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "num_epochs = 5 # Train for a few epochs for demonstration\n",
    "print(f\"\\nStarting Feature Extraction training for {num_epochs} epochs...\")\n",
    "\n",
    "start_time_train = time.time()\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Train for one epoch\n",
    "    train_loss, train_acc = train_one_epoch(model_to_train, dataloaders['train'], criterion, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model_to_train, dataloaders['val'], criterion, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # --- Store metrics for plotting ---\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc.item()) # Use .item() to get scalar value if it's a tensor\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc.item()) # Use .item() to get scalar value if it's a tensor\n",
    "\n",
    "end_time_train = time.time()\n",
    "training_duration = end_time_train - start_time_train\n",
    "print(f\"\\nTraining finished in {training_duration // 60:.0f}m {training_duration % 60:.0f}s\")\n",
    "\n",
    "# --- The variables train_losses, val_losses, train_accs, val_accs now hold the history ---\n",
    "print(\"\\nMetric history collected:\")\n",
    "print(f\"Train Losses per epoch: {[f'{x:.4f}' for x in train_losses]}\")\n",
    "print(f\"Val Losses per epoch:   {[f'{x:.4f}' for x in val_losses]}\")\n",
    "print(f\"Train Accs per epoch:   {[f'{x:.4f}' for x in train_accs]}\")\n",
    "print(f\"Val Accs per epoch:     {[f'{x:.4f}' for x in val_accs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Training & Validation Metrics ---\n",
    "# Assumes train_losses, val_losses, train_accs, val_accs lists exist from the previous cell.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create epoch range for x-axis (e.g., if num_epochs=5, this is [1, 2, 3, 4, 5])\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "# Create a figure with two subplots: one for loss, one for accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.subplot(1, 2, 1) # (rows, columns, panel number)\n",
    "plt.plot(epochs_range, train_losses, 'o-', label='Training Loss') # 'o-' creates markers and lines\n",
    "plt.plot(epochs_range, val_losses, 'o-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend() # Show legend based on labels\n",
    "plt.grid(True) # Add a grid for easier reading\n",
    "\n",
    "# Plot Training & Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_accs, 'o-', label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accs, 'o-', label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout to prevent titles/labels overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311c78d",
   "metadata": {},
   "source": [
    "### Interpreting Feature Extraction Results\n",
    "\n",
    "After just a few epochs of training only the classifier head (Feature Extraction), you should observe:\n",
    "* *Loss Decreasing:* Both training and validation loss should generally decrease.\n",
    "* *Accuracy Increasing:* Both training and validation accuracy should increase significantly from random chance (~2.7% for 37 classes). Reaching accuracies above 50% or even higher is common in this phase, demonstrating the power of the pre-trained features.\n",
    "* *Speed:* This phase should be relatively fast because backpropagation only happens through the final layer.\n",
    "\n",
    "Next lecture, we'll build on this foundation by doing the following:\n",
    "1.  *Full Evaluation:* Implement more detailed evaluation metrics (Precision, Recall, F1, Confusion Matrix).\n",
    "2.  *Data Augmentation:* Add techniques like random flips and crops to the training data to improve generalization.\n",
    "3.  *Fine-Tuning:* Unfreeze some of the later layers of the base model and train them with a low learning rate (Differential Learning Rates)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833f6bf",
   "metadata": {},
   "source": [
    "### Homework: Feature Extraction for ResNet18 (to be completed by Wednesday, Apr 30)\n",
    "\n",
    "**Goal:** Reinforce the complete \"Feature Extraction\" pipeline (data loading with normalization, model adaptation, optimizer setup, training loop) by applying it to the *second* project model (ResNet18). This ensures you have a working baseline for both models before moving to fine-tuning in Lecture 3.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1.  **Adapt Lecture 2 Code:** You should be working within this Jupyter Notebook from Lecture 2, which implemented and ran Feature Extraction for MobileNetV3 on the Pet dataset.\n",
    "2.  **Target ResNet18:** Modify the \"Feature Extraction Training Run\" section (likely starting around Cell 8):\n",
    "    * Change the `model_to_train` variable to use the `resnet18` model instance (which was already loaded and adapted earlier in this notebook).\n",
    "    * Update the `model_name` variable accordingly (e.g., `\"ResNet18_FeatureExtract\"`).\n",
    "    * Double-check that the `optimizer` is correctly configured to target *only* the trainable parameters of the modified `resnet18` model (i.e., the parameters of the new `resnet18.fc` layer). The code provided, which filters `params_to_update` based on `param.requires_grad`, should handle this correctly. Keep the loss function (`nn.CrossEntropyLoss`) and Adam learning rate (`lr=0.001`) the same as the MobileNetV3 run for a fair comparison at this stage.\n",
    "3.  **Run Training:** Execute the training loop for the same number of epochs used for the MobileNetV3 demo in this lecture (e.g., 5 epochs).\n",
    "4.  **Report Results:** Ensure the output cells clearly display the final training loss, training accuracy, validation loss, and validation accuracy achieved by ResNet18 after the training run completes.\n",
    "5.  **Brief Comparison:** Add a short Markdown cell at the end comparing the final validation accuracy of ResNet18 (from your run) to the final validation accuracy achieved by MobileNetV3 in the lecture's demo run. Just a sentence or two noting which performed better in this initial feature extraction phase is sufficient."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
