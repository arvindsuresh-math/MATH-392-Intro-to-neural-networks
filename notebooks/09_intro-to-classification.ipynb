{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to classification\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Introduction to Classification (10 min)\n",
    "\t•\tGoal: Introduce classification as a fundamental problem in machine learning.\n",
    "\t•\tKey Concepts:\n",
    "\t•\tWhat is classification?\n",
    "\t•\tExamples of binary classification problems (e.g., spam vs. non-spam, disease vs. no disease).\n",
    "\t•\tDistinction between linear and non-linear classifiers.\n",
    "\t•\tCode Snippets/Visualizations:\n",
    "\t•\tGenerate and visualize a simple 2D dataset with two classes using matplotlib and scikit-learn.datasets.make_classification.\n",
    "\n",
    "2. Basics of Probability Theory (15 min)\n",
    "\t•\tGoal: Introduce key probability concepts relevant to classification.\n",
    "\t•\tKey Concepts:\n",
    "\t•\tJoint probability ￼, conditional probability ￼, and independence.\n",
    "\t•\tBayes’ Theorem and its intuition.\n",
    "\t•\tExample: Given symptoms, what is the probability of having a disease?\n",
    "\t•\tCode Snippets/Visualizations:\n",
    "\t•\tDefine and compute joint/conditional probabilities using NumPy.\n",
    "\t•\tVisualize conditional probability using a simple probability table or heatmap.\n",
    "\n",
    "3. Naïve Bayes Classifier (20 min)\n",
    "\t•\tGoal: Introduce the Naïve Bayes model as a simple probabilistic classifier.\n",
    "\t•\tKey Concepts:\n",
    "\t•\tBayes’ rule applied to classification.\n",
    "\t•\tNaïve assumption: Features are conditionally independent given the class.\n",
    "\t•\tDerivation of the log-likelihood form for classification.\n",
    "\t•\tExample: Classifying spam emails based on word frequencies.\n",
    "\t•\tCode Snippets/Visualizations:\n",
    "\t•\tImplement a basic Naïve Bayes classifier on a toy dataset.\n",
    "\t•\tShow decision boundaries using matplotlib.contourf.\n",
    "\n",
    "4. Logistic Regression (20 min)\n",
    "\t•\tGoal: Introduce logistic regression as a linear classifier based on probability modeling.\n",
    "\t•\tKey Concepts:\n",
    "\t•\tLogistic function: ￼.\n",
    "\t•\tLog-odds and interpretation of model parameters.\n",
    "\t•\tOptimization via gradient descent.\n",
    "\t•\tComparison with Naïve Bayes.\n",
    "\t•\tCode Snippets/Visualizations:\n",
    "\t•\tImplement logistic regression using scikit-learn or torch.\n",
    "\t•\tVisualize the sigmoid function and decision boundary.\n",
    "\n",
    "5. Summary and Discussion (10 min)\n",
    "\t•\tKey Takeaways:\n",
    "\t•\tClassification as probability estimation.\n",
    "\t•\tComparison of Naïve Bayes and logistic regression.\n",
    "\t•\tLimitations of linear classifiers and transition to non-linear methods.\n",
    "\t•\tInteractive Elements:\n",
    "\t•\tSmall quiz: Given some data, which classifier would work better?\n",
    "\t•\tDiscussion on real-world applications.\n",
    "\n",
    "Suggested Code Elements\n",
    "\t•\tData Visualization: Use matplotlib to show decision boundaries and probabilities.\n",
    "\t•\tProbability Computation: Use NumPy to compute conditional probabilities.\n",
    "\t•\tClassifier Implementation: Implement Naïve Bayes and logistic regression using scikit-learn.\n",
    "\t•\tOptimization: Demonstrate gradient descent using torch (optional).\n",
    "\n",
    "Would you like any refinements, such as more focus on derivations or more interactive elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "So far, we've been focussing on *regression* tasks in machine learning, where the goal is to predict a continuous value. In this lesson, we'll shift our focus to *classification*, in which the goal is to predict a discrete label or category."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
