{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2aeeaa0",
   "metadata": {},
   "source": [
    "# Neural network training with PyTorch\n",
    "\n",
    "Outline:\n",
    "1. Making train, validation, and test datasets\n",
    "2. Splitting into features and targets\n",
    "3. Converting to PyTorch tensors\n",
    "4. Creating a DataLoader for batching\n",
    "5. Defining the neural network architecture\n",
    "6. Defining the loss function\n",
    "7. Training the model\n",
    "8. Evaluating the model\n",
    "9. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "326d8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e531b1f",
   "metadata": {},
   "source": [
    "### 1. Making train, validation, and test datasets\n",
    "\n",
    "The datasets `probability_{year}.csv` in the `data` folder contain various probabilities as features (and targets). For this notebook:\n",
    "- We combine the years $2008,2012$ into the training set.\n",
    "- We use the year $2016$ for validation.\n",
    "- We use the year $2020$ for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "93aba3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6196, 172)\n",
      "Validation shape: (3098, 172)\n",
      "Test shape: (3098, 172)\n"
     ]
    }
   ],
   "source": [
    "# load the datasets\n",
    "df_2008 = pd.read_csv('../midterm/data/probabilities_2008.csv')\n",
    "df_2012 = pd.read_csv('../midterm/data/probabilities_2012.csv')\n",
    "df_2016 = pd.read_csv('../midterm/data/probabilities_2016.csv')\n",
    "df_2020 = pd.read_csv('../midterm/data/probabilities_2020.csv')\n",
    "\n",
    "# make train, val, and test sets\n",
    "df_train = pd.concat([df_2008, df_2012])\n",
    "df_val = df_2016.copy()\n",
    "df_test = df_2020.copy()\n",
    "\n",
    "# display shapes of the datasets\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Validation shape: {df_val.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84cbcb",
   "metadata": {},
   "source": [
    "### 2. Splitting into features and targets\n",
    "\n",
    "- **Targets**: Our target for this notebook will be not one column, but the three columns:\n",
    "    - `P(d|C)` - probability of voting democrat, given the county\n",
    "    - `P(r|C)` - probability of voting republican, given the county\n",
    "    - `P(o|C)` - probability of voting third party or not voting, given the county\n",
    "Note that for each row, the three columns sum to $1$, i.e. they form a probability distribution with $3$ classes. Our goal is to predict the three probabilities for each county, as well as for the whole country.\n",
    "- **Features**: The features are all the columns in the dataset *after* the three target columns. That is, `features = df.columns[8:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5e270615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr shape: (6196, 164), y_tr shape: (6196, 3)\n",
      "X_val shape: (3098, 164), y_val shape: (3098, 3)\n",
      "X_te shape: (3098, 164), y_te shape: (3098, 3)\n"
     ]
    }
   ],
   "source": [
    "# Get features and targets\n",
    "target_cols = ['P(d|C)', 'P(r|C)', 'P(o|C)']\n",
    "feature_cols = df_train.columns[8:]  # All columns after the target columns\n",
    "\n",
    "# Split into features and targets for each set\n",
    "X_tr = df_train[feature_cols].values\n",
    "y_tr = df_train[target_cols].values\n",
    "\n",
    "X_val = df_val[feature_cols].values\n",
    "y_val = df_val[target_cols].values\n",
    "\n",
    "X_te = df_test[feature_cols].values\n",
    "y_te = df_test[target_cols].values\n",
    "\n",
    "# Also need county weights for custom loss; explained later\n",
    "w_tr = df_train['P(C)'].values\n",
    "w_val = df_val['P(C)'].values\n",
    "w_te = df_test['P(C)'].values\n",
    "\n",
    "# print shapes of all X sets\n",
    "print(f\"X_tr shape: {X_tr.shape}, y_tr shape: {y_tr.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_te shape: {X_te.shape}, y_te shape: {y_te.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a400fe",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "Before going any further, it is important to pre-process the data. In our present case, we will standardize each feature in the training set to have mean $0$ and standard deviation $1$. We will then apply the same transformation to the validation and test sets (using the mean and standard deviation from the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "70876535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After splitting into X_train, X_val, X_test but before creating tensors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "\n",
    "# Transform validation and test sets using training parameters\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_te_scaled = scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed458e4",
   "metadata": {},
   "source": [
    "### 3. PyTorch tensors\n",
    "\n",
    "#### What are tensors?\n",
    "Mathematically, tensors are a generalization of matrices. Namely, a tensor is a multi-dimensional array. For example, a vector is a 1D tensor, a matrix is a 2D tensor, and a 3D tensor is a 3D array. \n",
    "\n",
    "The **shape** of a tensor is the number of elements in each dimension. For example:\n",
    "- A row vector is a 1D tensor with shape `(,n)` or `(1,n)`, where `n` is the number of elements in the vector.\n",
    "- A column vector is a 1D tensor with shape `(n,)` or `(n,1)`, where `n` is the number of elements in the vector.\n",
    "- A matrix is a 2D tensor with shape `(m,n)`, where `m` is the number of rows and `n` is the number of columns. \n",
    "- A 3D array (consisting of entries which are located using $3$ indices) is a 3D tensor with shape `(m,n,p)`, where `m` is the number of rows, `n` is the number of columns, and `p$ is the number of... something else (say, depth).\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch. They are similar to NumPy arrays, but they can be used on GPUs to accelerate computing. PyTorch tensors can be created from NumPy arrays, and they can also be converted back to NumPy arrays. Let's demonstrate some of the common operations on PyTorch tensors below.\n",
    "\n",
    "#### Creating PyTorch tensors\n",
    "There are multiple ways to create PyTorch tensors:\n",
    "- From a NumPy array, using `torch.from_numpy()` or `torch.FloatTensor()` (if you want to create a float tensor)\n",
    "- From a list, using `torch.tensor()`\n",
    "- From a scalar, using `torch.tensor()`\n",
    "- From a random number generator, using `torch.randn()`, `torch.rand()`, `torch.randint()`, etc.\n",
    "We give examples of each below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b0ebce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From NumPy:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "DataType: torch.int64\n",
      "\n",
      "From list:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "DataType: torch.float32\n",
      "\n",
      "From scalar: 3.140000104904175\n",
      "DataType: torch.float32\n",
      "\n",
      "Random uniform [0,1]:\n",
      "tensor([[0.0461, 0.0600, 0.3064],\n",
      "        [0.2226, 0.1156, 0.6165]])\n",
      "\n",
      "Random normal (mean=0, std=1):\n",
      "tensor([[-0.9841,  0.4489,  0.0991],\n",
      "        [-0.5005, -0.4277,  1.4543]])\n",
      "\n",
      "Random integers [0,10):\n",
      "tensor([[8, 8, 7],\n",
      "        [0, 9, 4]])\n",
      "\n",
      "Ones:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Zeros:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Identity:\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors from different sources\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. From NumPy arrays\n",
    "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)  # Keeps original datatype\n",
    "tensor_float = torch.FloatTensor(numpy_array)      # Converts to float32\n",
    "print(f\"From NumPy:\\n{tensor_from_numpy}\")\n",
    "print(f\"DataType: {tensor_from_numpy.dtype}\\n\")\n",
    "\n",
    "# 2. From Python lists\n",
    "list_data = [[1, 2, 3], [4, 5, 6]]\n",
    "tensor_from_list = torch.tensor(list_data, dtype=torch.float32)\n",
    "print(f\"From list:\\n{tensor_from_list}\")\n",
    "print(f\"DataType: {tensor_from_list.dtype}\\n\")\n",
    "\n",
    "# 3. From scalars\n",
    "scalar_tensor = torch.tensor(3.14)\n",
    "print(f\"From scalar: {scalar_tensor}\")\n",
    "print(f\"DataType: {scalar_tensor.dtype}\\n\")\n",
    "\n",
    "# 4. From random number generators\n",
    "# 4a. Uniform random [0,1]\n",
    "random_uniform = torch.rand(2, 3)  # 2x3 matrix with values in [0,1]\n",
    "print(f\"Random uniform [0,1]:\\n{random_uniform}\\n\")\n",
    "\n",
    "# 4b. Normal distribution (mean=0, std=1)\n",
    "random_normal = torch.randn(2, 3)  # 2x3 matrix from standard normal\n",
    "print(f\"Random normal (mean=0, std=1):\\n{random_normal}\\n\")\n",
    "\n",
    "# 4c. Random integers\n",
    "random_ints = torch.randint(low=0, high=10, size=(2, 3))  # 2x3 matrix, ints in [0,10)\n",
    "print(f\"Random integers [0,10):\\n{random_ints}\\n\")\n",
    "\n",
    "# 5. Special tensors\n",
    "ones = torch.ones(2, 3)      # 2x3 matrix of ones\n",
    "zeros = torch.zeros(2, 3)    # 2x3 matrix of zeros\n",
    "identity = torch.eye(3)      # 3x3 identity matrix\n",
    "print(f\"Ones:\\n{ones}\\n\")\n",
    "print(f\"Zeros:\\n{zeros}\\n\")\n",
    "print(f\"Identity:\\n{identity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47ab76",
   "metadata": {},
   "source": [
    "In our case, we already have dataframes with the training, validation, and test sets. We can convert them to PyTorch tensors using `torch.FloatTensor()` (which is appropriate because all our data consists of floats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b10ccd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_tr_tensor = torch.FloatTensor(X_tr_scaled)\n",
    "y_tr_tensor = torch.FloatTensor(y_tr)\n",
    "w_tr_tensor = torch.FloatTensor(w_tr)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val)\n",
    "w_val_tensor = torch.FloatTensor(w_val)\n",
    "\n",
    "X_te_tensor = torch.FloatTensor(X_te_scaled)\n",
    "y_te_tensor = torch.FloatTensor(y_te)\n",
    "w_te_tensor = torch.FloatTensor(w_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c652e",
   "metadata": {},
   "source": [
    "#### Attributes of PyTorch tensors\n",
    "PyTorch tensors have the following attributes:\n",
    "- `shape`: the shape of the tensor\n",
    "- `dtype`: the data type of the tensor\n",
    "- `device`: the device on which the tensor is stored (CPU or GPU)\n",
    "- `requires_grad`: whether the tensor requires gradients (for backpropagation)\n",
    "\n",
    "Let's first demonstrate the `shape`, `dtype`, and `requires_grad` attributes of PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db1c1c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr_tensor attributes:\n",
      "Shape: torch.Size([6196, 164])\n",
      "Data type: torch.float32\n",
      "Device: cpu\n",
      "Requires gradients: False\n",
      "\n",
      "Neural network weight tensor attributes:\n",
      "Shape: torch.Size([164, 3])\n",
      "Data type: torch.float32\n",
      "Device: cpu\n",
      "Requires gradients: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate tensor attributes using our training tensors\n",
    "print(\"X_tr_tensor attributes:\")\n",
    "print(f\"Shape: {X_tr_tensor.shape}\")  # Shows dimensions (num_samples, num_features)\n",
    "print(f\"Data type: {X_tr_tensor.dtype}\")  # Should be torch.float32\n",
    "print(f\"Device: {X_tr_tensor.device}\")  # Shows if tensor is on CPU or GPU\n",
    "print(f\"Requires gradients: {X_tr_tensor.requires_grad}\\n\")  # Default is False\n",
    "\n",
    "# Create a tensor that requires gradients (common for neural network parameters)\n",
    "weight = torch.randn(X_tr_tensor.shape[1], 3, requires_grad=True)\n",
    "print(\"Neural network weight tensor attributes:\")\n",
    "print(f\"Shape: {weight.shape}\")  # (num_features, num_classes)\n",
    "print(f\"Data type: {weight.dtype}\")\n",
    "print(f\"Device: {weight.device}\")\n",
    "print(f\"Requires gradients: {weight.requires_grad}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1082dc5a",
   "metadata": {},
   "source": [
    "#### Device for PyTorch tensors\n",
    "PyTorch tensors can be stored on either the CPU or the GPU. By default, PyTorch tensors are stored on the CPU. However, if you have a GPU available, you can move the tensor to the GPU using the `.to()` method. For example, `tensor.to('cuda')` will move the tensor to the GPU on a Windows OS, and `tensor.to('mps')` will move the tensor to the GPU on a Mac OS. You can also move the tensor back to the CPU using `tensor.to('cpu')`.\n",
    "\n",
    "Let's illustrate this with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6d4f439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Tensor device: mps:0\n",
      "Tensor moved back to CPU: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for MPS (Metal Performance Shaders) availability on Apple Silicon\n",
    "# or CUDA availability on machines with NVIDIA GPUs\n",
    "device = (\n",
    "    \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Move tensor to appropriate device\n",
    "device = torch.device(device)\n",
    "X_tr_gpu = X_tr_tensor.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Tensor device: {X_tr_gpu.device}\")\n",
    "\n",
    "# Move tensor back to CPU\n",
    "X_tr_cpu = X_tr_gpu.to(\"cpu\")\n",
    "print(f\"Tensor moved back to CPU: {X_tr_cpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf77fb",
   "metadata": {},
   "source": [
    "#### Best practices for device usage\n",
    "- Initially store all tensors on the CPU.\n",
    "- Move the tensors to the GPU only when you need to perform computations on them (i.e. inside the training loop), and don't move too many at once because GPU memory is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b51f04",
   "metadata": {},
   "source": [
    "#### Operations on PyTorch tensors\n",
    "PyTorch tensors support a wide variety of operations, including:\n",
    "- Element-wise operations (addition, subtraction, multiplication, division)\n",
    "- Matrix operations (dot product, matrix multiplication, transpose)\n",
    "- Reduction operations (sum, mean, max, min)\n",
    "- Indexing and slicing\n",
    "- Reshaping and resizing\n",
    "- Concatenation and stacking\n",
    "\n",
    "Let's demonstrate some of these operations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ae7a9d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor shape: torch.Size([6196, 164])\n",
      "\n",
      "Element-wise operations:\n",
      "After standardization - mean: 0.000, std: 1.000\n",
      "\n",
      "Standardized tensor shape: torch.Size([6196, 164])\n",
      "\n",
      "Tensor of means: tensor([ 1.2313e-09, -1.2313e-09,  3.0784e-10,  3.0784e-10, -2.7705e-09,\n",
      "         8.4655e-10,  0.0000e+00,  4.6175e-10, -1.5392e-09,  2.4627e-09,\n",
      "         9.2351e-10,  3.6940e-09, -6.1567e-09,  1.0774e-09,  4.6175e-10,\n",
      "         0.0000e+00,  1.0774e-09, -4.3097e-09, -4.8484e-09, -2.0009e-09,\n",
      "        -6.1567e-10,  1.5392e-10,  2.1548e-09,  4.0019e-09,  9.2351e-10,\n",
      "        -2.4627e-09, -3.0784e-10, -2.4627e-09,  9.2351e-10,  1.8470e-09,\n",
      "         1.0774e-09,  2.1548e-09,  3.6940e-09,  6.1567e-10, -1.5392e-09,\n",
      "        -4.6175e-10,  1.7701e-09,  8.4655e-10, -6.1567e-10, -6.1567e-10,\n",
      "        -2.1548e-09,  3.0784e-10,  1.2313e-09,  1.5392e-09, -3.0784e-09,\n",
      "         1.8470e-09,  1.0774e-09, -4.3097e-09, -6.1567e-10, -1.8470e-09,\n",
      "        -1.5392e-09, -2.1548e-09, -1.8470e-09,  1.2236e-08, -2.4627e-09,\n",
      "         1.8470e-09,  2.4627e-09,  1.8470e-09, -1.2313e-09,  1.2313e-09,\n",
      "        -6.1567e-09, -2.7705e-09, -2.4627e-09,  0.0000e+00, -6.1567e-10,\n",
      "        -3.6940e-09,  1.2313e-09,  2.7705e-09, -3.0784e-09, -4.6175e-10,\n",
      "         2.1548e-09,  2.1548e-09,  0.0000e+00,  2.0009e-09, -1.5392e-09,\n",
      "        -3.0784e-09,  4.3097e-09, -2.1548e-09,  6.1567e-10, -6.1567e-10,\n",
      "        -4.6175e-10,  3.0784e-09,  1.8470e-09,  1.2313e-09,  2.1548e-09,\n",
      "         1.2313e-09,  2.4627e-09, -1.3853e-09,  0.0000e+00, -3.6940e-09,\n",
      "         2.4627e-09,  2.4627e-09,  1.2313e-09,  1.3545e-08, -4.6175e-09,\n",
      "         1.8470e-09, -7.5420e-09,  1.2313e-09,  1.8470e-09, -3.3862e-09,\n",
      "         4.6175e-09,  9.2351e-10,  0.0000e+00,  6.1567e-10,  6.1567e-09,\n",
      "        -6.1567e-10,  6.1567e-10,  1.0774e-09, -2.4627e-09, -2.4627e-09,\n",
      "         1.8470e-09, -6.1567e-10,  0.0000e+00, -3.0784e-09,  7.6959e-09,\n",
      "        -6.1567e-10, -6.1567e-10,  1.2313e-09,  2.7705e-09,  3.0784e-10,\n",
      "        -9.2351e-10,  1.0774e-08, -1.2313e-09, -6.1567e-10, -1.0466e-08,\n",
      "        -9.2351e-10,  6.1567e-10,  2.4627e-09, -1.1159e-09, -2.7705e-09,\n",
      "         6.1567e-10, -1.6931e-09,  6.1567e-10,  6.1567e-10, -1.8470e-09,\n",
      "         8.0037e-09,  1.5392e-09,  0.0000e+00, -3.2323e-09,  1.8470e-09,\n",
      "        -1.2313e-09,  1.2313e-09,  6.9263e-10, -3.0784e-10, -1.8470e-09,\n",
      "         1.2313e-09, -3.6940e-09,  4.9254e-09,  1.2313e-09, -5.0793e-09,\n",
      "        -1.2313e-09,  6.1567e-10, -6.1567e-10,  4.2327e-09,  0.0000e+00,\n",
      "        -6.1567e-10,  2.5396e-09, -1.4622e-09,  3.0784e-10, -3.0784e-10,\n",
      "         9.2351e-10,  2.1548e-09,  6.1567e-10, -9.2351e-10])\n",
      "\n",
      "Tensor of stds: tensor([1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001, 1.0001,\n",
      "        1.0001, 1.0001])\n",
      "\n",
      "Shape of tensor of means: torch.Size([164])\n",
      "\n",
      "Shape of tensor of stds: torch.Size([164])\n",
      "\n",
      "Matrix operations:\n",
      "Original shape: torch.Size([6196, 164]),\n",
      "Transposed shape: torch.Size([164, 6196])\n",
      "Gram matrix shape: torch.Size([164, 164])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of training tensor for demonstrations\n",
    "X = X_tr_tensor.clone()  # clone() creates a copy\n",
    "print(f\"Original tensor shape: {X.shape}\\n\")\n",
    "\n",
    "# 1. Element-wise operations\n",
    "print(\"Element-wise operations:\")\n",
    "X_scaled = X * 2.0  # multiplication by scalar\n",
    "X_normalized = (X - X.mean()) / X.std()  # standardization along all rows and columns\n",
    "print(f\"After standardization - mean: {X_normalized.mean():.3f}, std: {X_normalized.std():.3f}\\n\")\n",
    "# standardize each feature\n",
    "X_means = X.mean(dim=0) # tensor with feature means\n",
    "X_stds = X.std(dim=0)   # tensor with feature stds\n",
    "X_standardized = (X - X_means) / X_stds\n",
    "print(f\"Standardized tensor shape: {X_standardized.shape}\\n\")\n",
    "print(f'Tensor of means: {X_means}\\n')\n",
    "print(f'Tensor of stds: {X_stds}\\n')\n",
    "print(f'Shape of tensor of means: {X_means.shape}\\n')\n",
    "print(f'Shape of tensor of stds: {X_stds.shape}\\n')\n",
    "\n",
    "# 2. Matrix operations\n",
    "print(\"Matrix operations:\")\n",
    "# Transpose\n",
    "X_t = X.T  # or X.transpose(0,1)\n",
    "print(f\"Original shape: {X.shape},\\nTransposed shape: {X_t.shape}\")\n",
    "\n",
    "# Matrix multiplication (assuming X has shape (n_samples, n_features))\n",
    "X_gram = torch.mm(X_t, X)  # Gram matrix\n",
    "print(f\"Gram matrix shape: {X_gram.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d4592a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction operations:\n",
      "Mean of all elements: 0.000\n",
      "Sum of all elements: 0.000\n",
      "Max value: 68.518\n",
      "Min value: -9.495\n",
      "Mean per feature (first 3): tensor([ 1.2313e-09, -1.2313e-09,  3.0784e-10])\n",
      "\n",
      "Indexing and slicing:\n",
      "Subset shape: torch.Size([10, 5])\n",
      "\n",
      "Reshaping:\n",
      "Reshaped tensor shape: torch.Size([6196, 164])\n",
      "\n",
      "Concatenation and stacking:\n",
      "After row concatenation: torch.Size([12392, 164])\n",
      "After stacking: torch.Size([2, 6196, 164])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Reduction operations\n",
    "print(\"Reduction operations:\")\n",
    "print(f\"Mean of all elements: {X.mean():.3f}\")\n",
    "print(f\"Sum of all elements: {X.sum():.3f}\")\n",
    "print(f\"Max value: {X.max():.3f}\")\n",
    "print(f\"Min value: {X.min():.3f}\")\n",
    "# Per-feature statistics\n",
    "print(f\"Mean per feature (first 3): {X.mean(dim=0)[:3]}\\n\")\n",
    "\n",
    "# 4. Indexing and slicing\n",
    "print(\"Indexing and slicing:\")\n",
    "first_sample = X[0]  # First sample\n",
    "first_feature = X[:, 0]  # First feature for all samples\n",
    "subset = X[:10, :5]  # First 10 samples, first 5 features\n",
    "print(f\"Subset shape: {subset.shape}\\n\")\n",
    "\n",
    "# 5. Reshaping and resizing\n",
    "print(\"Reshaping:\")\n",
    "n_samples, n_features = X.shape\n",
    "X_reshaped = X.reshape(n_samples, -1)  # -1 automatically calculates size\n",
    "X_viewed = X.view(n_samples, -1)  # similar to reshape\n",
    "print(f\"Reshaped tensor shape: {X_reshaped.shape}\\n\")\n",
    "\n",
    "# 6. Concatenation and stacking\n",
    "print(\"Concatenation and stacking:\")\n",
    "# Concatenate along rows (dim=0)\n",
    "X_doubled = torch.cat([X, X], dim=0)\n",
    "print(f\"After row concatenation: {X_doubled.shape}\")\n",
    "# Stack creates a new dimension (the first dimension)\n",
    "X_stacked = torch.stack([X, X], dim=0)\n",
    "print(f\"After stacking: {X_stacked.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569da8a8",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "Broadcasting is a powerful mechanism that allows PyTorch to perform operations on tensors of different shapes. It automatically expands smaller tensors to match the shape of larger tensors without creating copies of the data. This is useful for performing element-wise operations on tensors of different shapes.\n",
    "\n",
    "**Broadcasting rules**:\n",
    "1. Starting from the *right-most dimension*, the dimensions of the two tensors are compared one-by-one:\n",
    "    - If they are equal, they are compatible (no broadcasting is needed).\n",
    "    - If the smaller tensor has a dimension of size $1$, it is expanded to match the size of the larger tensor.\n",
    "    - If the smaller tensor is missing a dimension, it is added with size $1$.\n",
    "    - If the dimensions are not compatible, an error is raised.\n",
    "\n",
    "The result of the operation is a tensor with the shape of the larger tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "052a09e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix: tensor([[-1.1553,  1.7549, -0.3633],\n",
      "        [-0.9738, -0.8056, -0.6675],\n",
      "        [-0.0763, -0.1439,  0.7567],\n",
      "        [ 0.2190, -0.5779,  0.7476],\n",
      "        [ 0.2499,  0.1428,  0.4817]])\n",
      "Matrix shape: torch.Size([5, 3]) \n",
      "\n",
      "Vector: tensor([-0.1299,  0.4747, -0.3385])\n",
      "Vector: torch.Size([3]) \n",
      "\n",
      "matrix + vector:\n",
      "tensor([[-1.2853,  2.2296, -0.7019],\n",
      "        [-1.1037, -0.3309, -1.0060],\n",
      "        [-0.2062,  0.3308,  0.4182],\n",
      "        [ 0.0890, -0.1032,  0.4091],\n",
      "        [ 0.1200,  0.6175,  0.1432]])\n",
      "Shape of matrix + vector: torch.Size([5, 3])\n",
      "\n",
      "Scalar tensor tensor(2.)\n",
      "Scalar shape torch.Size([]) \n",
      "\n",
      "matrix * scalar:\n",
      "tensor([[-2.3107,  3.5098, -0.7267],\n",
      "        [-1.9475, -1.6112, -1.3350],\n",
      "        [-0.1526, -0.2879,  1.5134],\n",
      "        [ 0.4379, -1.1558,  1.4953],\n",
      "        [ 0.4998,  0.2856,  0.9635]])\n",
      "Shape of matrix * scalar: torch.Size([5, 3])\n",
      "\n",
      "\n",
      "Complex broadcasting:\n",
      "A shape: torch.Size([4, 1, 3])\n",
      "B shape: torch.Size([1, 2, 3])\n",
      "Result shape: torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic Broadcasting Example\n",
    "\n",
    "# Create tensors of different shapes\n",
    "matrix = torch.randn(5, 3)           # Shape: (5, 3)\n",
    "print('Matrix:', matrix)\n",
    "print('Matrix shape:', matrix.shape,'\\n')\n",
    "\n",
    "vector = torch.randn(3)              # Shape: (3,)\n",
    "print('Vector:', vector)\n",
    "print('Vector:', vector.shape,'\\n')\n",
    "\n",
    "result1 = matrix + vector           # Vector is broadcast to shape (5, 3)\n",
    "\n",
    "# Display individual tensors and the result after adding/multiplying\n",
    "print(f\"matrix + vector:\\n{result1}\")\n",
    "print(f'Shape of matrix + vector: {result1.shape}\\n')\n",
    "\n",
    "scalar = torch.tensor(2.0)           # Shape: ()\n",
    "print('Scalar tensor', scalar)\n",
    "print('Scalar shape', scalar.shape,'\\n')\n",
    "\n",
    "result2 = matrix * scalar           # Scalar is broadcast to shape (5, 3)\n",
    "\n",
    "print(f\"matrix * scalar:\\n{result2}\")\n",
    "print(f'Shape of matrix * scalar: {result2.shape}\\n')\n",
    "\n",
    "# 2. More Complex Broadcasting Example\n",
    "# Create tensors with compatible shapes for broadcasting\n",
    "A = torch.randn(4, 1, 3)           # Shape: (4, 1, 3)\n",
    "B = torch.randn(1, 2, 3)           # Shape: (1, 5, 3)\n",
    "C = A + B                          # Result shape: (4, 5, 3)\n",
    "\n",
    "print(\"\\nComplex broadcasting:\")\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"Result shape: {C.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b376bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 10])\n",
      "bias shape: torch.Size([10])\n",
      "X + bias shape: torch.Size([32, 10])\n",
      "\n",
      "Batch mean shape: torch.Size([10])\n",
      "Normalized shape: torch.Size([32, 10])\n",
      "\n",
      "Batch mean shape with keepdim=True: torch.Size([1, 10])\n",
      "Normalized shape with keepdim=True: torch.Size([32, 10])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Common use cases in neural networks\n",
    "\n",
    "# 1. Adding bias terms\n",
    "X = torch.randn(32, 10)    # 32 samples, 10 features\n",
    "bias = torch.randn(10)            # 10 bias terms\n",
    "output = X + bias          # bias is broadcast to (32, 10)\n",
    "\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'bias shape: {bias.shape}')\n",
    "print(f'X + bias shape: {output.shape}\\n')\n",
    "\n",
    "# 2. Batch normalization\n",
    "batch_mean = X.mean(dim=0, keepdim=False)  # Shape: (n_features)\n",
    "normalized = X - batch_mean      \n",
    "print(f'Batch mean shape: {batch_mean.shape}')\n",
    "print(f'Normalized shape: {normalized.shape}\\n')\n",
    "\n",
    "# Alternative with keepdim=True\n",
    "batch_mean = X.mean(dim=0, keepdim=True)  # Shape: (1, n_features)\n",
    "normalized = X - batch_mean\n",
    "print(f'Batch mean shape with keepdim=True: {batch_mean.shape}')\n",
    "print(f'Normalized shape with keepdim=True: {normalized.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f63c1da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      3\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m c \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m+\u001b[39m b\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# This will raise an error\n",
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(2, 3)\n",
    "c = a + b  # Error: shapes (3,4) and (2,3) cannot be broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94d41c",
   "metadata": {},
   "source": [
    "### 4. Creating a DataLoader for batching\n",
    "In today's notebook we will use \"mini-batch\" gradient descent to train our network. This means that we will use a small batch of data to compute the gradients and update the weights, instead of using the entire dataset. This is done to speed up the training process and to reduce the memory usage.\n",
    "\n",
    "PyTorch provides a convenient way to create mini-batches using the `DataLoader` class. The `DataLoader` class takes a dataset and creates mini-batches of a specified size. It also shuffles the data and allows for parallel loading of data using multiple workers. \n",
    "\n",
    "Below, we will create a `DataLoader` for our training, validation, and test sets. Later, we will use the `DataLoader` to iterate over the mini-batches during training and evaluation (moving the tensors to the GPU when needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f3772c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating batch iteration:\n",
      "Batch 0\n",
      "Features shape: torch.Size([32, 164])\n",
      "Targets shape: torch.Size([32, 3])\n",
      "Weights shape: torch.Size([32])\n",
      "Batch 1\n",
      "Features shape: torch.Size([32, 164])\n",
      "Targets shape: torch.Size([32, 3])\n",
      "Weights shape: torch.Size([32])\n",
      "Batch 2\n",
      "Features shape: torch.Size([32, 164])\n",
      "Targets shape: torch.Size([32, 3])\n",
      "Weights shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Import TensorDataset and DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_tr_tensor, y_tr_tensor, w_tr_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor, w_val_tensor)\n",
    "test_dataset = TensorDataset(X_te_tensor, y_te_tensor, w_te_tensor)\n",
    "\n",
    "batch_size = 32 # Common batch sizes are 32, 64, 128\n",
    "\n",
    "# Create DataLoaders\n",
    "\n",
    "# Training DataLoader with shuffling\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True        # Shuffle training data to avoid learning order patterns\n",
    ")\n",
    "# Validation and Test DataLoaders (no shuffling needed)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Example: Iterating through batches\n",
    "print(\"Demonstrating batch iteration:\")\n",
    "# choose device ('mps' for mac and 'cuda' for NVIDIA GPUs)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for batch_idx, (features, targets, weights) in enumerate(train_loader):\n",
    "    # Move batch to device if using GPU\n",
    "    features = features.to(device)\n",
    "    targets = targets.to(device)\n",
    "    weights = weights.to(device)\n",
    "    \n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    print(f\"Features shape: {features.shape}\")  # Should be (batch_size, n_features)\n",
    "    print(f\"Targets shape: {targets.shape}\")    # Should be (batch_size, 3)\n",
    "    print(f\"Weights shape: {weights.shape}\")    # Should be (batch_size,)\n",
    "    \n",
    "    if batch_idx >= 2:  # Only show first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e1967b",
   "metadata": {},
   "source": [
    "### 5. Defining the neural network architecture\n",
    "\n",
    "#### Layers\n",
    "We will use the following architecture for our neural network:\n",
    "- An input layer with `n_features` neurons (where `n_features` is the number of features in the dataset)\n",
    "- A hidden layer with `n_hidden` neurons (where `n_hidden` is a hyperparameter that we will tune)\n",
    "- An output layer with `3` neurons (one for each target)\n",
    "\n",
    "#### Activation functions\n",
    "We will try multiple activation functions for the hidden layer (ReLU, tanh, sigmoid) and the softmax activation function for the output layer. Thus, mathematically, our neural network can be represented as the composition of functions:\n",
    "\\begin{equation*}\n",
    "    \\mathbb{R}^{n_{\\textup{features}}} \\xrightarrow{W_1,b_1} \\mathbb{R}^{n_{\\textup{hidden}}} \\xrightarrow{\\textup{activation}} \\mathbb{R}^{n_{\\textup{hidden}}} \\xrightarrow{W_2,b_2} \\mathbb{R}^{3} \\xrightarrow{\\textup{softmax}} \\mathbb{R}^{3},\n",
    "\\end{equation*}\n",
    "where:\n",
    "- $W_1$ and $b_1$ are the weights and biases of the first layer, shapes $(n_{\\textup{hidden}},n_{\\textup{features}})$ and $(n_{\\textup{hidden}},1)$ respectively\n",
    "- $W_2$ and $b_2$ are the weights and biases of the second layer, shapes $(3,n_{\\textup{hidden}})$ and $(3,1)$ respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f0e1f",
   "metadata": {},
   "source": [
    "### 6. Defining the loss function\n",
    "\n",
    "#### Sample loss\n",
    "In a usual three-class classification problem, there would be a **single** column with the class labels (e.g. $1,2,3$). In our case, we have three columns with the probabilities of each class (sometimes called the **soft labels**). \n",
    "\n",
    "You can check that basically all counties have maximum probability of `P(o|C)`, so it is not useful to convert the three columns into a single column with the class labels. \n",
    "\n",
    "Instead, we can *directly* use the three columns as the vector of true probabilities in the **cross-entropy loss function**. Thus, if our output for a particular row is $y = (y_1,y_2,y_3)$ and the true probabilities are $p = (p_1,p_2,p_3)$, the loss function (for the particular sample $C$) is given by:\n",
    "\\begin{equation*}\n",
    "    \\textup{sample loss} = L(y,p;C) = -\\sum_{i=1}^{3} p_i \\log(y_i).\n",
    "\\end{equation*}\n",
    "This is the same as the usual cross-entropy loss function, but with the true probabilities from the dataset instead of the class labels.\n",
    "\n",
    "#### Batch loss\n",
    "Note that different counties contribute differently to the overall vote counts. Namely, the counties with more voters contribute more to the overall vote counts. Thus, it is natural to weight each sample loss by the `P(C)` column, which represents the probability that a randomly chosen person is from that county. Thus, our batch loss is given by:\n",
    "\\begin{equation*}\n",
    "    \\textup{batch loss} = L(\\textup{batches},y,p) = \\sum_{C \\in \\textup{batches}} P(C) \\cdot L(y,p;C).\n",
    "\\end{equation*}\n",
    "\n",
    "#### Implementation of custom loss function\n",
    "We will implement the custom loss function as a PyTorch module. This will allow us to use it in the training loop and to compute the gradients automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a5338a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, outputs, targets, weights):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs: Model predictions (batch_size, 3) before softmax\n",
    "            targets: True probability distributions (batch_size, 3)\n",
    "            weights: County weights P(C) (batch_size,)\n",
    "        \"\"\"\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        eps = 1e-7\n",
    "        outputs = torch.clamp(outputs, eps, 1.0)\n",
    "        \n",
    "        # Compute cross entropy for each sample: -sum(p_i * log(y_i))\n",
    "        sample_losses = -torch.sum(targets * torch.log(outputs), dim=1)\n",
    "        \n",
    "        # Weight each sample loss by P(C)\n",
    "        weighted_losses = sample_losses * weights\n",
    "        \n",
    "        # Return mean over batch\n",
    "        return torch.mean(weighted_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d7b7d",
   "metadata": {},
   "source": [
    "#### Creating the model class\n",
    "We will create a class for our model that inherits from `torch.nn.Module`. This class will contain the following methods:\n",
    "- `__init__`: constructor that initializes the model\n",
    "- `forward`: method that defines the forward pass of the model\n",
    "- `loss`: method that computes the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6b5fe9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_features, # Number of input features\n",
    "                 n_hidden=128, # Number of neurons in hidden layer\n",
    "                 activation='relu', # Activation function ('relu' or 'tanh')\n",
    "                 dropout_rate=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_features (int): Number of input features\n",
    "            n_hidden (int): Number of neurons in hidden layer\n",
    "            activation (str): Activation function ('relu' or 'tanh')\n",
    "            dropout_rate (float): Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input layer -> Hidden layer\n",
    "        self.fc1 = nn.Linear(n_features, n_hidden)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = nn.ReLU() if activation == 'relu' else nn.Tanh()\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        # Hidden layer -> Output layer (3 classes)\n",
    "        self.fc2 = nn.Linear(n_hidden, 3)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using He/Kaiming initialization for ReLU\n",
    "        and Xavier/Glorot initialization for Tanh\"\"\"\n",
    "        if isinstance(self.activation, nn.ReLU):\n",
    "            # He initialization for ReLU\n",
    "            nn.init.kaiming_normal_(self.fc1.weight)\n",
    "            nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        else:\n",
    "            # Xavier initialization for Tanh\n",
    "            nn.init.xavier_normal_(self.fc1.weight)\n",
    "            nn.init.xavier_normal_(self.fc2.weight)\n",
    "        \n",
    "        # Initialize biases to small values\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, 3)\n",
    "        \"\"\"\n",
    "        # First layer + activation\n",
    "        x = self.activation(self.fc1(x))\n",
    "        \n",
    "        # Apply dropout (only during training)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer (no activation - will be applied with temperature scaling later)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5eef3b",
   "metadata": {},
   "source": [
    "### 7. Training the model\n",
    "We will use the following steps to train the model:\n",
    "1. Iterate over all combinations of hyperparameters.\n",
    "2. For each combination, initialize the model, optimizer, and loss function.\n",
    "3. Iterate over `n_epochs`:\n",
    "    - Iterate over mini-batches:\n",
    "        - Move the mini-batch to the GPU (if available)\n",
    "        - Zero the gradients\n",
    "        - Forward pass\n",
    "        - Compute the loss\n",
    "        - Backward pass\n",
    "        - Update the weights\n",
    "4. Save the model with the best validation loss.\n",
    "\n",
    "First, let's define a function for the training loop (since we will want to experiment with different hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3caf11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                train_loader, \n",
    "                val_loader, \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                device, \n",
    "                n_epochs, \n",
    "                temperature):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "            train_loader: DataLoader for training data\n",
    "            val_loader: DataLoader for validation data\n",
    "            criterion: Loss function\n",
    "            optimizer: Optimizer\n",
    "            device: Device to use ('cpu' or 'cuda')\n",
    "            n_epochs: Number of epochs to train\n",
    "            temperature: Temperature for softmax scaling\n",
    "        Returns: tuple with\n",
    "            best_model_state: Best model state\n",
    "            best_val_loss: Best validation loss\n",
    "    \"\"\"\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, targets, weights in train_loader:\n",
    "            # Move to device\n",
    "            features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            outputs = F.softmax(outputs / temperature, dim=1)  # Apply temperature scaling\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets, weights)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation phase, switch to eval mode\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, targets, weights in val_loader:\n",
    "                features, targets, weights = features.to(device), targets.to(device), weights.to(device)\n",
    "                outputs = model(features)\n",
    "                outputs = F.softmax(outputs / temperature, dim=1)\n",
    "                loss = criterion(outputs, targets, weights)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Save model state\n",
    "            best_model_state = model.state_dict()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{n_epochs}:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "    \n",
    "    return best_model_state, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "65b10193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the training loop works\n",
    "\n",
    "#initialize VotingModel\n",
    "n_features = X_tr_tensor.shape[1]  # Number of input features\n",
    "model = VotingModel(n_features, \n",
    "                    n_hidden=128, \n",
    "                    activation='relu', \n",
    "                    dropout_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "618664a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "Training Loss: 0.0003\n",
      "Validation Loss: 0.0003\n",
      "Epoch 2/5:\n",
      "Training Loss: 0.0003\n",
      "Validation Loss: 0.0003\n",
      "Epoch 3/5:\n",
      "Training Loss: 0.0003\n",
      "Validation Loss: 0.0003\n",
      "Epoch 4/5:\n",
      "Training Loss: 0.0003\n",
      "Validation Loss: 0.0003\n",
      "Epoch 5/5:\n",
      "Training Loss: 0.0003\n",
      "Validation Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "n_epochs = 5\n",
    "temperature = 1.0\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = WeightedCrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "best_model_state, best_val_loss = train_model(model, \n",
    "                                              train_loader, \n",
    "                                              val_loader, \n",
    "                                              criterion, \n",
    "                                              optimizer, \n",
    "                                              device, \n",
    "                                              n_epochs, \n",
    "                                              temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2dffd",
   "metadata": {},
   "source": [
    "### 8. Evaluating the model\n",
    "We will use the following steps to evaluate the model:\n",
    "1. Load the model with the best validation loss (i.e. the best choice of hyperparameters).\n",
    "2. Move the model to the GPU (if available).\n",
    "3. Re-training the model on the entire training set (train + val).\n",
    "4. Evaluate the model on the test set:\n",
    "    - Move the test set to the GPU (if available)\n",
    "    - Forward pass\n",
    "    - Compute the weighted cross entropy loss on the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "413eb239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0003\n",
      "\n",
      "Country-wide probabilities:\n",
      "True - Dem: 0.2444, Rep: 0.2241, Other: 0.5315\n",
      "Pred - Dem: 0.1922, Rep: 0.1771, Other: 0.6307\n",
      "\n",
      "Absolute errors:\n",
      "Dem: 0.0523\n",
      "Rep: 0.0469\n",
      "Other: 0.0992\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device, temperature):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and compute country-wide predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        device: Device to use ('cpu' or 'cuda' or 'mps')\n",
    "        temperature: Temperature for softmax scaling\n",
    "    \n",
    "    Returns:\n",
    "        test_loss: Average loss on test set\n",
    "        country_true: True probabilities for whole country (size 3)\n",
    "        country_pred: Predicted probabilities for whole country (size 3)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    \n",
    "    # Arrays to store predictions and true values for all counties\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    all_weights = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for features, targets, weights in test_loader:\n",
    "            # Move to device\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            weights = weights.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            outputs = F.softmax(outputs / temperature, dim=1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets, weights)\n",
    "\n",
    "            # Accumulate loss\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and true values\n",
    "            all_preds.append(outputs.cpu())\n",
    "            all_true.append(targets.cpu())\n",
    "            all_weights.append(weights.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_true = torch.cat(all_true, dim=0)\n",
    "    all_weights = torch.cat(all_weights, dim=0)\n",
    "    \n",
    "    # Compute weighted average for whole country\n",
    "    # Expand weights to match prediction shape\n",
    "    weights_expanded = all_weights.unsqueeze(1)  # Shape: (n_counties, 1)\n",
    "    \n",
    "    # Compute weighted sum of predictions and true values\n",
    "    country_pred = (all_preds * weights_expanded).sum(dim=0)  # Shape: (3,)\n",
    "    country_true = (all_true * weights_expanded).sum(dim=0)   # Shape: (3,)\n",
    "    \n",
    "    # Compute average test loss\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    return test_loss, country_true, country_pred\n",
    "\n",
    "# Use the function to evaluate your model\n",
    "model.load_state_dict(best_model_state)  # Load best model state\n",
    "model.to(device)\n",
    "test_loss, country_true, country_pred = evaluate_model(\n",
    "    model, test_loader, criterion, device, temperature\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Loss: {test_loss:.4f}\\n\")\n",
    "print(\"Country-wide probabilities:\")\n",
    "print(f\"True - Dem: {country_true[0]:.4f}, Rep: {country_true[1]:.4f}, Other: {country_true[2]:.4f}\")\n",
    "print(f\"Pred - Dem: {country_pred[0]:.4f}, Rep: {country_pred[1]:.4f}, Other: {country_pred[2]:.4f}\")\n",
    "\n",
    "# Compute absolute errors\n",
    "abs_errors = torch.abs(country_pred - country_true)\n",
    "print(\"\\nAbsolute errors:\")\n",
    "print(f\"Dem: {abs_errors[0]:.4f}\")\n",
    "print(f\"Rep: {abs_errors[1]:.4f}\")\n",
    "print(f\"Other: {abs_errors[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef145629",
   "metadata": {},
   "source": [
    "### 9. Hyperparameter tuning\n",
    "\n",
    "#### Dropout (for regularization)\n",
    "We will also use dropout in the hidden layer to prevent overfitting. Dropout is a regularization technique that randomly sets a fraction of the neurons to $0$ during training. This forces the network to learn more robust features and prevents overfitting. The dropout rate is a hyperparameter that we will tune.\n",
    "\n",
    "For example, if the dropout rate is $0.5$, then during training, each neuron in the hidden layer has a $50\\%$ chance of being set to $0$. This forces the network to not rely on any particular neuron and to \"use\" all the neurons in the hidden layer. \n",
    "\n",
    "#### Weight decay\n",
    "We will also use weight decay (L2 regularization) to prevent overfitting. Weight decay adds a penalty to the loss function that is proportional to the square of the weights. This forces the network to learn smaller weights and prevents overfitting. The weight decay coefficient is a hyperparameter that we will tune.\n",
    "\n",
    "#### Hyperparameters to be tuned\n",
    "In summary, our training loop will be used to train the weights and also tune the following hyperparameters:\n",
    "- `n_hidden`: number of neurons in the hidden layer\n",
    "- `learning_rate`: learning rate for the optimizer\n",
    "- `batch_size`: size of the mini-batches\n",
    "- `temperature`: temperature for the softmax function (scaled the logits before applying softamx; higher temperature means more uniform distribution, lower temperature means more peaked distribution)\n",
    "- `dropout_rate`: dropout rate for the dropout layer\n",
    "- `activation_function`: activation function for the hidden layer\n",
    "\n",
    "We organize the hyperparameters in a dictionary for easy access during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter configurations\n",
    "hyperparams = {\n",
    "    # Architecture\n",
    "    'n_hidden': [32, 256],\n",
    "    'activation_function': ['relu', 'tanh'],\n",
    "    'dropout_rate': [0.1, 0.5],\n",
    "    \n",
    "    # Training process\n",
    "    'learning_rate': [1e-3, 1e-4],\n",
    "    'batch_size': [32, 64],\n",
    "    'temperature': [1.0, 2.0]  # For softmax\n",
    "}\n",
    "\n",
    "# Example of a specific configuration\n",
    "example_config = {\n",
    "    'n_hidden': 128,\n",
    "    'activation_function': 'relu',\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2d458",
   "metadata": {},
   "source": [
    "Next, let's define a function for the hyperparameter tuning, which requires us to try all combinations of hyperparameters and do a separate training loop for each combo. This is called a **grid search**, because we are searching for the best combination out of a grid of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over hyperparameters\n",
    "def tune_hyperparameters(train_loader, \n",
    "                         val_loader, \n",
    "                         device, \n",
    "                         hyperparams):\n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Iterate over hyperparameter combinations\n",
    "    for n_hidden in hyperparams['n_hidden']:\n",
    "        for act_fn in hyperparams['activation_function']:\n",
    "            for dropout in hyperparams['dropout_rate']:\n",
    "                for lr in hyperparams['learning_rate']:\n",
    "                    for temp in hyperparams['temperature']:\n",
    "                        # Initialize model and optimizer\n",
    "                        model = VotingModel(n_hidden=n_hidden, \n",
    "                                       activation=act_fn,\n",
    "                                       dropout_rate=dropout).to(device)\n",
    "                        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                        criterion = WeightedCrossEntropyLoss()\n",
    "                        \n",
    "                        # Train model\n",
    "                        n_features = X_tr_tensor.shape[1]\n",
    "                        model_state, val_loss = train_model(\n",
    "                            model, train_loader, val_loader,\n",
    "                            criterion, optimizer, device,\n",
    "                            n_epochs=50, temperature=temp\n",
    "                        )\n",
    "                        \n",
    "                        # Update best model if needed\n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            best_config = {\n",
    "                                'n_hidden': n_hidden,\n",
    "                                'activation': act_fn,\n",
    "                                'dropout': dropout,\n",
    "                                'learning_rate': lr,\n",
    "                                'temperature': temp\n",
    "                            }\n",
    "                            best_model_state = model_state\n",
    "    \n",
    "    return best_config, best_model_state, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05cbf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (1567937797.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[147], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# run the grid search\n",
    "best_config, best_model_state, best_val_loss = tune_hyperparameter(n_features=X_tr_tensor.shape[1], train_loader, val_loader, device, hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
