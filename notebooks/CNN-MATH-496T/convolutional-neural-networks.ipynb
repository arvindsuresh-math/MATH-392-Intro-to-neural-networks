{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline for CNNs notebook\n",
    "\n",
    "1. Recall general architecture of MLPs\n",
    "2. High-level overview of CNN architecture\n",
    "    - Input layer\n",
    "    - Convolutional layer\n",
    "    - Pooling layer\n",
    "    - Fully connected layer\n",
    "3. Convolutions\n",
    "    - What kind of data do we use convolutions on? (images, time series, ordered data in general)\n",
    "    - What is a convolution good for?\n",
    "        - Finite differences to approximate derivatives in time series\n",
    "        - Moving average to smooth out noise\n",
    "        - Convolution with dirac delta to extract a part of a signal\n",
    "        - Convolution of probability distributions\n",
    "        - Edge detection in images\n",
    "        - Feature extraction in general by convolution with a filter/kernel\n",
    "4. Convolutional layer\n",
    "    - Convolution operation\n",
    "    - Stride\n",
    "    - Padding\n",
    "    - Number of filters\n",
    "    - Activation function\n",
    "5. Pooling layer\n",
    "    - Max pooling\n",
    "    - Average pooling\n",
    "    - Global average pooling\n",
    "6. Fully connected layer\n",
    "7. CNNs in practice\n",
    "    - Example of a CNN architecture\n",
    "    - Example of a CNN training\n",
    "    - Example of a CNN application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Recall general architecture of MLPs\n",
    "\n",
    "In the previous lecture, we explored the XOR problem and discovered why simple perceptrons are insufficient for solving it, necessitating the use of multilayer perceptrons (MLPs). Let's recall the general architecture of an MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image('Neural networks - Lamarr Institute.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram above shows a Multilayer Perceptron with:\n",
    "- Input layer: 2 neurons ($x_1$, $x_2$)\n",
    "- Hidden layer 1: 4 neurons\n",
    "- Hidden layer 2: 5 neurons\n",
    "- Output layer: 3 neurons\n",
    "\n",
    "Each connection between neurons represents a weight parameter that the network learns during training. The thickness of the arrows represents the magnitude of the weights (analogous to how synapses have different strengths in biological neural networks).\n",
    "\n",
    "The MLP processes data by passing it through these layers sequentially:\n",
    "- From each layer, the data is first multiplied by the weights and aggregated to form a bunch of scores (weighted sums).\n",
    "- Then, the scores are passed through an activation function (like ReLU, Sigmoid, or Tanh) to introduce non-linearity. In fact, the picture above visualizes a sigmoidal activation function, such as the logistic function $$\\sigma(z) = \\frac{1}{1 + e^{-z}},$$ or the hyperbolic tangent function $$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}.$$\n",
    "- The two operations above together constitute a single layer of the network. \n",
    "\n",
    "The final layer above is the output layer, which produces the final predictions of the network. For example, if we are using the neural network for a multi-class classification task with $3$ classes, the output layer will have $3$ neurons, each representing the probability of the input belonging to a particular class, and these probabilities would be obtained by taking the last bunch of scores and passing them through a softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU activation function\n",
    "The power and flexibility of neural networks comes in large part from the non-linear activation functions. Note that $\\sigma$ has range $(0, 1)$ and $\\tanh$ has range $(-1, 1)$. \n",
    "\n",
    "A very popular activation function (of a different flavor from the previous two) is the Rectified Linear Unit (ReLU), which is defined as $$\\text{ReLU}(z) = \\max(0, z).$$\n",
    "The ReLU function is computationally efficient and has been shown to work well in practice; it is used in many state-of-the-art neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# python let's you define functions in a very simple way:\n",
    "def reLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define a list of functions and names\n",
    "activations = [('reLU', reLU), \n",
    "               ('logistic', logistic), \n",
    "               ('tanh', np.tanh)]\n",
    "\n",
    "# Define the input range\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "# loop over the list for plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(8, 2))\n",
    "for i, (name, f) in enumerate(activations):\n",
    "    axes[i].plot(x, f(x), c='darkred')\n",
    "    axes[i].set_title(f'{name}')\n",
    "    axes[i].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. High-level overview of CNN architecture\n",
    "Suppose we are working with an image classification task. Thus, we want to feed in an image as input and obtain a class label as output. (For this, we will have a pre-defined list of class labels, such as \"cat\", \"dog\", \"car\", etc.)\n",
    "\n",
    "In such a scenario, the architecture of a CNN would look something like this:\n",
    "1. **Input layer**: The input to the network is an image, stored as a tensor of shape `(height, width, channels)`. \n",
    "    - For color images, we typically have `channels = 3`, for Red, Green, and Blue (RGB). \n",
    "    - Alternatively, grayscale images have `channel = 1`, which represents pixel intensity.\n",
    "2. **Convolutional layer**: The convolutional layer is the core building block of a CNN. \n",
    "    - It applies a set of *filters* to the input image, each of which detects a particular \"feature\" (e.g. edges, texture). \n",
    "    - These filters are defined by weights, which are learned during model training. \n",
    "    - The outputs of the convolutional layer are called *feature maps*; they are tensors with the same spatial dimension, but with one channel for each filter. \n",
    "    - These feature maps are then passed through an activation function (like ReLU) to introduce non-linearity.\n",
    "    - **Fact**: The act of \"applying\" the filter is a linear operation called convolution, and convolutional layers can in fact be understood as usual fully connected layers, but with a particular weight structure in which most weights are zero, and the non-zero weights represent the filter values.\n",
    "3. **Pooling layer**: Every convolutional layer is followed by a pooling layer to reduce model complexity and guard and overfitting.\n",
    "    - It reduces the spatial dimensions of the incoming feature maps, resulting in a smaller representation of the learned features. \n",
    "    - The pooling operation involves applying a simple function (like max or average) to sub-regions of the feature map. In particular, there are no weights to learn in a pooling layer.\n",
    "    - The output of the pooling layer is a \"downsampled\" version of the input feature map, with reduced spatial dimensions but the same number of channels as the input.\n",
    "- **Fully connected layer**: After several convolutional and pooling layers, the final output of the network is typically fed through a multi-layer perceptron.\n",
    "    - It takes the output of the last pooling layer and flattens it into a 1D array.\n",
    "    - This 1D array is then passed through an MLP in which each layer is fully connected to the next. Of course, these connections have weights that are learned during training.\n",
    "    - Applying a softmax function to the output of the MLP gives the final class probabilities.\n",
    "\n",
    "Given below is depiction of the CNN architecture (with only one convolutional layer and one pooling layer) applied to an image classification task. Note that the convolutional and pooling layers have the effect of converting the raw input image tensor into another tensor that has somehow absorbed the important features of the image. This tensor is then passed through a fully connected layer to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('CNN-architecture-Zilliz.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutions\n",
    "\n",
    "Convolutions are a particular type of mathematics operation performed on two functions $f$ and $g$ (from some set to another set), to produce a third function $f*g$ (with possibly different domain from $f$ and $g$). \n",
    "\n",
    "To understand convolutions, I want to work with a few different classes of functions to see how convolutions work in each case:\n",
    "- Functions from a finite set of time steps to $\\mathbb{R}$; these are time series (example stock prices).\n",
    "- Functions from $\\mathbb{R}$ to $\\mathbb{R}$.\n",
    "- Functions of the form $f: \\{ \\textup{ Pixel grid } \\} \\to [0,1]$; these are (grayscale) images, where the output represents pixel intesity (normalized to lie in $[0,1]$)!\n",
    "\n",
    "Note that in each of the above cases, there is a natural arrangement or ordering of the points in the domain which allows us to ask questions about how the function behaves \"near\" a point. To answer these questions, we often resort to convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dot products to get behavior near a point\n",
    "The basic idea is as follows.\n",
    "- Start with the vector or function $f$ that you want to understand, and the point $t$ in the domain where you want to understand some local behavior.\n",
    "- Consider a small window of points around $t$, and choose values for all points in this window. Here, small is meant relative to the domain of $f$.\n",
    "- Extend your selection of values into a vector or function $g$ of the same shape as $f$ by setting all other values to $0$.\n",
    "- Take the dot product of $f$ and $g$ (which we here denote by $\\langle f, g \\rangle$) to get a single number that represents the local behavior of $f$ near $t$. The meaning of \"dot product\" depends on the type of function $f$ and $g$:\n",
    "    - For time series (i.e. sequences of real numbers that are eventually $0$), the dot product is the sum of the products of the corresponding values of $f$ and $g$:\n",
    "    \\begin{equation*}\n",
    "        \\langle f , g \\rangle = \\sum_{i = 1}^{\\infty} f_i g_i.\n",
    "    \\end{equation*}\n",
    "    - For functions from $\\mathbb{R}$ to $\\mathbb{R}$, the dot product is the integral of the product of the two functions:\n",
    "    \\begin{equation*}\n",
    "        \\langle f , g \\rangle = \\int_{-\\infty}^{\\infty} f(x) g(x) \\, dx.\n",
    "    \\end{equation*}\n",
    "    - For functions defined on a pixel grid, the dot product is again the sum of the products over the pixels:\n",
    "    \\begin{equation*}\n",
    "        \\langle f , g \\rangle = \\sum_{\\textup{pixels}} f_{\\textup{pixel}} g_{\\textup{pixel}}.\n",
    "    \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Convolutions as a family of dot products\n",
    "The above situation does not yet describe a convolution, because it only gives us the behavior of $f$ near a single point $t$ in the form of a scalar. Note that the window of values chosen by us is located near $t$ (often-times, it is centered at $t$), and $g$ is zero outside this window. \n",
    "\n",
    "The idea now is that we can repeat the above process for every point $t$ in the domain by sliding the window along the domain so that it is centered at $t$. \n",
    "For example:\n",
    "- In the time series case, we can slide the window by $x$, which amounts to replacing $g(t)$ by $g(t - x)$. Thus, the dot product at $t$ (which is the output of the convolution at $t$) is:\n",
    "\\begin{equation*}\n",
    "    (f * g)(t) = \\sum_{i = 1}^{\\infty} f_i g_{t-i}.\n",
    "\\end{equation*}\n",
    "\n",
    "- In the case of functions from $\\mathbb{R}$ to $\\mathbb{R}$, we can slide the window by $x$, which amounts to replacing $g(t)$ by $g(t - x)$. Thus, the dot product at $t$ (which is the output of the convolution at $t$) is:\n",
    "\\begin{equation*}\n",
    "    (f * g)(t) = \\int_{-\\infty}^{\\infty} f(x) g(t - x) \\, dx.\n",
    "\\end{equation*}\n",
    "- In the case of functions defined on a pixel grid, sliding the window by a vector $\\mathbf{v} = (x,y)$ amounts to replacing $g(\\mathbf{t})$ by $g_{\\mathbf{t}- \\mathbf{v}}$ (NOTE: here, $\\mathbf{t}$ is a 2d-vector representing the pixel position in the array, so the formula makes sense). The dot product in this case is a sum over the pixels:\n",
    "\\begin{equation*}\n",
    "    (f * g)(\\mathbf{t}) = \\sum_{\\mathbf{t} \\in \\{\\textup{pixels}\\}} f(\\mathbf{v}) g(\\mathbf{t} - \\mathbf{v}).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples with one-dimensional data\n",
    "\n",
    "#### Example 1: Moving averages of stock prices\n",
    "\n",
    "We can visualize the NVDA stock as a **time series**, which basically an ordered list of real numbers indexed by date. Equivalently, you can think of the stock data as a function from the \"dateline\" to $\\mathbb{R}$. \n",
    "\n",
    "Here is a visualization of the stock price from 2014 to 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nvda_stock = pd.read_csv('NVDA_stock.csv')\n",
    "# use the date as the index\n",
    "nvda_stock['date'] = pd.to_datetime(nvda_stock['date'])\n",
    "nvda_stock.set_index('date', inplace=True)\n",
    "# plot close price of NVDA as a time series\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(nvda_stock['close'], c='darkred', linewidth=0.8)\n",
    "plt.title('Nvidia stock price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to get rid of the small variations in the stock price and only keep the long-term trends. One way to replace each stock price by its average over the past $n$ days. That is, we seek a new function $f_n(t)$ whose value at $t$ equals the average stock price over the interval $[t-n, t]$.\n",
    "\n",
    "We can accomplish this by applying the **$n$-day moving average** filter:\n",
    "\\begin{equation*}\n",
    "    g_n(x) = \\begin{cases}\n",
    "        \\frac{1}{n} & \\text{if } 0 \\leq x < n, \\\\\n",
    "        0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "That is, $g_n$ is a function that is $1/n$ on the interval $[0, n)$ and $0$ elsewhere. It follows that for any $t$, the translated function $g_n(t-x)$ is $1/n$ on the interval $[t-n, t]$ and $0$ elsewhere. Thus, for any $t$ and any date $i$, we have:\n",
    "\\begin{equation*}\n",
    "    f(i)g_n(t-i) = \\begin{cases}\n",
    "        \\frac{f(i)}{n} & \\text{if } t-n \\leq i \\leq t, \\\\\n",
    "        0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "Summing over all $i$ gives the $n$-day moving average of the stock price at date $t$, which is the output of the convolution at $t$:\n",
    "\\begin{align*}\n",
    "    (f * g_n)(t) & = \\sum_{i = 0}^{n-1} f(i)g_n(t-i) \\\\\n",
    "    & = \\frac{f(t) + f(t-1) + \\dotsb + f(t-n+1)}{n}.\n",
    "\\end{align*}\n",
    "\n",
    "Let's illustrate this by taking moving averages of NVDA in the years 2022 and 2023. You will notice that longer time windows smooth out the stock price more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 10-day moving average and 50-day moving average\n",
    "nvda_stock['10d'] = nvda_stock['close'].rolling(10).mean()\n",
    "nvda_stock['50d'] = nvda_stock['close'].rolling(50).mean()\n",
    "nvda_stock['100d'] = nvda_stock['close'].rolling(100).mean()\n",
    "\n",
    "# plot close, 10d, 50d, and 100d moving averages for the year 2023\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(nvda_stock['close']['2023'], c='darkred', linewidth=0.8, label='close')\n",
    "plt.plot(nvda_stock['10d']['2023'], c='blue', linewidth=0.8, label='10d')\n",
    "plt.plot(nvda_stock['50d']['2023'], c='green', linewidth=0.8, label='50d')\n",
    "plt.plot(nvda_stock['100d']['2023'], c='orange', linewidth=0.8, label='100d')\n",
    "plt.title('Nvidia stock price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Differencing time series\n",
    "Another very natural thing we might wish to do is to compute the change in stock price from one day to the next. This is called **differencing** the time series. To accomplish this, we apply the **difference filter**:\n",
    "\\begin{equation*}\n",
    "    g(x) = \\begin{cases}\n",
    "        1 & \\text{if } x = 0, \\\\\n",
    "        -1 & \\text{if } x = 1, \\\\\n",
    "        0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "It follows that for any $t$ and date $i$, we have:\n",
    "\\begin{equation*}\n",
    "    f(i)g(t-i) = \\begin{cases}\n",
    "        f(t) & \\text{if } i = t, \\\\\n",
    "        -f(t-1) & \\text{if } i = t-1, \\\\\n",
    "        0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "Summing over all $i$ gives the difference in stock price at date $t$, which is the output of the convolution at $t:\n",
    "\\begin{align*}\n",
    "    (f * g)(t) & = \\sum_{i = 0}^{1} f(i)g(t-i) \\\\\n",
    "    & = f(t) - f(t-1).\n",
    "\\end{align*}\n",
    "\n",
    "Let's illustrate this by differencing the stock price of NVDA in the year 2023. You will notice that the differenced stock price is much more volatile than the original stock price; the moral of the story is that even if there is a clear overall increasing trend, the stock price (hence, the returns) can still fluctuate wildly from day to day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the daily returns\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(nvda_stock['close']['2023'] - nvda_stock['close']['2023'].shift(1), c='darkred', linewidth=0.8)\n",
    "plt.title('Nvidia stock daily returns')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Convolution with a dirac delta\n",
    "Suppose $f : \\mathbb{R} \\to \\mathbb{R}$ is an integrable function with anti-derivative $F$. For every $\\epsilon > 0$, consider the function $g_{\\epsilon}$ defined by:\n",
    "\\begin{equation*}\n",
    "    g_{\\epsilon}(x) = \\begin{cases}\n",
    "        \\frac{1}{2\\epsilon} & \\text{if } -\\epsilon \\leq x \\leq \\epsilon, \\\\\n",
    "        0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "Note that $g_{\\epsilon}$ can be thought of as the probability density function of a random variable that is uniformly distributed on the interval $[-\\epsilon, \\epsilon]$, since\n",
    "\\begin{equation*}\n",
    "    \\int_{-\\infty}^{\\infty} g_{\\epsilon}(x) \\, dx = \\frac{1}{2\\epsilon} \\int_{-\\epsilon}^{\\epsilon} \\, dx = 1.\n",
    "\\end{equation*}\n",
    "More generally, for any $t$, the translated function $g_{\\epsilon}(t-x)$ is $1/(2\\epsilon)$ on the interval $[t-\\epsilon, t+\\epsilon]$ and $0$ elsewhere, so that\n",
    "\\begin{equation*}\n",
    "    \\int_{-\\infty}^{\\infty} g_{\\epsilon}(t-x) \\, dx = \\frac{1}{2\\epsilon} \\int_{t-\\epsilon}^{t+\\epsilon} \\, dx = 1.\n",
    "\\end{equation*}\n",
    "The convolution of $f$ with $g_{\\epsilon}$ is then:\n",
    "\\begin{align*}\n",
    "    (f * g_{\\epsilon})(t) & = \\int_{-\\infty}^{\\infty} f(x)g_{\\epsilon}(t-x) \\, dx \\\\\n",
    "    & = \\frac{1}{2\\epsilon} \\int_{t-\\epsilon}^{t+\\epsilon} f(x) \\, dx \\\\\n",
    "    & = \\frac{ F(t+\\epsilon) - F(t-\\epsilon) }{2\\epsilon}.\n",
    "\\end{align*}\n",
    "This is a difference quotient! As $\\epsilon \\to 0$, the difference quotient converges to the derivative of $F$ at $t$. By the Fundamental Theorem of Calculus, this is $f(t)$, i.e.\n",
    "\\begin{equation*}\n",
    "    \\lim_{\\epsilon \\to 0} (f * g_{\\epsilon})(t) = \\lim_{\\epsilon \\to 0} \\frac{ F(t+\\epsilon) - F(t-\\epsilon) }{2\\epsilon} = f(t).\n",
    "\\end{equation*}\n",
    "The **Dirac Delta function** $\\delta$ is born out of the desire to pass the limit to the function $g_{\\epsilon}$ insides the convolution, so that one could then write\n",
    "\\begin{equation*}\n",
    "    f(t) = (f * \\delta)(t) = \\int_{-\\infty}^{\\infty} f(x) \\delta(t-x) \\, dx.\n",
    "\\end{equation*}\n",
    "Unfortunately, this doesn't directly work, simply by noting that as $\\epsilon \\to 0$, the limit of $g_{\\epsilon}$ would look like:\n",
    "\\begin{equation*}\n",
    "    \\lim_{\\epsilon \\to 0} g_{\\epsilon}(x) = \\begin{cases}\n",
    "        \\infty & \\text{if } x = 0, \\\\\n",
    "        0 & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "Well, a lot of folks are okay with this definition (plus the additional assumption that the integral over the whole real line is $1$), but it's clearly not a function. There are two ways to resolve this:\n",
    "- View it as a \"distribution\" and not a function.\n",
    "- Drop the function altogether and define a suitable *Dirac measure* on the real line, which assigns a measure of $1$ to any set containing $0$, and $0$ otherwise. \n",
    "\n",
    "**Remark**: The Dirac Delta is famous because of its role in physics. However, it also serves a fundamental purpose from a more abstract perspective. Namely, there are several natural spaces of functions that are closed under the convolution operation. In fact, convolution behaves just like multiplication on these spaces (associative, distributive, commutative, etc.) There's just one problem... these spaces may not have an identity element for this \"multiplication\" operation! Thus, the The Dirac Delta serves as the identity element for the convolution operation on these spaces (even if it's not actually a function, lol)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4: Convolution of probability distributions\n",
    "I won't say much about this, simply mention that if $X$ and $Y$ are two random variables with probability density functions $f$ and $g$, then the probability density function of $X + Y$ is the convolution of $f$ and $g$:\n",
    "\\begin{equation*}\n",
    "    f_{X+Y}(x) = (f * g)(x) = \\int_{-\\infty}^{\\infty} f(x-y)g(y) \\, dy.\n",
    "\\end{equation*}\n",
    "This is a fundamental result in probability theory. \n",
    "\n",
    "Watch the following masterpiece of a video by 3Blue1Brown for an incredibly clear explanation: https://www.youtube.com/watch?v=IaSGqQa5O-M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples with images\n",
    "\n",
    "#### Representing and working with images\n",
    "Suppose we have now a grayscale image $I$ of `width = n` and `height = m`. In terms of data, $I$ can be understood then as the data of a collection of pixel intensities $I(i,j) \\in [0,1]$, where $i$ is in `range(m)` and $j$ is in `range(n)`. In other words, giving an $m \\times n$ grayscale is equivalent to giving a function \n",
    "\\begin{equation*}\n",
    "    I : \\{0, 1, \\dotsc, m-1\\} \\times \\{0, 1, \\dotsc, n-1\\} \\to [0,1].\n",
    "\\end{equation*}\n",
    "\n",
    "In PyTorch, an image like the one above would typically be represented as a tensor of shape `(channels, height, width)`. \n",
    "- For grayscale images, `channels = 1`, and for RGB images, `channels = 3`.\n",
    "- In our notation above, `height` corresponds to `m` and `width` corresponds to `n`.\n",
    "\n",
    "Let's recall how to load and display images in PyTorch, using the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a simple transform to convert images to tensors.\n",
    "# Thie transforms the initial data (images, with pixel values from 0 to 255) to tensors (with pixel values from 0 to 1).\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load MNIST from the local 'data' folder with download=False.\n",
    "mnist_data = MNIST(root='data', train=True, download=False, transform=transform)\n",
    "\n",
    "# Create a DataLoader to sample a few images.\n",
    "data_loader = DataLoader(mnist_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch of images.\n",
    "images, labels = next(iter(data_loader))\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(str(labels[i].item()))\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the image sizes and what they represent\n",
    "img = images[0]\n",
    "print(img)\n",
    "print(f'Image shape: {img.shape}')\n",
    "print(f'Channels: {img.shape[0]}')\n",
    "print(f'Height: {img.shape[1]}')\n",
    "print(f'Width: {img.shape[2]}')\n",
    "\n",
    "#display parts of the image\n",
    "fig, ax = plt.subplots(1, 4, figsize=(8, 2))\n",
    "# top half\n",
    "ax[0].imshow(img.squeeze().numpy()[:14, :], cmap='gray')\n",
    "ax[0].set_title('Top half')\n",
    "# left half\n",
    "ax[1].imshow(img.squeeze().numpy()[:, :14], cmap='gray')\n",
    "ax[1].set_title('Left half')\n",
    "# bottom right quadrant\n",
    "ax[2].imshow(img.squeeze().numpy()[14:, 14:], cmap='gray')\n",
    "ax[2].set_title('Bottom right quadrant')\n",
    "# top right quadrant\n",
    "ax[3].imshow(img.squeeze().numpy()[:14, 14:], cmap='gray')\n",
    "ax[3].set_title('Top right quadrant')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Converting color images to grayscale\n",
    "One way to convert a color image to grayscale is to take the average of the pixel intensities across the three color channels (Red, Green, Blue). This can be done by applying an **averaging filter**:\n",
    "\\begin{equation*}\n",
    "    K = \\frac{1}{3} \\begin{bmatrix}\n",
    "        1 & 1 & 1 \\\\\n",
    "        1 & 1 & 1 \\\\\n",
    "        1 & 1 & 1\n",
    "    \\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "The output of the convolution of this filter with the color image is a grayscale image, where each pixel intensity is the average of the pixel intensities across the three color channels.\n",
    "\n",
    "Let's demonstrate this below by converting a color image to grayscale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "# Open the images using PIL instead of IPython.display.Image\n",
    "murphy = PILImage.open(\"murphy.jpeg\")\n",
    "anbu = PILImage.open(\"anbu.jpeg\")\n",
    "\n",
    "# Convert the images to tensors\n",
    "murphy_tensor = transforms.ToTensor()(murphy)\n",
    "anbu_tensor = transforms.ToTensor()(anbu)\n",
    "\n",
    "# Display the tensor shapes\n",
    "print(f'Before resizing:')\n",
    "print(f'Murphy tensor shape: {murphy_tensor.shape}')\n",
    "print(f'Anbu tensor shape: {anbu_tensor.shape}')\n",
    "\n",
    "# Reshape them into 256x256 images for convenience\n",
    "murphy_smol = F.interpolate(murphy_tensor.unsqueeze(0), size=256)\n",
    "anbu_smol = F.interpolate(anbu_tensor.unsqueeze(0), size=256)\n",
    "\n",
    "# Display the tensor shapes\n",
    "print(f'After resizing:')\n",
    "print(f'Murphy tensor shape: {murphy_smol.shape}')\n",
    "print(f'Anbu tensor shape: {anbu_smol.shape}')\n",
    "\n",
    "# Display the images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(murphy_smol.squeeze().numpy().transpose(1, 2, 0))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Murphy')\n",
    "axes[1].imshow(anbu_smol.squeeze().numpy().transpose(1, 2, 0))\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Anbu')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply the averaging filter to convert the color image to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply averaging filter to color channels\n",
    "# Define the filter\n",
    "avg_filter = torch.ones(1, 3, 1, 1) / 3\n",
    "\n",
    "# Apply the filter to the images\n",
    "murphy_grayscale = F.conv2d(murphy_smol, avg_filter)\n",
    "anbu_grayscale = F.conv2d(anbu_smol, avg_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images along with their grayscale versions\n",
    "fig, ax = plt.subplots(2, 2, figsize=(6,6))\n",
    "ax = ax.flatten()\n",
    "ax[0].imshow(murphy_smol.squeeze().permute(1, 2, 0))\n",
    "ax[0].set_title('Murphy')\n",
    "ax[1].imshow(murphy_grayscale.squeeze(), cmap='gray')\n",
    "ax[1].set_title('Murphy grayscale')\n",
    "ax[2].imshow(anbu_smol.squeeze().permute(1, 2, 0))\n",
    "ax[2].set_title('Anbu')\n",
    "ax[3].imshow(anbu_grayscale.squeeze(), cmap='gray')\n",
    "ax[3].set_title('Anbu grayscale')\n",
    "# remove all grids and axes\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Edge detection in images\n",
    "One of the most famous applications of convolutions in image processing is edge detection. The idea is to apply a filter to the image that highlights the edges in the image.  We can do this by applying a 3x3 filter that looks like this (here, applying means we slide the filter around the image and compute the dot product at each position).\n",
    "\n",
    "Since edges are regions where the pixel intensity changes rapidly, we might expect to find an edge when the **difference** in pixel intensity between nearby pixels is large. Thus, we can use the **difference filter** to capture (say) large changes in pixel intensity along the vertical dimension, which would correspond to horizontal edges in the image. \n",
    "\n",
    "In general, one computes a filter $K_v$ to detect vertical edges and a filter $K_h$ to detect horizontal edges.\n",
    "There are a few different popular choices:\n",
    "1. The **Prewitt filter**: They are defined as:\n",
    "\\begin{equation*}\n",
    "    K_v = \\begin{bmatrix}\n",
    "        -1 & 0 & 1 \\\\\n",
    "        -1 & 0 & 1 \\\\\n",
    "        -1 & 0 & 1\n",
    "    \\end{bmatrix}, \\quad\n",
    "    K_h = \\begin{bmatrix}\n",
    "        -1 & -1 & -1 \\\\\n",
    "        0 & 0 & 0 \\\\\n",
    "        1 & 1 & 1\n",
    "    \\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "2. The **Sobel filter**: They are defined as:\n",
    "\\begin{equation*}\n",
    "    K_v = \\begin{bmatrix}\n",
    "        -1 & 0 & 1 \\\\\n",
    "        -2 & 0 & 2 \\\\\n",
    "        -1 & 0 & 1\n",
    "    \\end{bmatrix}, \\quad\n",
    "    K_h = \\begin{bmatrix}\n",
    "        -1 & -2 & -1 \\\\\n",
    "        0 & 0 & 0 \\\\\n",
    "        1 & 2 & 1\n",
    "    \\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "\n",
    "NOTE: In both these example, note that the the filter for detecting vertical edges $K_v$ looks like a row-wise differencing operator, while the filter for detecting horizontal edges $K_h$ looks like a column-wise differencing operator. Thus, they are able to detect pixel intensity changes in the vertical and horizontal directions, respectively!\n",
    "\n",
    "Let's apply the Sobel filters to the grayscale images of Murphy and Anbu to detect edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sobel filter for vertical edge detection\n",
    "sobel_filter_v = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "# Define the Sobel filter for horizontal edge detection\n",
    "sobel_filter_h = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n",
    "\n",
    "# Reshape the filters to match the expected shape for PyTorch's conv2d\n",
    "sobel_filter_v = sobel_filter_v.view(1, 1, 3, 3)\n",
    "sobel_filter_h = sobel_filter_h.view(1, 1, 3, 3)\n",
    "\n",
    "# Apply vertical-edge detection filter\n",
    "murphy_v = F.conv2d(murphy_grayscale, sobel_filter_v)\n",
    "anbu_v = F.conv2d(anbu_grayscale, sobel_filter_v)\n",
    "\n",
    "# Apply horizontal-edge detection filter\n",
    "murphy_h = F.conv2d(murphy_grayscale, sobel_filter_h)\n",
    "anbu_h = F.conv2d(anbu_grayscale, sobel_filter_h)\n",
    "\n",
    "# Compute the magnitude of the gradient; this is another image!\n",
    "murphy_vh = torch.sqrt(murphy_v**2 + murphy_h**2)\n",
    "anbu_vh = torch.sqrt(anbu_v**2 + anbu_h**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original image and the convolution result (4 in a row)\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12,6))\n",
    "\n",
    "for i, (img, title) in enumerate(zip([murphy_grayscale, murphy_v, murphy_h, murphy_vh], ['Murphy', 'Vertical edges', 'Horizontal edges', 'Gradient magnitude'])):\n",
    "    ax[0, i].imshow(img.squeeze().detach().numpy(), cmap='gray')\n",
    "    ax[0, i].set_title(title)\n",
    "    ax[0, i].axis('off')\n",
    "for i, (img, title) in enumerate(zip([anbu_grayscale, anbu_v, anbu_h, anbu_vh], ['Anbu', 'Vertical edges', 'Horizontal edges', 'Gradient magnitude'])):\n",
    "    ax[1, i].imshow(img.squeeze().detach().numpy(), cmap='gray')\n",
    "    ax[1, i].set_title(title)\n",
    "    ax[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "A key component in the architecture of a CNN is the *pooling layer*, which is used to reduce the spatial dimensions of the input image while retaining the important features. There are two specifications the go into defining the pooling operation:\n",
    "1. **Size of the pooling filter**: This is the size of the sub-region of the image that the pooling operation is applied to. Common choices are 2x2 or 3x3. Each region of this shape will be compressed into a single pixel in the output image. One way to execute this is to average out the pixel intensities in the sub-region, which is called **average pooling**. The other (more common) method is to take the maximum pixel intensity in the sub-region, which is called **max pooling**. In fact, this operation is not linear, so it is not actually a convolution operation!\n",
    "2. **Stride**: This is the number of pixels by which the pooling filter is moved across the image. A stride of 1 means that the filter is moved one pixel at a time, while a stride of 2 means that the filter is moved two pixels at a time. For example, suppose we use a 2x2 pooling filter with stride 2. Then, the first sub-region of the image that the filter is applied to is the top-left 2x2 square of pixels. Then, we slide it two pixels to the right to get the next 2x2 square of pixels, and so on.\n",
    "\n",
    "**Remark**: When applying convolutional layers to images, it is common to use a stride of 1, which means that the filter is moved one pixel at a time. Note that if a stride of $k$ is used, then the output image will have dimensions that are $\\frac{1}{k}$ times the dimensions of the input image.\n",
    "\n",
    "Let's implement this below by down-sampling the grayscale images of Murphy and Anbu using max pooling with a 2x2 filter and stride 2. We use the original large images for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the filter to the images\n",
    "# murphy_grayscale = F.conv2d(murphy_tensor, avg_filter)\n",
    "# anbu_grayscale = F.conv2d(anbu_tensor, avg_filter)\n",
    "\n",
    "# Apply max pooling to the images; second argument is the stride\n",
    "murphy_pool = F.max_pool2d(murphy_tensor, kernel_size=2, stride= 2)\n",
    "anbu_pool = F.max_pool2d(anbu_tensor, kernel_size=2, stride= 2)\n",
    "\n",
    "# Pool again to reduce the size further\n",
    "murphy_pool2 = F.max_pool2d(murphy_pool, kernel_size=2, stride= 2)\n",
    "anbu_pool2 = F.max_pool2d(anbu_pool, kernel_size=2, stride= 2)\n",
    "\n",
    "# And one last time\n",
    "murphy_pool3 = F.max_pool2d(murphy_pool2, kernel_size=2, stride= 2)\n",
    "anbu_pool3 = F.max_pool2d(anbu_pool2, kernel_size=2, stride= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the original grayscale images along with the pooled images (4 per row)\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12,6))\n",
    "\n",
    "for i, (img, title) in enumerate(zip([murphy_tensor, murphy_pool, murphy_pool2, murphy_pool3],\n",
    "                                      ['Murphy', 'Pooled 1', 'Pooled 2', 'Pooled 3'])):\n",
    "    im = img.squeeze()\n",
    "    ax[0, i].imshow(im.permute(1, 2, 0))\n",
    "    shape = img.shape  # expecting shape: [N, C, H, W] or [1, H, W]\n",
    "    ax[0, i].set_title(f\"{title} ({shape[-2]}, {shape[-1]})\")\n",
    "    ax[0, i].axis('off')\n",
    "for i, (img, title) in enumerate(zip([anbu_tensor, anbu_pool, anbu_pool2, anbu_pool3],\n",
    "                                      ['Anbu', 'Pooled 1', 'Pooled 2', 'Pooled 3'])):\n",
    "    im = img.squeeze()\n",
    "    ax[1, i].imshow(im.permute(1, 2, 0))\n",
    "    shape = img.shape\n",
    "    ax[1, i].set_title(f\"{title} ({shape[-2]}, {shape[-1]})\")\n",
    "    ax[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above images exhibit the powerful feature of max pooling: it retains the important features of the image while reducing the spatial dimensions. This is crucial for reducing model complexity and guarding against overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with edges and corners\n",
    "One of the main issues with convolutions is that they can't be applied to the edges of the image, since the filter would \"hang off\" the edge. There are a few ways to deal with this:\n",
    "1. **Valid padding**: In this case, the filter is only applied to the parts of the image where the filter fits entirely. This means that the output image will have smaller dimensions than the input image. This is the default behavior in PyTorch.\n",
    "2. **Same padding**: In this case, the filter is applied to the entire image, and the missing parts are filled in with zeros. (So, we extend the image with zeroes till infinity and beyond.) \n",
    "This means that the output image will have the same dimensions as the input image, which is useful when we want to retain the spatial dimensions of the image.\n",
    "3. **Reflect padding**: In this case, the filter is applied to the entire image, and the missing parts are filled in by reflecting the image at the edges. This is useful when we want to avoid the \"black border\" that comes from zero padding.\n",
    "4. **Circular padding**: In this case, the filter is applied to the entire image, and the missing parts are filled in by wrapping the image around itself. This is useful when we want to avoid the \"black border\" that comes from zero padding.\n",
    "\n",
    "Let's quickly illustrate all four types of padding on the grayscale image of Murphy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a vertical Prewitt filter (for vertical edge detection)\n",
    "prewitt_filter = torch.tensor([[1, 0, -1],\n",
    "                               [1, 0, -1],\n",
    "                               [1, 0, -1]], dtype=torch.float32)\n",
    "# Reshape to [out_channels, in_channels, height, width]\n",
    "prewitt_filter = prewitt_filter.view(1, 1, 3, 3)\n",
    "\n",
    "# Define different padding modes to visualize\n",
    "padding_modes = {\n",
    "    'constant': 'constant',    # zero padding\n",
    "    'reflect': 'reflect',\n",
    "    'replicate': 'replicate',\n",
    "    'circular': 'circular'\n",
    "}\n",
    "\n",
    "# Create a figure to display the results\n",
    "fig, axes = plt.subplots(1, len(padding_modes) + 1, figsize=(15, 3))\n",
    "\n",
    "# Without any padding (normal convolution)\n",
    "conv_no_pad = F.conv2d(murphy_grayscale, prewitt_filter, padding=0)\n",
    "axes[0].imshow(conv_no_pad.squeeze().detach().numpy(), cmap='gray')\n",
    "axes[0].set_title(f'No Padding ({conv_no_pad.shape[-2]}x{conv_no_pad.shape[-1]})')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Loop over each padding mode. For a 3x3 filter, adding a padding of 1 yields \"same\" convolution.\n",
    "for ax, (mode_label, mode) in zip(axes[1:], padding_modes.items()):\n",
    "    \n",
    "    # Pad the grayscale image using the current padding mode.\n",
    "    # Here, pad=(1, 1, 1, 1) adds 1 pixel of padding to each side,\n",
    "    # ensuring that a 3x3 convolution produces an output of the same spatial size.\n",
    "    padded_img = F.pad(murphy_grayscale, pad=(1, 1, 1, 1), mode=mode)\n",
    "    \n",
    "    # Apply the vertical Prewitt filter on the padded image.\n",
    "    # Note: padding=0 in conv2d because we've already padded the image manually.\n",
    "    conv_padded = F.conv2d(padded_img, prewitt_filter, padding=0)\n",
    "    ax.imshow(conv_padded.squeeze().detach().numpy(), cmap='gray')\n",
    "    ax.set_title(f\"{mode_label.capitalize()} ({conv_padded.shape[-2]}x{conv_padded.shape[-1]})\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting hand-written digits in the MNIST dataset\n",
    "Now that we have a good understanding of convolutions and pooling, let's apply this knowledge to a real-world problem: detecting hand-written digits in the MNIST dataset. The MNIST dataset consists of 28x28 grayscale images of hand-written digits from 0 to 9. The task is to classify each image into one of the 10 classes (0 to 9).\n",
    "\n",
    "Before any CNN business, we need to process the images.\n",
    "\n",
    "1. MNIST images are provided as 28×28 pixel grayscale images. Their initial shape is `(height, width) = (28, 28)`.\n",
    "2. After `transforms.ToTensor()`, the shape of the image becomes `(C, H, W) = (1, 28, 28)`. This is because the `ToTensor()` transform converts the image to a PyTorch tensor and also scales the pixel values to lie in the range `[0, 1]`.\n",
    "3. After `transforms.Normalize((0.1307,), (0.3081,))`, the pixel values are normalized by subtracting the mean `0.1307` and dividing by the standard deviation `0.3081`. This is a common normalization step in deep learning, as it helps the model learn better. However, this operation does not change the shape of the tensor. The final processed image remains with shape `(1, 28, 28)`.\n",
    "4. The MNIST dataset is split into two parts: a training set and a test set. The training set is used to train the model, while the test set is used to evaluate the model's performance on unseen data. The training set contains 60,000 images, while the test set contains 10,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pre-processing steps for the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert images to tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # normalize the dataset\n",
    "])\n",
    "\n",
    "# Load MNIST train and test sets from the data folder\n",
    "mnist_train = MNIST(root='data', train=True, download=False, transform=transform)\n",
    "mnist_test = MNIST(root='data', train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the folllowing architecture for our CNN:\n",
    "Let's assume the input tensor represents a batch of MNIST images with shape `(N, 1, 28, 28)`, where N is the batch size.\n",
    "\n",
    "1. **First Convolutional Layer (`conv1`):**  \n",
    "   - **Input:** `(N, 1, 28, 28)`  , where `N` is the batch size.\n",
    "   - This layer uses 16 filters of size 3×3 with padding=1. Padding of 1 preserves the spatial dimensions.  \n",
    "   - **Output after `conv1`:** `(N, 16, 28, 28)`  \n",
    "   - A ReLU activation is applied, but it does not change the tensor shape.\n",
    "   - There are $16 * 3 * 3 = 144$ learnable weights in this layer.\n",
    "\n",
    "2. **First Pooling Layer (`pool` after `conv1`):**  \n",
    "   - **Input:** `(N, 16, 28, 28)`  \n",
    "   - The max pooling layer uses a kernel size of 2 and stride 2, which halves the spatial dimensions.  \n",
    "   - **Output after pooling:** `(N, 16, 14, 14)`\n",
    "\n",
    "3. **Second Convolutional Layer (`conv2`):**  \n",
    "   - **Input:** `(N, 16, 14, 14)`  \n",
    "   - This layer uses 32 filters of size 3×3 with padding=1, preserving the spatial dimensions.  \n",
    "   - **Output after `conv2`:** `(N, 32, 14, 14)`  \n",
    "   - Again, applying ReLU does not change the shape.\n",
    "   - There are $32 * 3 * 3 = 288$ learnable weights in this layer.\n",
    "\n",
    "4. **Second Pooling Layer (`pool` after `conv2`):**  \n",
    "   - **Input:** `(N, 32, 14, 14)`  \n",
    "   - With the same 2×2 kernel and stride 2, this pooling operation further halves the spatial dimensions.  \n",
    "   - **Output after pooling:** `(N, 32, 7, 7)`\n",
    "\n",
    "5. **Flattening:**  \n",
    "   - **Input:** `(N, 32, 7, 7)`  \n",
    "   - This tensor is flattened into a vector for each example. The total number of features per image is `32 * 7 * 7 = 1568`.  \n",
    "   - **Output after flattening:** `(N, 1568)`\n",
    "\n",
    "6. **First Fully Connected Layer (`fc1`):**  \n",
    "   - **Input:** `(N, 1568)`  \n",
    "   - This layer maps the 1568 features to 128 neurons using an affine map following by ReLU.  \n",
    "   - **Output:** `(N, 128)`  \n",
    "   - There are $1568 * 128 + 128 = 200,832$ learnable weights in this layer.\n",
    "\n",
    "7. **Second Fully Connected Layer (`fc2`):**  \n",
    "   - **Input:** `(N, 128)`  \n",
    "   - This final layer maps the 128 features to 10 outputs (one per digit class 0-9).  \n",
    "   - **Output:** `(N, 10)`\n",
    "   - There are $128 * 10 + 10 = 1,290$ learnable weights in this layer.\n",
    "\n",
    "Thus, as data flows through the network, the tensor shapes change as follows:\n",
    "\n",
    "- Input: `(N, 1, 28, 28)`  \n",
    "- After `conv1` + ReLU: `(N, 16, 28, 28)`  \n",
    "- After first pooling: `(N, 16, 14, 14)`  \n",
    "- After `conv2` + ReLU: `(N, 32, 14, 14)`  \n",
    "- After second pooling: `(N, 32, 7, 7)`  \n",
    "- After flattening: `(N, 1568)`  \n",
    "- After `fc1` + ReLU: `(N, 128)`  \n",
    "- After `fc2`: `(N, 10)`\n",
    "\n",
    "In total, the network has\n",
    "\\begin{equation*}\n",
    "    144 + 288 + 200832 + 1290 = 202,554\n",
    "\\end{equation*}\n",
    "weights! \n",
    "\n",
    "Notice that the vast majority of them are in the fully connected layers. This pattern typically holds also for other kinds of architectures (like transformers); the special sauce in the architecture extracts features in some manner, and then one hits the extracted features with fully connected layers and hope that it is smart enough to make the right predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model for MNIST classification\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, # 1 input channel (grayscale)\n",
    "                               out_channels=16, # 16 output channels (feature maps)\n",
    "                               kernel_size=3, \n",
    "                               padding=1 # to preserve image dimensions\n",
    "                               ) \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, # 16 input channels (previous feature maps)\n",
    "                               out_channels=32, # 32 output channels (new, fancier feature maps)\n",
    "                               kernel_size=3, \n",
    "                               padding=1 # to preserve image dimensions\n",
    "                               )  \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, # Replace 2x2 grid of values with max value\n",
    "                                 stride=2 # Shift by 2 pixels, results in halving spatial dims\n",
    "                                 )  \n",
    "        # 1st Fully connected layer\n",
    "        # Input: flattened vector of 32 feature maps of size 7x7 (32*7*7 length in total)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128) # output: 128 features\n",
    "        # 2nd Fully connected layer \n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer (10 classes for digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # first Convolution\n",
    "        x = F.relu(x) # ReLU activation\n",
    "        x = self.pool(x) # first Pooling\n",
    "        x = self.conv2(x) # second Convolution\n",
    "        x = F.relu(x) # ReLU activation\n",
    "        x = self.pool(x) # second Pooling\n",
    "        x = x.view(-1, 32 * 7 * 7)  # Flatten\n",
    "        x = self.fc1(x)  # first fully connected layer\n",
    "        x = F.relu(x) # ReLU activation\n",
    "        x = self.fc2(x) # output layer\n",
    "        return x # Output layer (logits for 10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss evolution\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(loss_history, marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Evolution on MNIST\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test classification results\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    sample_image, sample_label = mnist_test[i]\n",
    "    sample_image = sample_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(sample_image)\n",
    "        predicted_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    axes[i].imshow(sample_image.squeeze(0).squeeze(0), cmap=\"gray\")\n",
    "    axes[i].set_title(f\"True: {sample_label}, Pred: {predicted_label}\")\n",
    "for ax in axes.flat:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. AlexNet architecture (2012)\n",
    "\n",
    "One of the key turning points for CNNs (and neural networks in general) was the development of AlexNet in 2012, by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. This was the first deep CNN to win the ImageNet Large Scale Visual Recognition Challenge, which is a prestigious competition in the field of computer vision. AlexNet was able to achieve a top-5 error rate of 15.3%, destroyed the 26.2% achieved by the second-place entry.\n",
    "\n",
    "Here is a  summary of the type of layers found in the famous AlexNet architecture:\n",
    "\n",
    "Below is a detailed description of the layers in AlexNet, including the layer types, kernel sizes, strides, and other relevant details:\n",
    "\n",
    "1. **Convolutional Layer 1:**  \n",
    "   - **Type:** Convolutional  \n",
    "   - **Filters:** 96  \n",
    "   - **Kernel Size:** 11×11  \n",
    "   - **Stride:** 4  \n",
    "   - **Padding:** Typically no padding (or minimal) so that the receptive field is large.  \n",
    "   - **Additional Operations:** Followed by ReLU activation, Local Response Normalization (LRN), and then max pooling.\n",
    "\n",
    "2. **Pooling Layer 1:**  \n",
    "   - **Type:** Max Pooling  \n",
    "   - **Kernel Size:** 3×3  \n",
    "   - **Stride:** 2  \n",
    "   - **Purpose:** Reduces spatial dimensions and provides translation invariance.\n",
    "\n",
    "3. **Convolutional Layer 2:**  \n",
    "   - **Type:** Convolutional (split into 2 groups to distribute across GPUs in the original implementation)  \n",
    "   - **Filters:** 256  \n",
    "   - **Kernel Size:** 5×5  \n",
    "   - **Stride:** 1  \n",
    "   - **Padding:** 2 (to preserve spatial dimensions)  \n",
    "   - **Additional Operations:** Followed by ReLU, LRN, and max pooling.\n",
    "\n",
    "4. **Pooling Layer 2:**  \n",
    "   - **Type:** Max Pooling  \n",
    "   - **Kernel Size:** 3×3  \n",
    "   - **Stride:** 2  \n",
    "   - **Purpose:** Further spatial dimensionality reduction.\n",
    "\n",
    "5. **Convolutional Layer 3:**  \n",
    "   - **Type:** Convolutional  \n",
    "   - **Filters:** 384  \n",
    "   - **Kernel Size:** 3×3  \n",
    "   - **Stride:** 1  \n",
    "   - **Padding:** 1 (to preserve spatial dimensions)  \n",
    "   - **Additional Operations:** Followed by ReLU activation.\n",
    "\n",
    "6. **Convolutional Layer 4:**  \n",
    "   - **Type:** Convolutional (again split into 2 groups)  \n",
    "   - **Filters:** 384  \n",
    "   - **Kernel Size:** 3×3  \n",
    "   - **Stride:** 1  \n",
    "   - **Padding:** 1  \n",
    "   - **Additional Operations:** Followed by ReLU activation.\n",
    "\n",
    "7. **Convolutional Layer 5:**  \n",
    "   - **Type:** Convolutional (split into 2 groups)  \n",
    "   - **Filters:** 256  \n",
    "   - **Kernel Size:** 3×3  \n",
    "   - **Stride:** 1  \n",
    "   - **Padding:** 1  \n",
    "   - **Additional Operations:** Followed by ReLU activation and then a max pooling layer.\n",
    "\n",
    "8. **Pooling Layer 3:**  \n",
    "   - **Type:** Max Pooling  \n",
    "   - **Kernel Size:** 3×3  \n",
    "   - **Stride:** 2  \n",
    "   - **Purpose:** Reduces spatial dimensions before transitioning to fully connected layers.\n",
    "\n",
    "9. **Fully Connected Layer 1:**  \n",
    "   - **Type:** Fully Connected (Dense)  \n",
    "   - **Neurons:** 4096  \n",
    "   - **Additional Operations:** Followed by ReLU activation and Dropout for regularization.\n",
    "\n",
    "10. **Fully Connected Layer 2:**  \n",
    "    - **Type:** Fully Connected (Dense)  \n",
    "    - **Neurons:** 4096  \n",
    "    - **Additional Operations:** Followed by ReLU activation and Dropout for regularization.\n",
    "\n",
    "11. **Fully Connected Layer 3 (Output Layer):**  \n",
    "    - **Type:** Fully Connected (Dense)  \n",
    "    - **Neurons:** 1000  \n",
    "    - **Purpose:** Produces the class scores for ImageNet classification.\n",
    "\n",
    "AlexNet's architecture incorporates several key design elements for effective feature extraction:\n",
    "\n",
    "- **Increasing Number of Filters:**  \n",
    "  As we progress deeper into the network, the number of filters increases. This allows the network to capture a growing variety of features and more complex patterns. Early layers detect simple features (e.g., edges and textures), while deeper layers combine these into higher-level representations.\n",
    "\n",
    "- **Decreasing Kernel Sizes:**  \n",
    "  The kernel size tends to decrease in later layers (from larger windows in early layers to smaller ones later) as the network focuses on fine-tuning and combining features from earlier layers. Smaller kernels can capture local details and enable the network to achieve higher resolution in the learned features, while larger kernels in early layers cover broader regions.\n",
    "\n",
    "- **Spatial Reduction:**  \n",
    "  Pooling layers and the use of strides in convolutional layers progressively reduce the spatial dimensions of the feature maps. This helps reduce computational complexity and allows the fully connected layers at the end of the network to operate on a manageable number of features.\n",
    "\n",
    "- **Hierarchical Feature Learning:**  \n",
    "  The combination of convolutional and pooling layers supports a hierarchical feature extraction process. Lower layers capture basic patterns, and as we move to higher layers, the network learns more abstract representations, effectively mimicking the visual processing hierarchy in the human visual system.\n",
    "\n",
    "The final fully connected layers in AlexNet serve several key purposes:\n",
    "\n",
    "- **Integration of Features:**  \n",
    "  The fully connected layers take the high-level feature maps extracted by the convolutional and pooling layers and integrate these features. They combine all of the learned local information into a global understanding of the image.\n",
    "\n",
    "- **Decision Making:**  \n",
    "  Acting like a traditional neural network (multilayer perceptron), these layers perform the final steps of \"reasoning\" on the consolidated features. They map the learned representations to class scores by applying learned weights and biases.\n",
    "\n",
    "- **Non-linear Transformation:**  \n",
    "  Each fully connected layer is typically followed by a non-linear activation function (e.g., ReLU), which helps the network learn complex, non-linear decision boundaries.\n",
    "\n",
    "- **Output Generation:**  \n",
    "  The very last fully connected layer produces output neurons corresponding to the number of classes (e.g., 1000 for ImageNet), which are then often processed by a softmax function to generate probabilistic class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import json\n",
    "# import urllib.request\n",
    "\n",
    "# Define the path to the parent folder of the current notebook\n",
    "# parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "# json_filename = os.path.join(parent_dir, 'imagenet_class_index.json')\n",
    "\n",
    "# # If the file doesn't exist in the parent folder, download it there.\n",
    "# if not os.path.exists(json_filename):\n",
    "#     url = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
    "#     print(\"Downloading 'imagenet_class_index.json' to the parent folder...\")\n",
    "#     urllib.request.urlretrieve(url, json_filename)\n",
    "#     print(\"Download complete.\")\n",
    "\n",
    "# Load ImageNet class labels from the JSON file in the same folder as the notebook\n",
    "with open('imagenet_class_index.json') as f:\n",
    "    class_idx = {int(key): value[1] for key, value in json.load(f).items()}\n",
    "class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained AlexNet model\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "alexnet = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "alexnet.eval()\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),             # Resize the shorter side to 256 pixels\n",
    "    transforms.CenterCrop(224),         # Crop a 224x224 region from the center\n",
    "    transforms.ToTensor(),              # Convert PIL image to Tensor\n",
    "    transforms.Normalize(               # Normalize using ImageNet's mean and std\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "def classify_image(image_path, model, preprocess, idx2label):\n",
    "    # Open the image and ensure it is in RGB mode\n",
    "    img = PILImage.open(image_path).convert('RGB')\n",
    "    # Preprocess the image\n",
    "    input_tensor = preprocess(img)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "    # Get the model predictions without computing gradients\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    # Compute probabilities and get the top predicted class\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    top_prob, top_idx = probabilities.topk(1)\n",
    "    predicted_label = class_idx[top_idx.item()]\n",
    "    return predicted_label, top_prob.item()\n",
    "\n",
    "# List of pet image filenames (ensure these images are in the same directory or adjust the paths)\n",
    "pet_images = ['murphy.jpeg', 'anbu.jpeg']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(pet_images), figsize=(10, 5))\n",
    "for ax, image_path in zip(axes, pet_images):\n",
    "    label, probability = classify_image(image_path, alexnet, preprocess, class_idx)\n",
    "    print(f\"Image: {image_path}, Predicted label: {label}, Probability: {probability:.4f}\")\n",
    "    \n",
    "    img = PILImage.open(image_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Predicted: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet model\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "resnet.eval()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(pet_images), figsize=(10, 5))\n",
    "for ax, image_path in zip(axes, pet_images):\n",
    "    label, probability = classify_image(image_path, resnet, preprocess, class_idx)\n",
    "    print(f\"Image: {image_path}, Predicted label: {label}, Probability: {probability:.4f}\")\n",
    "    \n",
    "    img = PILImage.open(image_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Predicted: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lol, AlexNet thinks Anbu is a giant panda, and ResNet thinks she is a Chickadee. For reference, here is a picture of a Chickadee (once you see it, you can forgive ResNet because the resemblance is uncanny):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('chickadee-indiana-audubon.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
