{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression\n",
    "In this notebook:\n",
    "- We discuss Lagrange's interpolation theorem on finding polynomials that interpolate a set of points in the plane.\n",
    "- We discuss polynomial regression, which is a method of fitting a polynomial to a set of points in the plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Suppose we have a regression problem with one variable $X$ and one target $Y$, both continuous variables. Assume that there are $m$ instances $(x_i,y_i)$, $i=1,\\ldots,m$, of the variables. \n",
    "Recall that in our parametric ML framework, we assume the following: There exists a class of functions $\\{f_{\\mathbf{w}}\\}$ (called a *model*), parameterized by a vector of parameters $\\mathbf{w} \\in \\mathbb{R}^n$, such that for some particular choice of $(\\mathbf{w})$, the relationship between $X$ and $Y$ can be modeled as\n",
    "\\begin{equation*}\n",
    "    Y = f_{\\mathbf{w}}(X) + \\epsilon,\n",
    "\\end{equation*}\n",
    "where $\\epsilon$ is a random variable representing noise in the data.\n",
    "\n",
    "For example, in the case of simple linear regression, the parameters are a pair $\\mathbf{w} = (w_0,w_1) \\in \\mathbb{R}^2$, and the model is $f_{\\mathbf{w}}(x) = w_0 + w_1x$. The particular choice of $\\mathbf{w}$ giving the best model (according to some choice of loss function) is the \"line of best fit\". \n",
    "\n",
    "It is possible to also use higher order polynomials to model the relationship between $X$ and $Y$. For example, we could use a quadratic model $f_{\\mathbf{w}}(x) = w_0 + w_1x + w_2x^2$, or a cubic model $f_{\\mathbf{w}}(x) = w_0 + w_1x + w_2x^2 + w_3x^3$, and so on. Such a model is then called a **polynomial regression** model. Before we discuss this in detail, we first discuss the concept of interpolation and compare it to regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "As above, let's suppose we have a collection of data points $(x_i,y_i) \\in \\mathbb{R}^2$, $i=1,\\ldots,m$. The goal of *interpolation* is to find a function $f(x)$ that passes through all the points *exactly* (without any error or noise term). In other words, we want to find a function $f(x)$ such that $f(x_i) = y_i$ for all $i=1,\\ldots,m$. Of course, this is doomed to fail if the same $x$-value occurs more than once with different associated $y$-values. For simplicity, let's suppose for now that all $x_i$ are distinct.\n",
    "\n",
    "There are many ways to interpolate a set of points, corresponding to the choice of the class of functions used. For this to make sense (i.e. be guaranteed to work), we need a class of functions that is \"rich enough\" to interpolate any set of points. Here are three commonly used classes with this \"universal interpolator\" property:\n",
    "- Polynomials\n",
    "- Piece-wise linear functions\n",
    "- Splines (piece-wise polynomials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial interpolation\n",
    "Of the three classes, the most natural (perhaps, because they are smooth?) are polynomials. We recall here that for a polynomial $f(x) = \\sum_{i=0}^n a_ix^i$, with $a_n \\neq 0$, the **degree** (or **order**) of $f(x)$ is the largest non-zero power of $x$ which appears, namely, $n$. Thus degree $0$ means a constant function, degree $1$ means a linear function, degree $2$ means a quadratic function, and so on.\n",
    "\n",
    "Now, suppose we have a degree $n$ polynomial\n",
    "\\begin{equation*}\n",
    "    f_{\\mathbf{w}}(x) = w_0 + w_1x + w_2x^2 + \\dotsb + w_nx^n.\n",
    "\\end{equation*}\n",
    "Given a data point $(x_i,y_i)$, asking for this polynomial to pass through this data point means we must have\n",
    "\\begin{equation*}\n",
    "    y_i = f_{\\mathbf{w}}(x_i) = w_0 + w_1x_i + w_2x_i^2 + \\dotsb + w_nx_i^n.\n",
    "\\end{equation*}\n",
    "Now, recall that the main principle/take-away from linear algebra is that whenever you see a sum of products, you should immediately try to express it as a dot product. In this case, we can re-write the above equation as\n",
    "\\begin{equation*}\n",
    "    y_i = \\begin{bmatrix} \\; 1 & x_i & \\dotsb & x_i^n \\; \\end{bmatrix} \\begin{bmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix} = \\mathbf{r}_i^T\\mathbf{w}.\n",
    "\\end{equation*}\n",
    "Thus, the polynomial interpolation problem can be re-stated as follows: Given a set of data points $(x_i,y_i)$, $i=1,\\ldots,m$, find a vector $\\mathbf{w} \\in \\mathbb{R}^{n+1}$ such that\n",
    "\\begin{equation*}\n",
    "    \\begin{bmatrix}\n",
    "        1 & x_1 & \\dotsb & x_1^n \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        1 & x_m & \\dotsb & x_m^n\n",
    "    \\end{bmatrix} \\begin{bmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix} =  \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "Thus, solving this equation amounts to solving a system of $m$ linear equations in $n+1$ unknowns. This leads to 3 cases:\n",
    "1. If $m < n+1$, then the system is said to be *under-determined*, and there are infinitely many solutions. (In fact this is not super obvious, and it requires us to know that the rows of the matrix are linearly independent.)\n",
    "2. If $m > n+1$, then the system is said to be *over-determined*, and there are no solutions (in general). In this case, we can try to find the \"closest approximation\" to a solution, which is the idea behind regression!\n",
    "3. If $m=n+1$, then there always exists (provided all $x_i$'s are distinct) a *unique* solution. Writing this out in terms of the polynomial, this is known as: \n",
    "\n",
    "**Lagrange's interpolation theorem**: \n",
    "*Given a set of $m$ points $(x_i,y_i)$, $i=1,\\dotsc,m$, with all $x_i$'s distinct, there exists a **unique** polynomial of degree at most $m-1$ that passes through all the points.*\n",
    "\n",
    "In your HW 2, you will walk through a guided proof of this beautiful theorem. \n",
    "\n",
    "Below, let's see how to implement polynomial interpolation in Python using `polyfit` method from `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create a 1D array of numbers from 0 to 9\n",
    "xs = np.arange(-5,6)\n",
    "\n",
    "# 2. Create an array of ys by applying f(x) = x^2 - 4x - 10 to xs\n",
    "ys = xs**3 - xs - 1\n",
    "\n",
    "# 3. Compute an interpolant using polyfit\n",
    "coeffs = np.polyfit(xs, ys, deg=9)\n",
    "\n",
    "# 4. Make polynomial object\n",
    "poly = np.poly1d(coeffs)\n",
    "\n",
    "# 5. Print the polynomial\n",
    "print(poly)\n",
    "\n",
    "# 6. Plot the points and the polynomial\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.scatterplot(x=xs, y=ys, color='red')\n",
    "sns.lineplot(x=xs, y=poly(xs))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, we did not get back the polynomial $x^3 - x -1$ that we originally used to generate the data. However, we came exceedingly close (because all the other coefficients are almost equal to zero). To rectify this issue, we can simply write a custom function to perform polynomial interpolation and round the coefficients to `round_to` decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounded_polyfit(x, y, deg, round_to=6):\n",
    "    \"\"\"\n",
    "    Fit a polynomial to the data and round the coefficients to a specified number of decimal places (default = 6). Returns a polynomial object.\n",
    "    \"\"\"\n",
    "    coeffs = np.polyfit(x, y, deg=deg)\n",
    "    rounded_coeffs = np.round(coeffs, round_to)\n",
    "    return np.poly1d(rounded_coeffs)\n",
    "\n",
    "# Compute an interpolant using rounded_polyfit\n",
    "rounded_poly = rounded_polyfit(xs, ys, deg=9)\n",
    "print(rounded_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression\n",
    "In polynomial regression, we fit a polynomial to a set of points $(x_i,y_i)$, $i=1,\\ldots,m$, by minimizing the sum of squared errors. In other words, we choose a degree $n$ and find the polynomial $f_{\\mathbf{w}}(x)$ of degree $n$ that minimizes the MSE loss function. Thus, our model parameters are the coefficients of the polynomial:\n",
    "\\begin{equation*}\n",
    "    \\mathbf{w} = \\begin{bmatrix} \\; w_0 & w_1 & \\dotsb & w_n \\; \\end{bmatrix}^T \\in \\mathbb{R}^{n+1}.\n",
    "\\end{equation*}\n",
    "The loss function is therefore defined by\n",
    "\\begin{align*}\n",
    "    J(\\mathbf{w}; \\mathbf{x},\\mathbf{y}) & = \\frac{1}{m} \\sum_{i=1}^m \\left( y_i - f_{\\mathbf{w}}(x_i) \\right)^2 \\\\\n",
    "    & = \\frac{1}{m} \\sum_{i=1}^m \\left( y_i - \\sum_{j=0}^n w_jx_i^j \\right)^2.\n",
    "\\end{align*}\n",
    "Above, we've included the columns $\\mathbf{x}$ and $\\mathbf{y}$ of the labelled dataset to emphasize that the loss function depends on the given data.\n",
    "\n",
    "Then, fitting the polynomial to the data amounts to finding the optimal parameters $\\hat{\\mathbf{w}} \\in \\mathbb{R}^{n+1}$ that minimize the loss function on the data:\n",
    "\\begin{equation*}\n",
    "    \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w} \\in \\mathbb{R}^{n+1}} J(\\mathbf{w};\\mathbf{x},\\mathbf{y}).\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity of polynomial regression\n",
    "It is important to note that polynomial regression is a linear regression problem (preceded by some data manipulation). This is because the model is linear in the parameters $\\mathbf{w}$, even though it is non-linear in the input variable $x$. Thus, one can think of a degree $n$ polynomial regression model as consisting of two steps:\n",
    "1. Use the single feature column $\\mathbf{x}$ to create $n+1$ feature columns $\\mathbf{1}, \\mathbf{x}, \\mathbf{x}^2, \\dotsc, \\mathbf{x}^n$, where\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x}^j = \\begin{bmatrix} \\; x_1^j & \\dotsb & x_m^j \\; \\end{bmatrix}^T \\in \\mathbb{R}^m.\n",
    "\\end{equation*}\n",
    "These columns comprise an augmented design matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times (n+1)}$. \n",
    "2. Perform multiple linear regression on the labelled dataset $(\\mathbf{X},\\mathbf{y})$ to find the optimal parameters $\\hat{\\mathbf{w}}$ as usual. \n",
    "\n",
    "Let's demonstrate this below by writing a custom polynomial regression class which uses `LinearRegression` from `sklearn` to perform the regression after creating the augmented design matrix. Below, the function `np.vander` creates the *Vandermonde matrix* (of degree $n$) from the input vector $\\mathbf{x}$, which is the matrix $X \\in \\mathbb{R}^{m \\times (n+1)}$ above whose $j$-th column is consists of $j$-th powers of the feature vector $\\mathbf{x}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class MyPolynomialRegression:\n",
    "    def __init__(self, deg=1, round_to=6):\n",
    "        self.deg = deg\n",
    "        self.poly = None\n",
    "        self.round_to = round_to\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # create augmented x matrix with powers of x up to deg (includes constant term)\n",
    "        X_train = np.vander(x, self.deg+1)\n",
    "        # create linear regression object with no extra intercept\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        # fit the model\n",
    "        model.fit(X_train, y)\n",
    "        # store the model as a np.poly1d object\n",
    "        self.poly = np.poly1d(np.round(model.coef_, self.round_to))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.poly(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish off this game, let's introduce a new (third) way to compute the polynomial regression: using the `PolynomialFeatures` class from `sklearn`. This class is used to generate polynomial and interaction features (i.e. it replaces the step where we create the Vandermonde matrix). Below, we demonstrate how to use this class to generate the augmented design matrix and perform polynomial regression. \n",
    "\n",
    "We also use the `make_pipeline` function from `sklearn` to create a pipeline that first generates the polynomial features and then performs linear regression. This is a very useful tool in practice, as it allows us to chain together multiple steps in the data processing and model fitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement polynomial regression using PolynomialFeatures from sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "class MyPolynomialRegression2:\n",
    "    def __init__(self, deg=1, round_to=6):\n",
    "        self.deg = deg\n",
    "        self.poly = None\n",
    "        self.round_to = round_to\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # create linear regression object with no extra intercept\n",
    "        model = make_pipeline(\n",
    "            PolynomialFeatures(degree=self.deg),\n",
    "            LinearRegression(fit_intercept=False)\n",
    "        )\n",
    "        # fit the model\n",
    "        model.fit(x.reshape(-1,1), y)\n",
    "        # get the coefficients from the linear regression step\n",
    "        coeffs = model.named_steps['linearregression'].coef_\n",
    "        # store the model as a np.poly1d object\n",
    "        self.poly = np.poly1d(np.round(coeffs[::-1], self.round_to))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.poly(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So now we have three ways to perform polynomial regression:\n",
    "1. Using `np.polyfit` to directly fit a polynomial to the data (without any data manipulation). If we want, we can also use the custom `rounded_polyfit` function to round the coefficients.\n",
    "2. Using a custom class to perform polynomial regression by creating the augmented design matrix with `np.vander` and then performing linear regression using `sklearn`'s `LinearRegression`.\n",
    "3. Using the `PolynomialFeatures` class from `sklearn` to generate the augmented design matrix and then performing linear regression using `sklearn`'s `LinearRegression`.\n",
    "\n",
    "Let's check that all three methods yield the same result by generating some synthetic data and fitting a polynomial to it using all three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 random x values in the range -5 to 5\n",
    "np.random.seed(0)\n",
    "x = np.random.uniform(-5, 5, 100)\n",
    "\n",
    "# generate a degree 5 polynomial with random small coefficients\n",
    "original_coeffs = np.random.uniform(-1, 1, 6).tolist()\n",
    "original_poly = np.poly1d(original_coeffs)\n",
    "print('Original polynomial to generate data: ')\n",
    "print(original_poly)\n",
    "\n",
    "# generate y values by evaluating the polynomial at the x values and adding random noise\n",
    "y = original_poly(x) + np.random.normal(0, 5, 100)\n",
    "\n",
    "# for degrees 1,..., 10, fit a polynomial to the data using the three methods, round to 8 decimal places. Store the polys as poly1d objects in a dataframe with index 1,...,10\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(index=range(1,11), columns=['rounded_polyfit', 'MyPolynomialRegression', 'MyPolynomialRegression2', 'MSE'])\n",
    "\n",
    "for deg in range(1, 11):\n",
    "    # rounded_polyfit\n",
    "    coeffs = rounded_polyfit(x, y, deg=deg,round_to=8)\n",
    "    df.loc[deg, 'rounded_polyfit'] = np.poly1d(coeffs)\n",
    "\n",
    "    # MyPolynomialRegression\n",
    "    my_poly_reg = MyPolynomialRegression(deg=deg, round_to=8)\n",
    "    my_poly_reg.fit(x, y)\n",
    "    df.loc[deg, 'MyPolynomialRegression'] = my_poly_reg.poly\n",
    "\n",
    "    # MyPolynomialRegression2\n",
    "    my_poly_reg2 = MyPolynomialRegression2(deg=deg, round_to=8)\n",
    "    my_poly_reg2.fit(x, y)\n",
    "    df.loc[deg, 'MyPolynomialRegression2'] = my_poly_reg2.poly\n",
    "\n",
    "    # compute the MSE for the MyPolynomialRegression model\n",
    "    y_pred = my_poly_reg.predict(x)\n",
    "    mse = np.mean((y - y_pred)**2)\n",
    "    df.loc[deg, 'MSE'] = mse\n",
    "\n",
    "# display the df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It seems that our custom polynomial regression class is working as expected; we got the exact same fitted polynomials (with coefficients rounded to $8$ decimal places) using all three methods. Let's plot the data and the fitted polynomial to see how well the polynomial fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 2, figsize=(10, 20))\n",
    "ax = ax.flatten()\n",
    "for deg in range(1,11):\n",
    "    i=deg-1\n",
    "    f = df.loc[deg, 'rounded_polyfit']\n",
    "    mse = df.loc[deg, 'MSE']\n",
    "    sns.scatterplot(x=x, y=y, ax=ax[i], s=10)\n",
    "    sns.lineplot(x=x, y=f(x), ax=ax[i], color='red')\n",
    "    # display the mean squared error in a box on the plot\n",
    "    ax[i].text(0.05, 0.95, f'MSE: {mse:.4f}', transform=ax[i].transAxes, fontsize=14, verticalalignment='top')\n",
    "    ax[i].set_title(f'Degree {deg}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the very sharp drop-off in MSE when the polynomial degree goes from $4$ to $5$ (which was the degree of the original polynomial used to generate the data). However, when we increase the degree further, the change in MSE is negligable in comparison. This is to be expected, since we created the original data using a degree $5$ polynomial, and so a degree $5$ polynomial should be able to fit the data very well. \n",
    "\n",
    "Determine the right degree polynomial to use in a regression problem is a bit of an art. We will discuss this later in the semester as part of a broader discussion on the bias-variance tradeoff in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomials in many variables\n",
    "Before moving on to the next section, let's briefly discuss some terminology about multi-variable polynomials. Suppose we have variables $x_1,\\dotsc,x_n$. A *monomial* in these variables is a function of the form\n",
    "\\begin{equation*}\n",
    "    x_1^{a_1}x_2^{a_2} \\dotsm x_n^{a_n}.\n",
    "\\end{equation*}\n",
    "The *degree* of such a monomial is the sum of the exponents $a_1 + a_2 + \\dotsb + a_n$. A *polynomial* in these variables is a linear combination of monomials, where (for us) the coefficients are real numbers. The *degree* of a polynomial is the maximum degree of any monomial appearing in the polynomial. For example, $x_1^2x_2 + x_1x_2^2 + x_1x_2$ is a degree $3$ polynomial in two variables, and $x_1x_2x_3 + x_4^5$ is a degree $5$ polynomial in four variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression with multiple features\n",
    "In the above discussion, we considered the case of polynomial regression with a single feature $X$. However, polynomial regression can be generalized to the case of multiple features. For example, if we have three features $X_1,X_2$ and $X_3$, and we want to fit a polynomial of degree $2$ to the data, then the model would be given by\n",
    "\\begin{equation*}\n",
    "    f_{\\mathbf{w}}(X_1,X_2,X_3) = w_0 + w_1X_1 + w_2X_2 + w_3X_3 + w_4X_1^2 + w_5X_2^2 + w_6X_3^2 + w_7X_1X_2 + w_8X_1X_3 + w_9X_2X_3.\n",
    "\\end{equation*}\n",
    "The augmented design matrix would consists of $10$ columns, one for each of the monomials:\n",
    "\\begin{equation*}\n",
    "    \\begin{bmatrix}\n",
    "        1 & X_1 & X_2 & X_3 & X_1^2 & X_2^2 & X_3^2 & X_1X_2 & X_1X_3 & X_2X_3\n",
    "    \\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "After creating the augmented design matrix with the appropriate monomials, we can perform linear regression as usual to find the optimal parameters $\\hat{\\mathbf{w}}$.\n",
    "\n",
    "Note that polynomials in multiple variables allow for more complex relationships between the features. Indeed, the terms $X_1X_2$, $X_1X_3$ and $X_2X_3$ allow for interactions between the features, which can capture more complex relationships than simple linear regression. \n",
    "\n",
    "We illustrate this below by generating some synthetic data with two features and fitting a polynomial of degree $2$ to the data. We also fit a multiple linear regression model to the data for comparison. Then, we plot the predictions for both fitted models to compare the MSE. Finally, we visualize the points in 3D space to see how well each model fits the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data for two features by sampling uniformly from 0 to 10\n",
    "n_samples = 100\n",
    "X1 = np.random.uniform(0, 10, n_samples)\n",
    "X2 = np.random.uniform(0, 10, n_samples)\n",
    "\n",
    "# Create an interaction between X1 and X2 in the target variable, plus some noise.\n",
    "# True relationship: Y = 2 + 1.5*X1 + 3*X2 - 2.5*(X1*X2) + (X1)**2 - (X2)**2 + noise\n",
    "noise = np.random.normal(0, 5, n_samples)\n",
    "Y = 2 + 1.5*X1 + 3*X2 - 2.5*(X1*X2) + (X1)**2 - (X2)**2 + noise\n",
    "\n",
    "# Stack X1 and X2 into a two-dimensional array\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Create a pipeline that generates polynomial features (including interaction terms) \n",
    "# up to degree 2 and then applies linear regression.\n",
    "poly_model = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=True),\n",
    "    LinearRegression(fit_intercept=False)  # we include bias in the polynomial features\n",
    ")\n",
    "# create a linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the models to the data\n",
    "poly_model.fit(X, Y)\n",
    "linear_model.fit(X, Y)\n",
    "\n",
    "# Predict values using the models for visualization\n",
    "Y_poly = poly_model.predict(X)\n",
    "Y_linear = linear_model.predict(X)\n",
    "\n",
    "# compute the MSE for both models\n",
    "mse_poly = np.mean((Y - Y_poly)**2)\n",
    "mse_linear = np.mean((Y - Y_linear)**2)\n",
    "\n",
    "# Print mse\n",
    "print(f\"MSE for quadratic polynomial model: {mse_poly:.3f}\")\n",
    "# Print the learned coefficients of poly_model\n",
    "# The order of coefficients corresponds to [1, X1, X2, X1^2, X1X2, X2^2]\n",
    "coeff_labels = ['Intercept', 'X1', 'X2', 'X1²', 'X1X2', 'X2²']\n",
    "coefficients = poly_model.named_steps['linearregression'].coef_\n",
    "print(\"Coefficients:\")\n",
    "for label, coef in zip(coeff_labels, np.round(coefficients, 3)):\n",
    "    print(f\"{label}: {coef}\")\n",
    "\n",
    "print()\n",
    "# Print mse\n",
    "print(f\"MSE for linear model: {mse_linear:.3f}\")\n",
    "# Print the learned coefficients of linear_model\n",
    "print(\"Coefficients:\")\n",
    "print(f\"Intercept: {np.round(linear_model.intercept_, 3)}\")\n",
    "print(f\"X1: {np.round(linear_model.coef_[0], 3)}\")\n",
    "print(f\"X2: {np.round(linear_model.coef_[1], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot the true vs predicted values for the polynomial model using seaborn scatterplot\n",
    "ax[0].set_title(\"Quadratic Polynomial Model\")\n",
    "sns.scatterplot(x=Y, y=Y_poly, ax=ax[0])\n",
    "ax[0].set(xlabel='True values', ylabel='Predicted values')\n",
    "# draw a diagonal line to represent perfect predictions (red, dashed)\n",
    "ax[0].plot([Y.min(), Y.max()], [Y.min(), Y.max()], 'r--')\n",
    "# Add MSE text box\n",
    "ax[0].text(0.05, 0.95, f'MSE: {mse_poly:.2f}', \n",
    "           transform=ax[0].transAxes,\n",
    "           bbox=dict(facecolor='white', alpha=0.8),\n",
    "           verticalalignment='top')\n",
    "\n",
    "# Plot the true vs predicted values for the linear model using seaborn scatterplot\n",
    "ax[1].set_title(\"Linear Model\")\n",
    "sns.scatterplot(x=Y, y=Y_linear, ax=ax[1])\n",
    "ax[1].set(xlabel='True values', ylabel='Predicted values')\n",
    "# draw a diagonal line to represent perfect predictions (red, dashed)\n",
    "ax[1].plot([Y.min(), Y.max()], [Y.min(), Y.max()], 'r--')\n",
    "# Add MSE text box\n",
    "ax[1].text(0.05, 0.95, f'MSE: {mse_linear:.2f}', \n",
    "           transform=ax[1].transAxes,\n",
    "           bbox=dict(facecolor='white', alpha=0.8),\n",
    "           verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def plot_3d_models(elevation, azimuth):\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot for polynomial model\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax1.scatter(X1, X2, Y, color='blue', alpha=0.6, label='True Y')\n",
    "    \n",
    "    # Create surface for polynomial model\n",
    "    x1_range = np.linspace(X1.min(), X1.max(), 100)\n",
    "    x2_range = np.linspace(X2.min(), X2.max(), 100)\n",
    "    X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n",
    "    Y_pred_grid = poly_model.predict(np.column_stack((X1_grid.ravel(), X2_grid.ravel()))).reshape(X1_grid.shape)\n",
    "    ax1.plot_surface(X1_grid, X2_grid, Y_pred_grid, alpha=0.5, color='red')\n",
    "    \n",
    "    ax1.set_xlabel('X1')\n",
    "    ax1.set_ylabel('X2')\n",
    "    ax1.set_zlabel('Y')\n",
    "    ax1.set_title('Quadratic Polynomial Model\\nMSE: {:.2f}'.format(mse_poly))\n",
    "    ax1.view_init(elev=elevation, azim=azimuth)\n",
    "    \n",
    "    # Plot for linear model\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    ax2.scatter(X1, X2, Y, color='blue', alpha=0.6, label='True Y')\n",
    "    \n",
    "    # Create surface for linear model\n",
    "    Y_linear_grid = linear_model.predict(np.column_stack((X1_grid.ravel(), X2_grid.ravel()))).reshape(X1_grid.shape)\n",
    "    ax2.plot_surface(X1_grid, X2_grid, Y_linear_grid, alpha=0.5, color='green')\n",
    "    \n",
    "    ax2.set_xlabel('X1')\n",
    "    ax2.set_ylabel('X2')\n",
    "    ax2.set_zlabel('Y')\n",
    "    ax2.set_title('Linear Model\\nMSE: {:.2f}'.format(mse_linear))\n",
    "    ax2.view_init(elev=elevation, azim=azimuth)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widgets\n",
    "elevation_slider = widgets.FloatSlider(\n",
    "    value=30,\n",
    "    min=0,\n",
    "    max=90,\n",
    "    step=1,\n",
    "    description='Elevation:',\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "azimuth_slider = widgets.FloatSlider(\n",
    "    value=45,\n",
    "    min=0,\n",
    "    max=360,\n",
    "    step=1,\n",
    "    description='Azimuth:',\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "# Create interactive plot\n",
    "widgets.interactive(plot_3d_models, elevation=elevation_slider, azimuth=azimuth_slider)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
