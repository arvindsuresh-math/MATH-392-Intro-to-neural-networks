{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Assume that we have a labelled dataset with $m$ samples, continuous features $X_1,\\dotsc,X_n$, and continuous target $Y$. We denote the feature columns by $\\mathbf{x}_i \\in \\mathbb{R}^m$, and the target column by $\\mathbf{y} \\in \\mathbb{R}^m$. \n",
    "\n",
    "For this notebook, let's work with the compressive sensing strength dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the usual libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the concrete compressive strength train dataset\n",
    "df_train = pd.read_csv('../data/regression/concrete_compressive_strength/train.csv')\n",
    "df_test = pd.read_csv('../data/regression/concrete_compressive_strength/test.csv')\n",
    "\n",
    "# print shapes of the two datasets\n",
    "print(f'Train set: {df_train.shape}')\n",
    "print(f'Test set: {df_test.shape}')\n",
    "\n",
    "# Make features and targets for train and test sets\n",
    "X_train = df_train.drop(columns=['Y'])\n",
    "y_train = df_train['Y']\n",
    "X_test = df_test.drop(columns=['Y'])\n",
    "y_test = df_test['Y']\n",
    "\n",
    "# load the info about the variable names\n",
    "df_info = pd.read_csv('../data/regression/concrete_compressive_strength/data_description.csv')\n",
    "\n",
    "df_info = df_info[['name','new_col_name','role','type']]\n",
    "df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLE model\n",
    "The multiple linear regression model assumes that the target is a linear combination of the features, plus some noise. That is, it assumes that there exists some linear function\n",
    "\\begin{equation*}\n",
    "    F_{b,\\mathbf{w}}(X_1,\\dotsc,X_n) = b + w_1X_1 + \\dotsb + w_nX_n,\n",
    "\\end{equation*}\n",
    "where $\\mathbf{w} = (w_1,\\dotsc,w_n) \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}$, such that\n",
    "\\begin{equation*}\n",
    "    y_i = F_{\\mathbf{w}}(x_{i1},\\dotsc,x_{in}) + \\epsilon_i, \\quad \\textup{ for } i=1,\\dotsc,m.\n",
    "\\end{equation*}\n",
    "Above, each $\\epsilon_i$ is a random variable representing the noise, $y_i$ is the target value for the $i$-th sample, and $x_{ij}$ is the $j$-th feature value for the $i$-th sample. The parameter space for this model consists of all vectors $(b,\\mathbf{w}) \\in \\mathbb{R}^{n+1}$. The $w_i$'s are called *weights*, and $b$ is called the *bias* (of the model).\n",
    "\n",
    "As usual, we denote the predicted values of the target by\n",
    "\\begin{equation*}\n",
    "    \\hat{y}_i = F_{\\mathbf{w}}(x_{i1},\\dotsc,x_{in}), \\quad \\textup{ for } i=1,\\dotsc,m,\n",
    "\\end{equation*}\n",
    "and we collect these all into a single vector of predictions $\\hat{\\mathbf{y}} = \\begin{bmatrix} \\; \\hat{y}_1 & \\dotsb & \\hat{y}_n \\; \\end{bmatrix} \\in \\mathbb{R}^m$. There are three equivalent ways to express the vector $\\hat{\\mathbf{y}}$ in terms of the features and weights:\n",
    "\n",
    "1. *As a linear combination of the features*: In this case, we write \n",
    "\\begin{equation*}\n",
    "    \\hat{\\mathbf{y}} = b \\mathbf{1} + w_1 \\mathbf{x}_1 + \\dotsb + w_n \\mathbf{x}_n \\in \\mathbb{R}^m.\n",
    "\\end{equation*}\n",
    "In this case, the residuals are given by\n",
    "\\begin{equation*}\n",
    "    \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - (b \\mathbf{1} + w_1 \\mathbf{x}_1 + \\dotsb + w_n \\mathbf{x}_n) \\in \\mathbb{R}^m.\n",
    "\\end{equation*}\n",
    "2. *As a matrix product plus a vector*: In this case, we write\n",
    "\\begin{equation*}\n",
    "    \\hat{\\mathbf{y}} = b\\mathbf{1} + \\mathbf{X} \\mathbf{w} \\in \\mathbb{R}^m,\n",
    "\\end{equation*}\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ is the matrix of features (the \"design matrix\"), and $\\mathbf{w} = (w_1,\\dotsc,w_n) \\in \\mathbb{R}^n$ is the vector of weights. In this case, the residuals are given by\n",
    "\\begin{equation*}\n",
    "    \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - (b \\mathbf{1} + \\mathbf{X} \\mathbf{w}) \\in \\mathbb{R}^m.\n",
    "\\end{equation*}\n",
    "3. *As a matrix product*: In this case, we write\n",
    "\\begin{equation*}\n",
    "    \\hat{\\mathbf{y}} = \\overline{\\mathbf{X}} \\overline{\\mathbf{w}} \\in \\mathbb{R}^m,\n",
    "\\end{equation*}\n",
    "where $\\overline{\\mathbf{X}} \\in \\mathbb{R}^{m \\times (n+1)}$ is the \"augmented design matrix\" (i.e. the original design matrix with an extra column of ones $\\mathbf{1} \\in \\mathbb{R}^m$ added to the left), and $\\overline{\\mathbf{w}} = (b,\\mathbf{w}) = (b,w_1,\\dotsc,w_n) \\in \\mathbb{R}^{n+1}$ is the vector of bias and weights. In this case, the residuals are given by\n",
    "\\begin{equation*}\n",
    "    \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - \\overline{\\mathbf{X}} \\overline{\\mathbf{w}} \\in \\mathbb{R}^m.\n",
    "\\end{equation*}\n",
    "\n",
    "### Formula for MSE loss\n",
    "Similar to the single-variable case, we can use the mean squared error (MSE) as a loss function. The MSE in this case is given by\n",
    "\\begin{align*}\n",
    "    J(\\mathbf{w}) & = \\frac{1}{m} || \\mathbf{y} - \\hat{\\mathbf{y}} ||^2 \\\\\n",
    "                & = \\frac{1}{m} || \\mathbf{y} - (b \\mathbf{1} + w_1 \\mathbf{x}_1 + \\dotsb + w_n \\mathbf{x}_n) ||^2.\n",
    "\\end{align*}\n",
    "Fitting the models means we want to find the optimal parameters $(\\hat{b},\\hat{\\mathbf{w}})$ that minimize the MSE:\n",
    "\\begin{equation*}\n",
    "    (\\hat{b},\\hat{\\mathbf{w}}) = \\argmin_{(b,\\mathbf{w})} J(b,\\mathbf{w}).\n",
    "\\end{equation*}\n",
    "\n",
    "### Strategy for finding optimal parameters\n",
    "We will use linear algebra to derive the optimal parameters. The broad idea is the same as the single-variable case. That is, as $(b,\\mathbf{w})$ varies over the parameter space $\\mathbb{R}^{n+1}$, the vector $\\hat{\\mathbf{y}}$ varies over the subspace\n",
    "\\begin{equation*}\n",
    "    U = \\textup{span}(\\mathbf{1},\\mathbf{x}_1,\\dotsc,\\mathbf{x}_n) \\subseteq \\mathbb{R}^m.\n",
    "\\end{equation*}\n",
    "The optimal parameters are found when $\\hat{\\mathbf{y}}$ is the closest point to $\\mathbf{y}$ in $U$ (i.e. it is the linear combination of $\\mathbf{1},\\mathbf{x}_1,\\dotsc,\\mathbf{x}_n$ that is closest to $\\mathbf{y}$). This is equivalent to saying that $\\hat{\\mathbf{y}}$ is the orthogonal projection of $\\mathbf{y}$ onto $U$. By definition, this means that $\\hat{\\mathbf{y}}$ is the unique vector in $U$ such that\n",
    "\\begin{equation*}\n",
    "    \\mathbf{y} - \\hat{\\mathbf{y}} \\textup{ is orthogonal to every vector in } U.\n",
    "\\end{equation*}\n",
    "As you will prove in homework, this is the same as saying that\n",
    "\\begin{equation}\\tag{1}\n",
    "    \\mathbf{y} - \\hat{\\mathbf{y}} \\textup{ is orthogonal to } \\mathbf{1},\\mathbf{x}_1,\\dotsc,\\mathbf{x}_n.\n",
    "\\end{equation}\n",
    "Now, we can proceed in two ways, to arrive at the same result:\n",
    "\n",
    "1.  The first way is to use the augmented design matrix\n",
    "\\begin{equation*}\n",
    "    \\overline{\\mathbf{X}} = \\begin{bmatrix} \\; \\mathbf{1} & \\mathbf{x}_1 & \\dotsb & \\mathbf{x}_n \\; \\end{bmatrix} \\in \\mathbb{R}^{m \\times (n+1)},\n",
    "\\end{equation*}\n",
    "and get a nice-looking matrix equation that needs solving. This method will require us to invert an $(n+1) \\times (n+1)$ matrix. \n",
    "2. The second way is to deal separately with the bias term $b$ first, because it turns out to be rather easy to determine the optimal value of $b$. Then, we do some finessing of the feature vectors and arrive at a beautiful matrix equation that again needs solving. This method will require us to invert an $n \\times n$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 for fitting: using the augmented design matrix\n",
    "Recall that orthogonality is expressed by saying that the dot product is $0$, and recall also that the dot product of vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^m$ can be expressed as matrix multiplication $\\mathbf{u}^T \\mathbf{v}$, where the first is a row vector and the second is a column vector. \n",
    "\n",
    "Thus, (1) is equivalent to saying that $(\\hat{b},\\hat{\\mathbf{w}})$ satisfy the following $n+1$ equations:\n",
    "\\begin{align*}\n",
    "    \\mathbf{1}^T(\\mathbf{y} - \\hat{\\mathbf{y}}) & = 0, \\\\\n",
    "    \\mathbf{x}_1^T(\\mathbf{y} - \\hat{\\mathbf{y}}) & = 0, \\\\\n",
    "    & \\vdots \\\\\n",
    "    \\mathbf{x}_n^T(\\mathbf{y} - \\hat{\\mathbf{y}}) & = 0.\n",
    "\\end{align*}\n",
    "Re-writing this in matrix form, this is equivalent to saying that\n",
    "\\begin{equation*}\n",
    "    \\overline{X}^T (\\mathbf{y} - \\hat{\\mathbf{y}}) = \\begin{bmatrix} \\mathbf{1}^T \\\\ \\mathbf{x}_1^T \\\\ \\vdots \\\\ \\mathbf{x}_n^T \\end{bmatrix} (\\mathbf{y} - \\hat{\\mathbf{y}}) = 0.\n",
    "\\end{equation*}\n",
    "If we write $\\hat{\\mathbf{y}}$ as $\\overline{\\mathbf{X}} \\overline{\\mathbf{w}}$, where $\\overline{\\mathbf{w}} = (b, \\mathbf{w}) \\in \\mathbb{R}^{n+1}$, then the above equation can be re-arranged to give the following matrix equation:\n",
    "\\begin{equation}\\tag{2}\n",
    "    \\overline{\\mathbf{X}}^T \\overline{\\mathbf{X}} \\overline{\\mathbf{w}} = \\overline{\\mathbf{X}}^T \\mathbf{y} = \\begin{bmatrix} \\mathbf{1} \\cdot \\mathbf{y} \\\\ \\mathbf{x}_1 \\cdot \\mathbf{y} \\\\ \\vdots \\\\ \\mathbf{x}_n \\cdot \\mathbf{y} \\end{bmatrix} \\in \\mathbb{R}^{n+1}. \n",
    "\\end{equation}\n",
    "Thus, our optimal parameters $(\\hat{b},\\hat{\\mathbf{w}})$ are given by solving (4). To solve it, observe that \n",
    "$$\\overline{\\mathbf{X}}^T \\overline{\\mathbf{X}} = \\begin{bmatrix}\n",
    "\\mathbf{1}\\cdot \\mathbf{1} & \\mathbf{1} \\cdot \\mathbf{x}_1 & \\dotsb & \\mathbf{1} \\cdot \\mathbf{x}_n \\\\\n",
    "\\mathbf{x}_1 \\cdot \\mathbf{1} & \\mathbf{x}_1 \\cdot \\mathbf{x}_1 & \\dotsb & \\mathbf{x}_1 \\cdot \\mathbf{x}_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{x}_n \\cdot \\mathbf{1} & \\mathbf{x}_n \\cdot \\mathbf{x}_1 & \\dotsb & \\mathbf{x}_n \\cdot \\mathbf{x}_n\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{(n+1) \\times (n+1)}.$$\n",
    "This square matrix is called the **Gram matrix** associated to the vectors $\\mathbf{1},\\mathbf{x}_1,\\dotsc,\\mathbf{x}_n$. It is of fundamental importance because: *the Gram matrix is invertible if and only if the vectors are linearly independent*. In practice, when the feature vectors arise from real-world data, the columns will almost certainly be linearly independent (otherwise, it would mean that one of our features is on-the-nose a linear combination of the others). Thus, it is safe to assume that the Gram matrix is invertible in our case. \n",
    "\n",
    "Now, we solve for $\\overline{\\mathbf{w}} = (b, \\mathbf{w})$ in (4) by multiplying both sides of (4) by the inverse of the Gram matrix:\n",
    "\\begin{equation}\\tag{3}\n",
    "    \\overline{\\mathbf{w}} = (\\overline{\\mathbf{X}}^T \\overline{\\mathbf{X}})^{-1} \\overline{\\mathbf{X}}^T \\mathbf{y} \\in \\mathbb{R}^{n+1}.\n",
    "\\end{equation}\n",
    "Computing the right-hand side immediately gives us the optimal parameters $(\\hat{b},\\hat{\\mathbf{w}}) \\in \\mathbb{R}^{n+1}$! This concludes method (1) for fitting the model.\n",
    "\n",
    "### Code implementation of method 1\n",
    "Below, we construct a class which implements method (1) for fitting the model. We will use the `numpy` library to do all the matrix computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinReg1:\n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "\n",
    "    # method for fitting the model\n",
    "    def fit(self, X, y):\n",
    "        # add column of ones to design matrix\n",
    "        X.insert(0, 'X0', 1)\n",
    "\n",
    "        # Compute X^T * X\n",
    "        XTX = np.dot(X.T, X)\n",
    "\n",
    "        # Compute inverse of X^T * X\n",
    "        XTX_inv = np.linalg.inv(XTX)\n",
    "\n",
    "        # Compute X^T * y\n",
    "        XTy = np.dot(X.T, y)\n",
    "\n",
    "        # Compute bias and weights vector\n",
    "        self.params = np.dot(XTX_inv, XTy)\n",
    "\n",
    "    # method for making predictions\n",
    "    def predict(self, X):\n",
    "        # add column of ones to design matrix\n",
    "        X.insert(0, 'X0', 1)\n",
    "\n",
    "        # Compute predictions\n",
    "        y_pred = np.dot(X, self.params)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the class\n",
    "method_1 = MyLinReg1()\n",
    "# fit the model\n",
    "method_1.fit(X_train.copy(), y_train)\n",
    "# make predictions on train set to record loss\n",
    "y_train_pred = method_1.predict(X_train.copy())\n",
    "# compute the MSE loss on train set\n",
    "mse_train = np.mean((y_train - y_train_pred) ** 2)\n",
    "\n",
    "# make predictions on test set\n",
    "y_test_pred = method_1.predict(X_test.copy())\n",
    "# compute MSE on test set\n",
    "mse_test = np.mean((y_test - y_test_pred) ** 2)\n",
    "\n",
    "# add results to a new dataframe\n",
    "results = pd.DataFrame(columns = [f'X{i}' for i in range(X_train.shape[1]+1)] + ['MSE_train', 'MSE_test'])\n",
    "# add the results to the dataframe\n",
    "results.loc[0] = np.hstack((method_1.params, mse_train, mse_test))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 for fitting: dealing with bias, then de-meaning\n",
    "\n",
    "### Means and de-meaned vectors\n",
    "Before moving on to method 2, it behooves us to recall some facts about means of vectors and de-meaned vectors. Recall that for any vector $\\mathbf{v} = \\begin{bmatrix} \\; v_1 & \\dotsb & v_m \\; \\end{bmatrix}^T \\in \\mathbb{R}^m$, we have\n",
    "\\begin{equation*}\n",
    "    \\mathbf{1} \\cdot \\mathbf{v} = \\sum_{i=1}^m v_i = m \\overline{\\mathbf{v}},\n",
    "\\end{equation*}\n",
    "where $\\overline{v} = \\frac{1}{m} \\sum_{i=1}^m v_i$ is the **mean** of the entries of $\\mathbf{v}$. Recall from the previous notebook that we denote the **de-meaned** vector $\\mathbf{v} - \\overline{\\mathbf{v}} \\mathbf{1}$ by $\\mathbf{v}' \\in \\mathbb{R}^m$. Explicitly,\n",
    "\\begin{equation*}\n",
    "    \\mathbf{v}' = \\begin{bmatrix} v_1 - \\overline{\\mathbf{v}} \\\\ \\vdots \\\\ v_m - \\overline{\\mathbf{v}} \\end{bmatrix} \\in \\mathbb{R}^m.\n",
    "\\end{equation*}\n",
    "It is called a \"de-meaned\" vector because its mean $\\overline{\\mathbf{v}'}$ equals $0$. You should think of $\\overline{\\mathbf{v}} \\mathbf{1}$ as the \"constant part\" of $\\mathbf{v}$. The entries of the de-meaned vector $\\mathbf{v}'$ should be thought of as the \"variations\" of the entries of $\\mathbf{v}$ from its mean. \n",
    "\n",
    "### Summary of method 2:\n",
    "In HW 2, you will prove that the optimal parameters $(\\hat{b},\\hat{\\mathbf{w}})$ are given by the following two steps:\n",
    "1. Find the optimal weights $\\hat{\\mathbf{w}}$ as follows:\n",
    "\\begin{equation}\\tag{4}\n",
    "    \\hat{\\mathbf{w}} = \\argmin_{\\mathbf{w} \\in \\mathbb{R}^n} || \\mathbf{y}' - \\mathbf{X}' \\mathbf{w} ||^2,\n",
    "\\end{equation}\n",
    "where $\\mathbf{X}' = \\begin{bmatrix} \\; \\mathbf{x}_1' & \\dotsb & \\mathbf{x}_n' \\; \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}$ is the matrix of de-meaned feature vectors, and $\\mathbf{y}' \\in \\mathbb{R}^m$ is the de-meaned target vector. Solving this turns out to be equivalent to solving the following matrix equation:\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{w}} =  \\begin{bmatrix} \\textup{cov}(\\mathbf{x}_1,\\mathbf{x}_1) & \\dotsb & \\textup{cov}(\\mathbf{x}_1,\\mathbf{x}_n) \\\\ \\vdots & \\ddots & \\vdots \\\\ \\textup{cov}(\\mathbf{x}_n,\\mathbf{x}_1) & \\dotsb & \\textup{cov}(\\mathbf{x}_n,\\mathbf{x}_n) \\end{bmatrix}^{-1} \\begin{bmatrix} \\textup{cov}(\\mathbf{x}_1,\\mathbf{y}) \\\\ \\vdots \\\\ \\textup{cov}(\\mathbf{x}_n,\\mathbf{y}) \\end{bmatrix} \\in \\mathbb{R}^n.\n",
    "\\end{equation*}\n",
    "In other words, taking a weighted average of the de-meaned feature vectors $\\mathbf{x}_1',\\dotsc,\\mathbf{x}_n'$ should bring us as close to the de-meaned target vector $\\mathbf{y}'$ as possible. In other words, we want to approximate how $\\mathbf{y}$ varies abouts its mean by using a weighted average of the variations of the features about their respective means!\n",
    "2. Find the optimal bias $\\hat{b}$ using:\n",
    "\\begin{equation}\\tag{5}\n",
    "    \\hat{b} = \\overline{\\mathbf{y}} - \\sum_{i=1}^n \\hat{w}_i \\overline{\\mathbf{x}}_i.\n",
    "\\end{equation}\n",
    "Equation (5) above admits a pleasant interpretation using the concept of \"center of mass\". That is, if we visualize each row (features plus target) as a point $(x_{i1},\\dotsc,x_{in},y_i)$ in $\\mathbb{R}^{n+1}$, then the labelled dataset can be visualized as a cloud of points in $\\mathbb{R}^{n+1}$. Then, the point $(\\overline{\\mathbf{x}}_1,\\dotsc, \\overline{\\mathbf{x}}_n,\\overline{\\mathbf{y}}) \\in \\mathbb{R}^{n+1}$ is the center of mass of this point-cloud. Condition (5) says that fitted model (more precisely, the graph of the fitted model) must pass through the center of mass of this point-cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code implementation of method 2\n",
    "Below, we construct a class which implements method (2) for fitting the model. We will use the `numpy` library to do all the matrix computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinReg2:\n",
    "    def __init__(self):\n",
    "        self.params = None\n",
    "\n",
    "    # method for fitting the model\n",
    "    def fit(self, X, y):\n",
    "        # get means of columsn of X and y\n",
    "        X_mean = X.mean()\n",
    "        y_mean = y.mean()\n",
    "\n",
    "        # de-mean the columns of X and y\n",
    "        X = X - X.mean()\n",
    "        y = y - y.mean()\n",
    "        # Compute X^T * X (scaled co-variance matrix)\n",
    "        XTX = np.dot(X.T, X)\n",
    "        # Compute inverse of X^T * X\n",
    "        XTX_inv = np.linalg.inv(XTX)\n",
    "        # Compute X^T * y (scaled covariance vector)\n",
    "        XTy = np.dot(X.T, y)\n",
    "        # Compute weights\n",
    "        weights = np.dot(XTX_inv, XTy)\n",
    "        bias = y_mean - np.dot(X_mean, weights)\n",
    "        self.params = [bias, *weights]\n",
    "\n",
    "    # method for making predictions\n",
    "    def predict(self, X):\n",
    "        # add column of ones to design matrix\n",
    "        X.insert(0, 'X0', 1)\n",
    "        # Compute predictions\n",
    "        y_pred = np.dot(X, self.params)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of the class\n",
    "method_2 = MyLinReg2()\n",
    "# fit the model\n",
    "method_2.fit(X_train.copy(), y_train.copy())\n",
    "# make predictions on train and test sets\n",
    "y_train_pred = method_2.predict(X_train.copy())\n",
    "y_test_pred = method_2.predict(X_test.copy())\n",
    "# compute the MSE loss on train and test set\n",
    "mse_train = np.mean((y_train - y_train_pred) ** 2)\n",
    "mse_test = np.mean((y_test - y_test_pred) ** 2)\n",
    "# add results to the dataframe\n",
    "results.loc[1] = np.hstack((method_2.params, mse_train, mse_test))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's fit the model using `sklearn`'s `LinearRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "y_train_pred = linreg.predict(X_train)\n",
    "y_test_pred = linreg.predict(X_test)\n",
    "mse_train = np.mean((y_train - y_train_pred) ** 2)\n",
    "mse_test = np.mean((y_test - y_test_pred) ** 2)\n",
    "results.loc[2] = np.hstack((linreg.intercept_, linreg.coef_, mse_train, mse_test))\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
