{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "### Outline\n",
    "In this notebook, we will do the following:\n",
    "\n",
    "1. (5 mins) Discuss how to view parametric models as approximations to a \"ground truth\" function in some function space.\n",
    "    - Polynomials (Lagrange interpolation/ Stone-Weierstrass theorem)\n",
    "    - Neural networks (universal approximation theorem)\n",
    "2. (5 mins) Discuss the historical context of perceptrons, and how they are inspired by biological neurons.\n",
    "    - Biological binary classification problem - to fire or not to fire?\n",
    "3. (20 mins) Define the perceptron model, and how to visualize different activation functions:\n",
    "    - Heaviside step function\n",
    "    - Sigmoid function\n",
    "    - ReLU function\n",
    "    - Tanh function\n",
    "4. (10 mins) Implement perceptrons in Python to solve AND, OR, and NOT problems.\n",
    "5. (5 mins) Discuss the AI winter caused by the inability of perceptrons to solve the XOR problem.\n",
    "6. (25 mins) Define the multilayer perceptron (MLP) model, and implement one to solve the XOR problem.\n",
    "    - From scratch\n",
    "    - Using PyTorch\n",
    "7. (5 mins) Discuss the limitations of MLPs, and how they can be overcome with more advanced architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# XOR dataset\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define the MLP model\n",
    "class XORMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORMLP, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 2)  # Two neurons in hidden layer\n",
    "        self.output = nn.Linear(2, 1)  # One neuron in output layer\n",
    "        self.activation = nn.Tanh()    # Non-linear activation for hidden layer\n",
    "        self.final_activation = nn.Sigmoid()  # Sigmoid for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        x = self.final_activation(self.output(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = XORMLP()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "    print(\"Predictions:\", predictions.round().squeeze().tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
