{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple linear regression\n",
    "\n",
    "## Intro\n",
    "\n",
    "### The SLE model\n",
    "\n",
    "Suppose we have a simple labelled dataset with one feature $X$ and one target $Y$. Assume that both are continuous variables, and we have $m$ samples. According to our convention, we denote the $i$-th sample as $(x_i, y_i)$, where $x_i$ is the feature and $y_i$ is the target.\n",
    "\n",
    "In **simple linear regression**, we assume that the relationship between $X$ and $Y$ is linear. That is, we assume that there exists a pair $\\mathbf{w} = (b,w) \\in \\mathbb{R}^2$ (the parameters of the model) such that\n",
    "\\begin{align*}\n",
    "    y_i & = F_{\\mathbf{w}}(x_i) + \\epsilon_i \\\\\n",
    "    & = b + wx_i + \\epsilon_i,\n",
    "\\end{align*}\n",
    "where each $\\epsilon_i$ is a small error term. (Later, we will be more precise on what exactly we mean by \"small error term\", but for now let's proceed with a vague idea.)\n",
    "\n",
    "### Formula for MSE loss\n",
    "The standard loss function for linear regression is the **mean squared error** (MSE). Recall that, if we denote each prediction as $\\hat{y}_i = F_{\\mathbf{w}}(x_i)$, then the MSE loss function is defined as\n",
    "\\begin{equation*}\n",
    "    J(m,b) = \\frac{1}{m} \\sum_{i=1}^m \\epsilon_i^2 = \\frac{1}{m} \\sum_{i=1}^m (y_i - (b + wx_i))^2. \\tag{1}\n",
    "\\end{equation*}\n",
    "Sometimes, we may also write it as $J(\\mathbf{w})$.\n",
    "\n",
    "### Fitting the model\n",
    "This requires us to solve the following optimization problem: *Find the parameters $\\hat{\\mathbf{w}} = (\\hat{b},\\hat{w})$ which minimize $J(m,b)$, i.e. which satisfy*:\n",
    "\\begin{align*}\n",
    "    \\hat{\\mathbf{w}} & = \\argmin_{\\mathbf{w} \\in \\mathbb{R}^2} \\; J(\\mathbf{w})\n",
    "\\end{align*}\n",
    "The graph of the fitted function $F_{\\hat{\\mathbf{w}}}$ is called the **regression line** or **line of best fit**. \n",
    "\n",
    "### Generating a synthetic dataset\n",
    "In this notebook, we will derive the formulas for $\\hat{\\mathbf{w}}$ in two ways, first using calculus, and then using linear algebra. Before any of that, however, we first generate a synthetic dataset of points to work with. That is, we construct a set of points on a line and then add in some noise. This will be convenient because we will know the \"true\" linear relationship in our data, and we can compare it with the line of best fit that we will derive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.375796</td>\n",
       "      <td>-6.630714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.404368</td>\n",
       "      <td>11.677371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.204509</td>\n",
       "      <td>0.015347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.420721</td>\n",
       "      <td>0.546205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.742586</td>\n",
       "      <td>-3.269908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X          Y\n",
       "0  0.375796  -6.630714\n",
       "1  0.404368  11.677371\n",
       "2  1.204509   0.015347\n",
       "3  1.420721   0.546205\n",
       "4  1.742586  -3.269908"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate 50 random x values between 0 and 20. Generate corresponding y values using the function y = 2x + 1. Add some noise to the y values.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "# Generate random x values and sort in ascending order\n",
    "x = np.random.uniform(0, 20, 50)\n",
    "x.sort()\n",
    "# Generate residuals\n",
    "residuals = np.random.normal(0, 5, 50)\n",
    "# Generate y values using the function y = 0.8x + 1.6 and add noise\n",
    "y = 0.8* x + 1.6 + residuals\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'X': x, 'Y': y})\n",
    "\n",
    "# Plot the data using seaborn\n",
    "plt.figure(figsize=(4, 6))\n",
    "sns.scatterplot(data=df, x='X', y='Y')\n",
    "#add labels\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Scatter plot of X vs Y')\n",
    "plt.show()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting 1: Calculus\n",
    "\n",
    "### Visualizing residuals\n",
    "\n",
    "Below, we add a nice animation illustrating how residuals, and the corresponding MSE, change as we move the regression line. The animation is created using the `manim` library made by the creator of 3Blue1Brown (the greatest math-video channel to have existed). If you want to run it, you can install it with `conda install manim`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.19.0</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m19.0\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"media/jupyter/ScatterPlotWithMovingLineAndParameters@2025-02-01@19-23-34.mp4\" controls autoplay loop style=\"max-width: 60%;\"  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use manim to create a nice scatterplot of the data\n",
    "from manim import *\n",
    "config.verbosity = \"ERROR\"\n",
    "\n",
    "class ScatterPlotWithMovingLineAndParameters(Scene):\n",
    "    def construct(self):\n",
    "        # Create axes with numbers and labels (using original parameters)\n",
    "        axes = Axes(\n",
    "            x_range=[0, 20, 5],\n",
    "            y_range=[-5, 30, 5],\n",
    "            x_length=10,\n",
    "            y_length=6,\n",
    "            axis_config={\"include_numbers\": True, \"font_size\": 30},\n",
    "        )\n",
    "        axes.scale(0.9)\n",
    "        x_label = axes.get_x_axis_label(\"X\")\n",
    "        y_label = axes.get_y_axis_label(\"Y\")\n",
    "        \n",
    "        # Create scatter points (df is assumed to be defined)\n",
    "        points = [\n",
    "            axes.c2p(x_val, y_val)\n",
    "            for x_val, y_val in zip(df['X'], df['Y'])\n",
    "        ]\n",
    "        scatter_points = VGroup(*[Dot(point=point) for point in points])\n",
    "        \n",
    "        # Add axes, numbers, and labels to the scene\n",
    "        self.play(Create(axes), Write(x_label), Write(y_label), run_time=1)\n",
    "        self.wait(1)\n",
    "        self.play(FadeIn(scatter_points), run_time=1)\n",
    "        self.wait(2)\n",
    "        \n",
    "        # Use ValueTrackers for m and b (initially m = 0.5, b = 1)\n",
    "        m_tracker = ValueTracker(0.5)\n",
    "        b_tracker = ValueTracker(1)\n",
    "        \n",
    "        # Always redraw the line using current m and b values.\n",
    "        line = always_redraw(lambda: axes.plot(\n",
    "            lambda x: m_tracker.get_value() * x + b_tracker.get_value(),\n",
    "            x_range=[0, 20],\n",
    "            color=BLUE\n",
    "        ))\n",
    "        \n",
    "        self.play(Create(line), run_time=2)\n",
    "        \n",
    "        # Fixed label for the line equation below the x-axis. It updates automatically.\n",
    "        eq_label = always_redraw(lambda: MathTex(\n",
    "            \"y =\",\n",
    "            f\"{m_tracker.get_value():.1f}x\",\n",
    "            \"+\",\n",
    "            f\"{b_tracker.get_value():.1f}\"\n",
    "        ).next_to(axes, UP))\n",
    "        \n",
    "        self.play(Write(eq_label), run_time=1)\n",
    "        self.wait(2)\n",
    "        \n",
    "        # Animate changes in the line parameters (the equation updates automatically)\n",
    "        self.play(\n",
    "            m_tracker.animate.set_value(1.5),\n",
    "            b_tracker.animate.set_value(-2),\n",
    "            run_time=3\n",
    "        )\n",
    "        self.wait(2)\n",
    "        \n",
    "        self.play(\n",
    "            m_tracker.animate.set_value(-0.5),\n",
    "            b_tracker.animate.set_value(10),\n",
    "            run_time=3\n",
    "        )\n",
    "        self.wait(2)\n",
    "\n",
    "%manim -qh ScatterPlotWithMovingLineAndParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directional derivatives\n",
    "\n",
    "The MSE loss function is a function of two variables, $m$ and $b$, so we can visualize it's graph as a surface in 3D space. The gradient of our loss function, \n",
    "\\begin{equation*}\n",
    "    \\nabla J(m,b) = \\begin{bmatrix} \\; \\frac{\\partial J}{\\partial m} & \\frac{\\partial J}{\\partial b} \\; \\end{bmatrix},\n",
    "\\end{equation*}\n",
    "is a vector in the $(m,b)$-plane which captures the instantaneous rate of change of $J$ at any given point. But what exactly does this mean? \n",
    "\n",
    "Well, if we choose a particular point $\\mathbf{w}_0 = (m_0,b_0) \\in \\mathbb{R}^2$, then we can move in any direction on the plane. The direction is represented uniquely by a unit vector $\\mathbf{u} \\in \\mathbb{R}^2$. Then, the rate of change of $J$ in the direction of $\\mathbf{u}$ (also knows as the **directional derivative** of $J$ with respect to $\\mathbf{u}$ at $\\mathbf{w}_0$) is given by the dot product\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\mathbf{u}} J(\\mathbf{w}_0) & \\coloneqq \\nabla J(\\mathbf{w}_0) \\cdot \\mathbf{u} \\\\\n",
    "                                        & = \\begin{bmatrix} \\; \\frac{\\partial J}{\\partial m} & \\frac{\\partial J}{\\partial b} \\; \\end{bmatrix} \\cdot \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} \\\\\n",
    "                                        & = \\frac{\\partial J}{\\partial m} u_1 + \\frac{\\partial J}{\\partial b} u_2.\n",
    "\\end{align*}\n",
    "The notation above might be a little confusing; $\\nabla_{\\mathbf{u}} J(\\mathbf{w})$ should be thought of as a new function from $\\mathbb{R}^2$ to $\\mathbb{R}$, which takes in a point $\\mathbf{w} = \\mathbf{w}_0$ and outputs the directional derivative along $\\mathbf{u}$ at that point.\n",
    "The interpretation of the directional derivative is then very similar to the interpretation of the classical one-variable derivative: if we move from $\\mathbf{w}_0$ to $\\mathbf{w}_0 + \\epsilon \\mathbf{u}$, then the change in $J$ is approximately equal to the directional derivative $\\epsilon \\nabla_{\\mathbf{u}} J(\\mathbf{w}_0)$. Thus, the dot product is used to incorporate the direction of movement into the rate of change!\n",
    "\n",
    "### Gradients and minima\n",
    "Suppose we are at some point $\\mathbf{w}_0 \\in \\mathbb{R}^2$. Observe that as we vary our direction of movement $\\mathbf{u}$, the directional derivative $\\nabla J(\\mathbf{w}_0) \\cdot \\mathbf{u}$ can be positive, negative, or zero. In particular, we have\n",
    "\\begin{equation*}\n",
    "    - || \\nabla J(\\mathbf{w}_0)|| \\leq \\nabla_{\\mathbf{u}} J(\\mathbf{w}_0) \\leq ||\\nabla J(\\mathbf{w}_0)||.\n",
    "\\end{equation*}\n",
    "This follows from the fact that the dot product of two vectors is the product of their magnitudes, times the cosine of the angle between them. Thus:\n",
    "\n",
    "- If the angle between $\\nabla J(\\mathbf{w}_0)$ and $\\mathbf{u}$ is acute, then the directional derivative is positive. It is maximized when $\\mathbf{u}$ is in the same direction as $\\nabla J(\\mathbf{w}_0)$. In particular **the gradient $\\nabla J(\\mathbf{w}_0)$ points in the direction of steepest ascent of $J$ at $\\mathbf{w}_0$**. That is, it tells us the most *efficient way* to adjust the parameters $(b_0,m_0)$ so as to increase the loss $J$.\n",
    "- If the angle between $\\nabla J(\\mathbf{w}_0)$ and $\\mathbf{u}$ is obtuse, then the directional derivative is negative. It is minimized when $\\mathbf{u}$ is in the opposite direction as $\\nabla J(\\mathbf{w}_0)$. In particular **the gradient $\\nabla J(\\mathbf{w}_0)$ points in the direction of steepest descent of $J$ at $\\mathbf{w}_0$**. That is, it tells us the most *efficient way* to adjust the parameters $(b_0,m_0)$ so as to decrease the loss $J$. This is what we ultimately will use to train complicated models.\n",
    "\n",
    "As a consequence of the above facts, we can cook up a \"pure thought\" argument for why the gradient needs to be the zero vector at any local minimum or maximum of $J$. It goes as follows. Suppose $J$ attains a local minimum at the point $\\mathbf{w}_0$. Then, for *any* direction $\\mathbf{u}$, the directional derivative $\\nabla_{\\mathbf{u}} J(\\mathbf{w}_0)$ must be non-negative. Taking the reverse direction $-\\mathbf{u}$, we see that the directional derivative $\\nabla_{-\\mathbf{u}} J(\\mathbf{w}_0) = - \\nabla_{\\mathbf{u}} J(\\mathbf{w}_0)$ must also be non-negative. Thus, we have \n",
    "\\begin{equation*}\n",
    "    \\nabla_{\\mathbf{u}} J(\\mathbf{w}_0) = \\nabla J(\\mathbf{w}_0) \\cdot \\mathbf{u} = 0, \\quad \\textup{ for all } \\mathbf{u} \\in \\mathbb{R}^2.\n",
    "\\end{equation*}\n",
    "The only vector whose dot product with every other vector is zero is the zero vector itself. Thus, we conclude that at any local minimum $\\mathbf{w}_0$, the gradient $\\nabla J(\\mathbf{w}_0) = \\mathbf{0} \\in \\mathbb{R}^2$!\n",
    "\n",
    "### Visualizing the loss surface\n",
    "Below, we give a nice animation illustrating how the loss surface looks like, and how the negative gradient always points in the direction of steepest ascent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for the manim animation of J(m,b) goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the minimum using calculus\n",
    "Now, we are ready to find the minimum of the loss function $J(m,b)$. We will do this by setting the gradient $\\nabla J(m,b)$ to zero. This will yield a system of two linear equations in $m$ and $b$, and we will solve this to extract the desired values $\\hat{m}$ and $\\hat{b}$. It will be helpful to use some facts about partial derivatives of vectors and dot products.\n",
    "\n",
    "Suppose we have a vector-valued function $\\mathbf{f}: \\mathbb{R} \\to \\mathbb{R}^m$, defined by $\\mathbf{f}(w) = \\begin{bmatrix} f_1(w) \\\\ f_2(w) \\\\ \\vdots \\\\ f_m(w) \\end{bmatrix}^T$. Then, the **derivative of $\\mathbf{f}$ with respect to $w$** is defined to be the vector-valued function $d\\mathbf{f}/dw : \\mathbb{R} \\to \\mathbb{R}^m$ given by\n",
    "\\begin{equation*}\n",
    "    \\frac{d\\mathbf{f}}{dw} = \\begin{bmatrix} \\frac{df_1}{dw} \\\\ \\frac{df_2}{dw} \\\\ \\vdots \\\\ \\frac{df_m}{dw} \\end{bmatrix}^T.\n",
    "\\end{equation*}\n",
    "(NOTE: we implicitly assume here that all these derivatives exist, in which case we say that \"\\mathbf{f}$ is differentiable\".)\n",
    "Given two such functions $\\mathbf{f},\\mathbf{g}: \\mathbb{R} \\to \\mathbb{R}^m$, the **dot product of $\\mathbf{f}$ and $\\mathbf{g}$** is the scalar function $\\mathbf{f} \\cdot \\mathbf{g} : \\mathbb{R} \\to \\mathbb{R}$ given by\n",
    "\\begin{equation*}\n",
    "    (\\mathbf{f} \\cdot \\mathbf{g})(w) = f_1g_1(w) + \\dotsb + f_mg_m(w).\n",
    "\\end{equation*}\n",
    "Then, we have the following **product rule for dot products** (which you will prove in HW 1):\n",
    "\\begin{lem}\n",
    "    Let $\\mathbf{f},\\mathbf{g}: \\mathbb{R} \\to \\mathbb{R}^m$ be two differentiable vector-valued functions. Then, the derivative of the dot product $\\mathbf{f} \\cdot \\mathbf{g}$ is given by\n",
    "    \\begin{equation*}\n",
    "        \\frac{d}{dw} (\\mathbf{f} \\cdot \\mathbf{g}) = \\frac{d\\mathbf{f}}{dw} \\cdot \\mathbf{g} + \\mathbf{f} \\cdot \\frac{d\\mathbf{g}}{dw}.\n",
    "    \\end{equation*}\n",
    "    In particular:\n",
    "    \\begin{enumerate}\n",
    "        \\item We have $d ||\\mathbf{f}||^2 / dw = 2 \\mathbf{f} \\cdot d\\mathbf{f}/dw$.\n",
    "        \\item If $\\mathbf{g}$ is a constant vector, then $d(\\mathbf{f} \\cdot \\mathbf{g}) / dw = \\mathbf{g} \\cdot d\\mathbf{f}/dw$.\n",
    "    \\end{enumerate}\n",
    "\\end{lemma}\n",
    "\n",
    "Similarly, for a vector-valued function $\\mathbf{f}:\\mathbb{R}^k \\to \\mathbb{R}^m$ defined by $f(\\mathbf{w}) = \\begin{bmatrix} \\; f_1(\\mathbf{w}) \\dotsb f_m(\\mathbf{w}) \\; \\end{bmatrix}^T$ (where $\\mathbf{w} = (w_1,\\dotsc,w_n)$), the **partial derivative of $\\mathbf{f}$ with respect to $w_i$** is defined to be the vector-valued function $\\partial \\mathbf{f}/ \\partial w_i : \\mathbb{R} \\to \\mathbb{R}^m$ given by\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial \\mathbf{f}}{\\partial w_i} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial w_i} \\\\ \\frac{\\partial f_2}{\\partial w_i} \\\\ \\vdots \\\\ \\frac{\\partial f_m}{\\partial w_i} \\end{bmatrix}^T.\n",
    "\\end{equation*}\n",
    "Taking the dot product of these functions yields a scalar function $\\mathbf{f} \\cdot \\mathbf{g} : \\mathbb{R}^k \\to \\mathbb{R}$. Again, we have a similar lemma for partial derivatives of dot products (which you will also prove in HW 1):\n",
    "\\begin{lemma*}\n",
    "    Let $\\mathbf{f},\\mathbf{g}: \\mathbb{R}^k \\to \\mathbb{R}^m$ be two differentiable vector-valued functions. Then, the partial derivative of the dot product $\\mathbf{f} \\cdot \\mathbf{g}$ with respect to $w_i$ is given by\n",
    "    \\begin{equation*}\n",
    "        \\frac{\\partial}{\\partial w_i} (\\mathbf{f} \\cdot \\mathbf{g}) = \\frac{\\partial \\mathbf{f}}{\\partial w_i} \\cdot \\mathbf{g} + \\mathbf{f} \\cdot \\frac{\\partial \\mathbf{g}}{\\partial w_i}.\n",
    "    \\end{equation*}\n",
    "    In particular:\n",
    "    \\begin{enumerate}\n",
    "        \\item We have $\\partial ||\\mathbf{f}||^2 / \\partial w_i = 2 \\mathbf{f} \\cdot \\partial \\mathbf{f}/\\partial w_i$.\n",
    "        \\item If $\\mathbf{g}$ is a constant vector, then $\\partial(\\mathbf{f} \\cdot \\mathbf{g}) / \\partial w_i = \\mathbf{g} \\cdot \\partial \\mathbf{f}/\\partial w_i$.\n",
    "    \\end{enumerate}\n",
    "\\end{lemma*}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
