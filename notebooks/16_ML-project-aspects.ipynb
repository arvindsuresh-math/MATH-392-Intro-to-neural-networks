{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML project aspects I\n",
    "\n",
    "### Outline\n",
    "1. Bias-variance tradeoff\n",
    "2. Cross-validation (with stratification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Bias-variance tradeoff\n",
    "\n",
    "### Data quality\n",
    "Recall that in supervised learning, we have a training set of input-output pairs $(x,y)$ where $x$ is the input and $y$ is the output. The goal is to learn a ground truth function $\\mathbf{F}$ that maps each $x$ to the corresponding $y$.  There are two ways in which the quality/quantity of the data may affect the learning process:\n",
    "1. The dataset may be too small to fully capture the underlying \"true\" distribution of the data. This is sometimes referred to as *bias* in the dataset (because it might lead you to a biased version of the true distribution).\n",
    "2. The dataset may contain a lot of \"noise\", which refers to random errors or irrelevant information in the data that do not reflect true underlying patterns. This is sometimes referred to as *variance* in the dataset.\n",
    "\n",
    "### Fit and generalizability\n",
    "Recall that training a model involves finding the best parameters $\\mathbf{w}$ that minimize the loss function $\\mathcal{L}(\\mathbf{w})$. All this, of course, is measured relative to some fixed training set, which is why we refer to training as \"fitting\" the model to the data.\n",
    "\n",
    "There is a very very important distinction to be made between a model that is a \"good fit\" for the data, and a \"good model\". \n",
    "- A good fit means that the model has learned the underlying patterns in the training dataset, and it is able to make accurate predictions on that dataset.\n",
    "- A good model means that the model is able to make accurate predictions on *any* dataset (with the same features as the training set); in particular, it must also fare well on the test set. Thus, a good model is one that is a good fit for the \"true\" distribution of the data, not just the training set.\n",
    "\n",
    "The **generalizability** of a model is the ability to make accurate predictions on new, unseen data. A model that is able to generalize well is one that has learned the underlying patterns in the data, rather than just memorizing the training set. \n",
    "\n",
    "Thus, in ML, we want to strike a balance between two things: on the one hand, we want a model that achieves low training error (i.e., a good fit), and on the other hand, we want a model that achieves low test error (i.e., a good model). This leads to a fundamental tradeoff in ML known as the:\n",
    "\n",
    "### Bias-variance tradeoff\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between two sources of error that can affect the performance of a model:\n",
    "- **Bias**: Bias refers to the error introduced by approximating a real-world problem (which may be complex) by a simplified model. A model with high bias pays very little attention to the training data and oversimplifies the model, leading to high training and test errors. This is known as **underfitting**.\n",
    "- **Variance**: Variance refers to the error introduced by the model's sensitivity to small fluctuations in the training data. A model with high variance pays too much attention to the training data and captures noise as if it were a true pattern, leading to low training error but high test error. This is known as **overfitting**.\n",
    "\n",
    "With a little theory, the concepts of bias and variance can be made more precise. The bias-variance tradeoff can be understood in terms of the three components of error, two of which are bias and variance. The third is the **irreducible error**, which is the error that cannot be reduced by any model, and it is caused by noise in the data. The **total error** of a model can be decomposed into three components: bias, variance, and irreducible error. The goal of machine learning is to minimize the total error by finding the right balance between bias and variance. \n",
    "\n",
    "### Detecting overfitting and underfitting\n",
    "In practice, we diagnose **underfitting** and **overfitting** by comparing how well the model performs on the training set and the test (or validation) set. (We've talked about the test set before, we introduce the validation set later in this notebook.)\n",
    "- **Underfitting**: If the model performs poorly on both the training and test sets, it is likely underfitting. This means that the model is too simple to capture the underlying patterns in the data.\n",
    "- **Overfitting**: If the model performs well on the training set but poorly on the test set, it is likely overfitting. This means that the model has learned the noise in the training data rather than the underlying patterns. This is arguably more dangerous than underfitting, because if we let our guard down we might think we have a good model when in fact we don't!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation sets\n",
    "\n",
    "### Train-test split\n",
    "Recall that we split our dataset into a training set and a test set. The training set is used to train the model, while the test set is used to evaluate the model's performance. The split should be carried out before any training or evaluation and the test set should be kept safe and far away from the training process. All data preprocessing (e.g. standardizing) and feature engineering (e.g. polynomial interaction terms) should be done to the training set *after* the split, and the same transformations should be applied to the test set *only* at the time of evaluation. This is to ensure that the train set has no information about the test set, and vice versa.\n",
    "\n",
    "### A three-way split\n",
    "In fact, one often splits the original dataset into three parts:\n",
    "1. **Training set** — Used to train the model.\n",
    "2. **Validation set** — Used to tune hyperparameters and detect overfitting.\n",
    "3. **Test set** — Used only once, at the very end, to evaluate final model performance.\n",
    "\n",
    "Thus, a typical workflow would be:\n",
    "1. Split the original dataset into a training set and a test set.\n",
    "2. Split the training set into a (smaller) training set and a validation set.\n",
    "3. Fit the model to the training set.\n",
    "4. Evaluate (*not train*) the model on the validation set.\n",
    "5. Tune hyperparameters based on validation set performance.\n",
    "6. Repeat steps 3-5 until the model is satisfactory. (These can be thought of as the \"training\" steps.)\n",
    "7. Evaluate the final model on the test set. (This is the \"testing\" step.)\n",
    "\n",
    "### Why not just use the test set for validation?\n",
    "It's natural to avoid the hassle of a validation set and simply do the following:\n",
    "- You try different hyperparameters or model choices.\n",
    "- You evaluate each one on the test set.\n",
    "- You pick the one that performs best on the test set.\n",
    "\n",
    "The problem with this is that it *leaks* information from the test set into the model. By indirectly optimizing for the test set, you can end up with a model that performs well on the test set but poorly on new data. That is, it is possible the model has learned to \"cheat\" by memorizing the test set rather than learning the underlying patterns in the data. In other words, we are back to square one: we have a model that (may be) overfitting to the test set, and we have no way of knowing how well it will perform on new data!\n",
    "\n",
    "**Remark.** Here is a neat analogy provided by Chat GPT. Think of it like this:\n",
    "- **Training set**: Studying for an exam.\n",
    "- **Validation set**: Practice quizzes to decide how to study or what strategy works.\n",
    "- **Test set**: The final exam — you don’t want to have seen it before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "### What is it?\n",
    "If we start off with a small dataset, we may not have enough data to split into three sets. Indeed, the test set will (typically) take up 20\\% of the original dataset, and the validation set will take up 20\\% of the training set, leaving us with an actual training set of only 64\\% of the original dataset, which may be too small to train a good model.\n",
    "\n",
    "One way to get around this is to use **cross-validation**. Cross-validation is a technique for assessing how the results of a statistical analysis will generalize to an independent dataset. The basic idea is to:\n",
    "- split the dataset into $k$ subsets (or \"folds\"),\n",
    "- train the model on $k-1$ of the folds, and\n",
    "- use the remaining fold for validation.\n",
    "Thus, we repeat this process $k$ times, each time using a different fold for validation. NOTE: we are basically creating $k$ distinct training sets (with overlapping data points). \n",
    "\n",
    "After training and validating the model on all $k$ folds, we can average the validation scores to get an overall estimate of the model's performance. This is a more robust estimate than using a single validation set, as it takes into account the variability in the data. Moreover, it provides some confidence that the particular choice of validation set did not unduly influence the results.\n",
    "\n",
    "### Use cross-validation when:\n",
    "- Your dataset is small or moderately sized, and you want to make the most of your data.\n",
    "- You want a more reliable estimate of model performance.\n",
    "- You're comparing multiple models or hyperparameters and want to avoid picking one that performs well due to chance.\n",
    "- You're preparing for model selection before using a final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../midterm/data/presidential_election_dataset.csv')\n",
    "df_description = pd.read_csv('../midterm/data/data_dictionary.csv')\n",
    "\n",
    "# group features by category for easier access\n",
    "\n",
    "idx = df_description[df_description['category'] == 'id']['feature'].values.tolist()\n",
    "\n",
    "sex_age_edus = df_description[df_description['category'] == 'sex ~ age ~ education']['feature'].values.tolist()\n",
    "\n",
    "sex_age_races = df_description[df_description['category'] == 'sex ~ age ~ race']['feature'].values.tolist()\n",
    "\n",
    "sex_maritals = df_description[df_description['category'] == 'sex ~ marital status']['feature'].values.tolist()\n",
    "\n",
    "households = df_description[df_description['category'] == 'household']['feature'].values.tolist()\n",
    "\n",
    "labors = df_description[df_description['category'] == 'labor force']['feature'].values.tolist()\n",
    "\n",
    "nativities = df_description[df_description['category'] == 'nativity']['feature'].values.tolist()\n",
    "\n",
    "sexes = df_description[df_description['category'] == 'sex']['feature'].values.tolist()\n",
    "\n",
    "incomes = df_description[df_description['category'] == 'income']['feature'].values.tolist()\n",
    "\n",
    "targets = df_description[df_description['category'] == 'target']['feature'].values.tolist()\n",
    "\n",
    "# possible values of age, edu, race\n",
    "ages = ['18_24', \n",
    "        '25_34', \n",
    "        '35_44', \n",
    "        '45_64', \n",
    "        '65_plus']\n",
    "edus = ['less_than_9th', \n",
    "        'some_hs', \n",
    "        'hs_grad', \n",
    "        'some_college', \n",
    "        'associates', \n",
    "        'bachelors', \n",
    "        'graduate']\n",
    "races = ['black',\n",
    "         'white',\n",
    "         'aian',\n",
    "         'asian',\n",
    "         'nhpi',\n",
    "         'multi',\n",
    "         'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>Year of presidential popular election</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gisjoin</td>\n",
       "      <td>Geographic identifier for joining with other d...</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>state</td>\n",
       "      <td>State name</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>county</td>\n",
       "      <td>County name</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>persons_total</td>\n",
       "      <td>Persons: Total</td>\n",
       "      <td>Persons: total</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>persons_male</td>\n",
       "      <td>Persons: Male</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>persons_female</td>\n",
       "      <td>Persons: Female</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>persons_hispanic</td>\n",
       "      <td>Persons: Hispanic or Latino</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>households_total</td>\n",
       "      <td>Households: Total</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>male_never_married</td>\n",
       "      <td>Males ~ never married</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>male_married</td>\n",
       "      <td>Males ~ currently married</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>male_separated</td>\n",
       "      <td>Males ~ separated from spouse</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>male_widowed</td>\n",
       "      <td>Males ~ widowed</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>male_divorced</td>\n",
       "      <td>Males ~ divorced</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>female_never_married</td>\n",
       "      <td>Females ~ never married</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>female_married</td>\n",
       "      <td>Females ~ currently married</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>female_separated</td>\n",
       "      <td>Females ~ separated from spouse</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>female_widowed</td>\n",
       "      <td>Females ~ widowed</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>female_divorced</td>\n",
       "      <td>Females ~ divorced</td>\n",
       "      <td>sex ~ marital status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>persons_native</td>\n",
       "      <td>Persons: Native</td>\n",
       "      <td>nativity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature                                        description  \\\n",
       "0                   year              Year of presidential popular election   \n",
       "1                gisjoin  Geographic identifier for joining with other d...   \n",
       "2                  state                                         State name   \n",
       "3                 county                                        County name   \n",
       "4          persons_total                                     Persons: Total   \n",
       "5           persons_male                                      Persons: Male   \n",
       "6         persons_female                                    Persons: Female   \n",
       "7       persons_hispanic                        Persons: Hispanic or Latino   \n",
       "8       households_total                                  Households: Total   \n",
       "9     male_never_married                              Males ~ never married   \n",
       "10          male_married                          Males ~ currently married   \n",
       "11        male_separated                      Males ~ separated from spouse   \n",
       "12          male_widowed                                    Males ~ widowed   \n",
       "13         male_divorced                                   Males ~ divorced   \n",
       "14  female_never_married                            Females ~ never married   \n",
       "15        female_married                        Females ~ currently married   \n",
       "16      female_separated                    Females ~ separated from spouse   \n",
       "17        female_widowed                                  Females ~ widowed   \n",
       "18       female_divorced                                 Females ~ divorced   \n",
       "19        persons_native                                    Persons: Native   \n",
       "\n",
       "                category  \n",
       "0                   year  \n",
       "1                     id  \n",
       "2                     id  \n",
       "3                     id  \n",
       "4         Persons: total  \n",
       "5                    sex  \n",
       "6                    sex  \n",
       "7              ethnicity  \n",
       "8              household  \n",
       "9   sex ~ marital status  \n",
       "10  sex ~ marital status  \n",
       "11  sex ~ marital status  \n",
       "12  sex ~ marital status  \n",
       "13  sex ~ marital status  \n",
       "14  sex ~ marital status  \n",
       "15  sex ~ marital status  \n",
       "16  sex ~ marital status  \n",
       "17  sex ~ marital status  \n",
       "18  sex ~ marital status  \n",
       "19              nativity  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_description.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male_never_married</th>\n",
       "      <th>male_married</th>\n",
       "      <th>male_separated</th>\n",
       "      <th>male_widowed</th>\n",
       "      <th>male_divorced</th>\n",
       "      <th>female_never_married</th>\n",
       "      <th>female_married</th>\n",
       "      <th>female_separated</th>\n",
       "      <th>female_widowed</th>\n",
       "      <th>female_divorced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5553</td>\n",
       "      <td>11814</td>\n",
       "      <td>507</td>\n",
       "      <td>435</td>\n",
       "      <td>1998</td>\n",
       "      <td>5035</td>\n",
       "      <td>11707</td>\n",
       "      <td>538</td>\n",
       "      <td>2095</td>\n",
       "      <td>2721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16489</td>\n",
       "      <td>43600</td>\n",
       "      <td>1014</td>\n",
       "      <td>1928</td>\n",
       "      <td>6754</td>\n",
       "      <td>13167</td>\n",
       "      <td>43274</td>\n",
       "      <td>1404</td>\n",
       "      <td>7626</td>\n",
       "      <td>9065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4406</td>\n",
       "      <td>5588</td>\n",
       "      <td>410</td>\n",
       "      <td>285</td>\n",
       "      <td>1764</td>\n",
       "      <td>2679</td>\n",
       "      <td>5160</td>\n",
       "      <td>558</td>\n",
       "      <td>1426</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3015</td>\n",
       "      <td>5515</td>\n",
       "      <td>677</td>\n",
       "      <td>145</td>\n",
       "      <td>1208</td>\n",
       "      <td>1794</td>\n",
       "      <td>4229</td>\n",
       "      <td>122</td>\n",
       "      <td>1083</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5012</td>\n",
       "      <td>13933</td>\n",
       "      <td>389</td>\n",
       "      <td>571</td>\n",
       "      <td>2626</td>\n",
       "      <td>3661</td>\n",
       "      <td>13523</td>\n",
       "      <td>668</td>\n",
       "      <td>2916</td>\n",
       "      <td>2832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12407</th>\n",
       "      <td>5364</td>\n",
       "      <td>9284</td>\n",
       "      <td>315</td>\n",
       "      <td>604</td>\n",
       "      <td>1988</td>\n",
       "      <td>3544</td>\n",
       "      <td>8787</td>\n",
       "      <td>303</td>\n",
       "      <td>1524</td>\n",
       "      <td>2135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12408</th>\n",
       "      <td>3847</td>\n",
       "      <td>5355</td>\n",
       "      <td>97</td>\n",
       "      <td>54</td>\n",
       "      <td>1024</td>\n",
       "      <td>2855</td>\n",
       "      <td>5430</td>\n",
       "      <td>45</td>\n",
       "      <td>422</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12409</th>\n",
       "      <td>2241</td>\n",
       "      <td>4544</td>\n",
       "      <td>71</td>\n",
       "      <td>261</td>\n",
       "      <td>1013</td>\n",
       "      <td>1524</td>\n",
       "      <td>4453</td>\n",
       "      <td>146</td>\n",
       "      <td>484</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12410</th>\n",
       "      <td>837</td>\n",
       "      <td>1842</td>\n",
       "      <td>35</td>\n",
       "      <td>140</td>\n",
       "      <td>462</td>\n",
       "      <td>576</td>\n",
       "      <td>1767</td>\n",
       "      <td>20</td>\n",
       "      <td>290</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12411</th>\n",
       "      <td>912</td>\n",
       "      <td>1678</td>\n",
       "      <td>29</td>\n",
       "      <td>115</td>\n",
       "      <td>487</td>\n",
       "      <td>518</td>\n",
       "      <td>1457</td>\n",
       "      <td>54</td>\n",
       "      <td>385</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12412 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       male_never_married  male_married  male_separated  male_widowed  \\\n",
       "0                    5553         11814             507           435   \n",
       "1                   16489         43600            1014          1928   \n",
       "2                    4406          5588             410           285   \n",
       "3                    3015          5515             677           145   \n",
       "4                    5012         13933             389           571   \n",
       "...                   ...           ...             ...           ...   \n",
       "12407                5364          9284             315           604   \n",
       "12408                3847          5355              97            54   \n",
       "12409                2241          4544              71           261   \n",
       "12410                 837          1842              35           140   \n",
       "12411                 912          1678              29           115   \n",
       "\n",
       "       male_divorced  female_never_married  female_married  female_separated  \\\n",
       "0               1998                  5035           11707               538   \n",
       "1               6754                 13167           43274              1404   \n",
       "2               1764                  2679            5160               558   \n",
       "3               1208                  1794            4229               122   \n",
       "4               2626                  3661           13523               668   \n",
       "...              ...                   ...             ...               ...   \n",
       "12407           1988                  3544            8787               303   \n",
       "12408           1024                  2855            5430                45   \n",
       "12409           1013                  1524            4453               146   \n",
       "12410            462                   576            1767                20   \n",
       "12411            487                   518            1457                54   \n",
       "\n",
       "       female_widowed  female_divorced  \n",
       "0                2095             2721  \n",
       "1                7626             9065  \n",
       "2                1426             1290  \n",
       "3                1083             1308  \n",
       "4                2916             2832  \n",
       "...               ...              ...  \n",
       "12407            1524             2135  \n",
       "12408             422              887  \n",
       "12409             484             1289  \n",
       "12410             290              472  \n",
       "12411             385              304  \n",
       "\n",
       "[12412 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[sex_maritals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose some features, e.g. sex ~ marital status\n",
    "X = df[sex_maritals]\n",
    "\n",
    "# create a \"winner\" column with 1 if 'democrat' > 'republican', 0 otherwise\n",
    "y = df[targets].apply(lambda x: 1 if x['democrat'] > x['republican'] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12412, 10), (12412,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing individual folds using KFold:\n",
      "\n",
      "Fold 1:\n",
      "  Training indices (first 10): [ 1  2  4  5  6  7  9 10 11 12]\n",
      "  Test indices (first 10): [ 0  3  8 14 17 19 31 33 35 39]\n",
      "  X_train shape: (9929, 10)\n",
      "  X_test shape: (2483, 10) \n",
      "\n",
      "Fold 2:\n",
      "  Training indices (first 10): [0 1 2 3 4 5 6 7 8 9]\n",
      "  Test indices (first 10): [10 12 20 23 29 30 32 36 37 42]\n",
      "  X_train shape: (9929, 10)\n",
      "  X_test shape: (2483, 10) \n",
      "\n",
      "Fold 3:\n",
      "  Training indices (first 10): [0 1 2 3 4 5 6 7 8 9]\n",
      "  Test indices (first 10): [15 26 27 28 34 44 51 66 69 75]\n",
      "  X_train shape: (9930, 10)\n",
      "  X_test shape: (2482, 10) \n",
      "\n",
      "Fold 4:\n",
      "  Training indices (first 10): [ 0  1  3  4  5  8  9 10 11 12]\n",
      "  Test indices (first 10): [ 2  6  7 18 22 24 25 40 49 52]\n",
      "  X_train shape: (9930, 10)\n",
      "  X_test shape: (2482, 10) \n",
      "\n",
      "Fold 5:\n",
      "  Training indices (first 10): [ 0  2  3  6  7  8 10 12 14 15]\n",
      "  Test indices (first 10): [ 1  4  5  9 11 13 16 21 38 54]\n",
      "  X_train shape: (9930, 10)\n",
      "  X_test shape: (2482, 10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# -------------------------------\n",
    "# Example 1: Using KFold\n",
    "# -------------------------------\n",
    "print(\"Accessing individual folds using KFold:\\n\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through each fold generated by KFold\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "    # Display the first 10 training and test indices for brevity\n",
    "    print(\"  Training indices (first 10):\", train_index[:10])\n",
    "    print(\"  Test indices (first 10):\", test_index[:10])\n",
    "    \n",
    "    # Extract the training and test sets using the indices with .iloc for positional indexing\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Here you can train your model on X_train, y_train and evaluate on X_test, y_test\n",
    "    # For demonstration, we'll just print the shapes of these sets.\n",
    "    print(\"  X_train shape:\", X_train.shape)\n",
    "    print(\"  X_test shape:\", X_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing individual folds using StratifiedKFold:\n",
      "\n",
      "Fold 1:\n",
      "  Training indices (first 10): [ 0  4  7  8  9 11 12 14 15 16]\n",
      "  Test indices (first 10): [ 1  2  3  5  6 10 13 21 24 28]\n",
      "  X_train shape: (9929, 10)\n",
      "  X_test shape: (2483, 10) \n",
      "\n",
      "Fold 2:\n",
      "  Training indices (first 10): [ 0  1  2  3  4  5  6  9 10 12]\n",
      "  Test indices (first 10): [ 7  8 11 14 19 20 27 29 36 39]\n",
      "  X_train shape: (9929, 10)\n",
      "  X_test shape: (2483, 10) \n",
      "\n",
      "Fold 3:\n",
      "  Training indices (first 10): [0 1 2 3 4 5 6 7 8 9]\n",
      "  Test indices (first 10): [12 16 17 18 22 25 32 34 35 37]\n",
      "  X_train shape: (9930, 10)\n",
      "  X_test shape: (2482, 10) \n",
      "\n",
      "Fold 4:\n",
      "  Training indices (first 10): [ 0  1  2  3  5  6  7  8 10 11]\n",
      "  Test indices (first 10): [ 4  9 15 23 26 30 31 33 38 40]\n",
      "  X_train shape: (9930, 10)\n",
      "  X_test shape: (2482, 10) \n",
      "\n",
      "Fold 5:\n",
      "  Training indices (first 10): [ 1  2  3  4  5  6  7  8  9 10]\n",
      "  Test indices (first 10): [  0  45  62  71  78  85  91  96  99 103]\n",
      "  X_train shape: (9930, 10)\n",
      "  X_test shape: (2482, 10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Example 2: Using StratifiedKFold\n",
    "# -------------------------------\n",
    "print(\"Accessing individual folds using StratifiedKFold:\\n\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=43)\n",
    "\n",
    "# Iterate through each fold generated by StratifiedKFold\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(\"  Training indices (first 10):\", train_index[:10])\n",
    "    print(\"  Test indices (first 10):\", test_index[:10])\n",
    "    \n",
    "    # Extract training and test sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # For stratified folds, each test set will have a similar class distribution as the full dataset.\n",
    "    print(\"  X_train shape:\", X_train.shape)\n",
    "    print(\"  X_test shape:\", X_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also combine the cross-validations splits with the `cross_val_score` function from `sklearn`, which will automatically perform the cross-validation for you. This is a very useful function that allows you to evaluate the performance of a model using cross-validation without having to manually split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard 5-Fold CV (negative) log-loss: [-0.42081235 -0.41270505 -0.40442961 -0.41632738 -0.42678047]\n",
      "Mean (negative) log-loss: -0.41621097219701236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model = LogisticRegression(random_state=42,\n",
    "                           max_iter=1000)\n",
    "\n",
    "# KFold splits the data into 5 parts (folds) randomly. \n",
    "# It does not take the distribution of classes into account.\n",
    "# ----------------------------------------------------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate the model using cross_val_score which performs training\n",
    "# and validation in each fold and returns (negative) log-loss.\n",
    "scores_kf = cross_val_score(model,      # Logistic Regression model\n",
    "                            X_scaled,   # Scaled features\n",
    "                            y,          # Target variable\n",
    "                            cv=kf,      # KFold cross-validation object\n",
    "                            scoring='neg_log_loss'  # Use log-loss as the scoring metric\n",
    "                            )\n",
    "\n",
    "# Print the log-loss for each fold and the mean log-loss across all 5 folds.\n",
    "print(\"Standard 5-Fold CV (negative) log-loss:\", scores_kf)\n",
    "print(\"Mean (negative) log-loss:\", np.mean(scores_kf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified 5-Fold CV (negative) log-loss: [-0.42435809 -0.43003124 -0.40638417 -0.39803823 -0.41940113]\n",
      "Mean (negative) log-loss: -0.4156425732193837\n"
     ]
    }
   ],
   "source": [
    "# StratifiedKFold ensures that each fold has approximately the same \n",
    "# percentage of samples of each target class as the complete set.\n",
    "# This is particularly useful for binary or multi-class classification.\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "model = LogisticRegression(random_state=42,\n",
    "                           max_iter=1000)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate the model using stratified cross-validation.\n",
    "scores_skf = cross_val_score(model, X_scaled, y, cv=skf, scoring='neg_log_loss')\n",
    "\n",
    "# Print the accuracy scores for each stratified fold and the mean accuracy.\n",
    "print(\"\\nStratified 5-Fold CV (negative) log-loss:\", scores_skf)\n",
    "print(\"Mean (negative) log-loss:\", np.mean(scores_skf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
