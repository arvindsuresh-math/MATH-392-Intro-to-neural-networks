{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML project aspects I\n",
    "\n",
    "### Outline\n",
    "1. Bias-variance tradeoff\n",
    "2. Cross-validation (with stratification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Bias-variance tradeoff\n",
    "\n",
    "### Data quality\n",
    "Recall that in supervised learning, we have a training set of input-output pairs $(x,y)$ where $x$ is the input and $y$ is the output. The goal is to learn a ground truth function $\\mathbf{F}$ that maps each $x$ to the corresponding $y$.  There are two ways in which the quality/quantity of the data may affect the learning process:\n",
    "1. The dataset may be too small to fully capture the underlying \"true\" distribution of the data. This is sometimes referred to as *bias* in the dataset (because it might lead you to a biased version of the true distribution).\n",
    "2. The dataset may contain a lot of \"noise\", which refers to random errors or irrelevant information in the data that do not reflect true underlying patterns. This is sometimes referred to as *variance* in the dataset.\n",
    "\n",
    "### Fit and generalizability\n",
    "Recall that training a model involves finding the best parameters $\\mathbf{w}$ that minimize the loss function $\\mathcal{L}(\\mathbf{w})$. All this, of course, is measured relative to some fixed training set, which is why we refer to training as \"fitting\" the model to the data.\n",
    "\n",
    "There is a very very important distinction to be made between a model that is a \"good fit\" for the data, and a \"good model\". \n",
    "- A good fit means that the model has learned the underlying patterns in the training dataset, and it is able to make accurate predictions on that dataset.\n",
    "- A good model means that the model is able to make accurate predictions on *any* dataset (with the same features as the training set); in particular, it must also fare well on the test set. Thus, a good model is one that is a good fit for the \"true\" distribution of the data, not just the training set.\n",
    "\n",
    "The **generalizability** of a model is the ability to make accurate predictions on new, unseen data. A model that is able to generalize well is one that has learned the underlying patterns in the data, rather than just memorizing the training set. \n",
    "\n",
    "Thus, in ML, we want to strike a balance between two things: on the one hand, we want a model that achieves low training error (i.e., a good fit), and on the other hand, we want a model that achieves low test error (i.e., a good model). This leads to a fundamental tradeoff in ML known as the:\n",
    "\n",
    "### Bias-variance tradeoff\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between two sources of error that can affect the performance of a model:\n",
    "- **Bias**: Bias refers to the error introduced by approximating a real-world problem (which may be complex) by a simplified model. A model with high bias pays very little attention to the training data and oversimplifies the model, leading to high training and test errors. This is known as **underfitting**.\n",
    "- **Variance**: Variance refers to the error introduced by the model's sensitivity to small fluctuations in the training data. A model with high variance pays too much attention to the training data and captures noise as if it were a true pattern, leading to low training error but high test error. This is known as **overfitting**.\n",
    "\n",
    "With a little theory, the concepts of bias and variance can be made more precise. The bias-variance tradeoff can be understood in terms of the three components of error, two of which are bias and variance. The third is the **irreducible error**, which is the error that cannot be reduced by any model, and it is caused by noise in the data. The **total error** of a model can be decomposed into three components: bias, variance, and irreducible error. The goal of machine learning is to minimize the total error by finding the right balance between bias and variance. \n",
    "\n",
    "### Detecting overfitting and underfitting\n",
    "In practice, we diagnose **underfitting** and **overfitting** by comparing how well the model performs on the training set and the test (or validation) set. (We've talked about the test set before, we introduce the validation set later in this notebook.)\n",
    "- **Underfitting**: If the model performs poorly on both the training and test sets, it is likely underfitting. This means that the model is too simple to capture the underlying patterns in the data.\n",
    "- **Overfitting**: If the model performs well on the training set but poorly on the test set, it is likely overfitting. This means that the model has learned the noise in the training data rather than the underlying patterns. This is arguably more dangerous than underfitting, because if we let our guard down we might think we have a good model when in fact we don't!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation sets\n",
    "\n",
    "### Train-test split\n",
    "Recall that we split our dataset into a training set and a test set. The training set is used to train the model, while the test set is used to evaluate the model's performance. The split should be carried out before any training or evaluation and the test set should be kept safe and far away from the training process. All data preprocessing (e.g. standardizing) and feature engineering (e.g. polynomial interaction terms) should be done to the training set *after* the split, and the same transformations should be applied to the test set *only* at the time of evaluation. This is to ensure that the train set has no information about the test set, and vice versa.\n",
    "\n",
    "### A three-way split\n",
    "In fact, one often splits the original dataset into three parts:\n",
    "1. **Training set** — Used to train the model.\n",
    "2. **Validation set** — Used to tune hyperparameters and detect overfitting.\n",
    "3. **Test set** — Used only once, at the very end, to evaluate final model performance.\n",
    "\n",
    "Thus, a typical workflow would be:\n",
    "1. Split the original dataset into a training set and a test set.\n",
    "2. Split the training set into a (smaller) training set and a validation set.\n",
    "3. Fit the model to the training set.\n",
    "4. Evaluate (*not train*) the model on the validation set.\n",
    "5. Tune hyperparameters based on validation set performance.\n",
    "6. Repeat steps 3-5 until the model is satisfactory. (These can be thought of as the \"training\" steps.)\n",
    "7. Evaluate the final model on the test set. (This is the \"testing\" step.)\n",
    "\n",
    "### Why not just use the test set for validation?\n",
    "It's natural to avoid the hassle of a validation set and simply do the following:\n",
    "- You try different hyperparameters or model choices.\n",
    "- You evaluate each one on the test set.\n",
    "- You pick the one that performs best on the test set.\n",
    "\n",
    "The problem with this is that it *leaks* information from the test set into the model. By indirectly optimizing for the test set, you can end up with a model that performs well on the test set but poorly on new data. That is, it is possible the model has learned to \"cheat\" by memorizing the test set rather than learning the underlying patterns in the data. In other words, we are back to square one: we have a model that (may be) overfitting to the test set, and we have no way of knowing how well it will perform on new data!\n",
    "\n",
    "**Remark.** Here is a neat analogy provided by Chat GPT. Think of it like this:\n",
    "- **Training set**: Studying for an exam.\n",
    "- **Validation set**: Practice quizzes to decide how to study or what strategy works.\n",
    "- **Test set**: The final exam — you don’t want to have seen it before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "### What is it?\n",
    "If we start off with a small dataset, we may not have enough data to split into three sets. Indeed, the test set will (typically) take up 20\\% of the original dataset, and the validation set will take up 20\\% of the training set, leaving us with an actual training set of only 64\\% of the original dataset, which may be too small to train a good model.\n",
    "\n",
    "One way to get around this is to use **cross-validation**. Cross-validation is a technique for assessing how the results of a statistical analysis will generalize to an independent dataset. The basic idea is to:\n",
    "- split the dataset into $k$ subsets (or \"folds\"),\n",
    "- train the model on $k-1$ of the folds, and\n",
    "- use the remaining fold for validation.\n",
    "Thus, we repeat this process $k$ times, each time using a different fold for validation. NOTE: we are basically creating $k$ distinct training sets (with overlapping data points). \n",
    "\n",
    "After training and validating the model on all $k$ folds, we can average the validation scores to get an overall estimate of the model's performance. This is a more robust estimate than using a single validation set, as it takes into account the variability in the data. Moreover, it provides some confidence that the particular choice of validation set did not unduly influence the results.\n",
    "\n",
    "### Use cross-validation when:\n",
    "- Your dataset is small or moderately sized, and you want to make the most of your data.\n",
    "- You want a more reliable estimate of model performance.\n",
    "- You're comparing multiple models or hyperparameters and want to avoid picking one that performs well due to chance.\n",
    "- You're preparing for model selection before using a final test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../midterm/data/presidential_election_dataset.csv')\n",
    "df_description = pd.read_csv('../midterm/data/data_dictionary.csv')\n",
    "\n",
    "# group features by category for easier access\n",
    "idx = df_description[df_description['category'] == 'id']['feature'].values.tolist()\n",
    "sex_age_edus = df_description[df_description['category'] == 'sex ~ age ~ education']['feature'].values.tolist()\n",
    "sex_age_races = df_description[df_description['category'] == 'sex ~ age ~ race']['feature'].values.tolist()\n",
    "sex_maritals = df_description[df_description['category'] == 'sex ~ marital status']['feature'].values.tolist()\n",
    "households = df_description[df_description['category'] == 'household']['feature'].values.tolist()\n",
    "labors = df_description[df_description['category'] == 'labor force']['feature'].values.tolist()\n",
    "nativities = df_description[df_description['category'] == 'nativity']['feature'].values.tolist()\n",
    "sexes = df_description[df_description['category'] == 'sex']['feature'].values.tolist()\n",
    "incomes = df_description[df_description['category'] == 'income']['feature'].values.tolist()\n",
    "targets = df_description[df_description['category'] == 'target']['feature'].values.tolist()\n",
    "\n",
    "# possible values of age, edu, race\n",
    "ages = ['18_24', \n",
    "        '25_34', \n",
    "        '35_44', \n",
    "        '45_64', \n",
    "        '65_plus']\n",
    "edus = ['less_than_9th', \n",
    "        'some_hs', \n",
    "        'hs_grad', \n",
    "        'some_college', \n",
    "        'associates', \n",
    "        'bachelors', \n",
    "        'graduate']\n",
    "races = ['black',\n",
    "         'white',\n",
    "         'aian',\n",
    "         'asian',\n",
    "         'nhpi',\n",
    "         'multi',\n",
    "         'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose some features, e.g. sex ~ marital status\n",
    "X = df[sex_maritals]\n",
    "\n",
    "# create a \"winner\" column with 1 if 'democrat' > 'republican', 0 otherwise\n",
    "y = df[targets].apply(lambda x: 1 if x['democrat'] > x['republican'] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing individual folds using KFold:\n",
      "\n",
      "Fold 1:\n",
      "  Training indices (first 10): [ 1  2  4  5  6  7  9 10 11 12]\n",
      "  Test indices (first 10): [ 0  3  8 14 17 19 31 33 35 39]\n",
      "  X_train shape: (9929, 10)\n",
      "  X_test shape: (2483, 10) \n",
      "\n",
      "Fold 2:\n",
      "  Training indices (first 10): [0 1 2 3 4 5 6 7 8 9]\n",
      "  Test indices (first 10): [10 12 20 23 29 30 32 36 37 42]\n",
      "  X_train shape: (9929, 10)\n",
      "  X_test shape: (2483, 10) \n",
      "\n",
      "Fold 3:\n",
      "  Training indices (first 10): [0 1 2 3 4 5 6 7 8 9]\n",
      "  Test indices (first 10): [15 26 27 28 34 44 51 66 69 75]\n",
      "  X_train shape: (9930, 10)\n",
      "  X_test shape: (2482, 10) \n",
      "\n",
      "Fold 4:\n",
      "  Training indices (first 10): [ 0  1  3  4  5  8  9 10 11 12]\n",
      "  Test indices (first 10): [ 2  6  7 18 22 24 25 40 49 52]\n",
      "  X_train shape: (9930, 10)\n",
      "  X_test shape: (2482, 10) \n",
      "\n",
      "Fold 5:\n",
      "  Training indices (first 10): [ 0  2  3  6  7  8 10 12 14 15]\n",
      "  Test indices (first 10): [ 1  4  5  9 11 13 16 21 38 54]\n",
      "  X_train shape: (9930, 10)\n",
      "  X_test shape: (2482, 10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# -------------------------------\n",
    "# Example 1: Using KFold\n",
    "# -------------------------------\n",
    "print(\"Accessing individual folds using KFold:\\n\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through each fold generated by KFold\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "    # Display the first 10 training and test indices for brevity\n",
    "    print(\"  Training indices (first 10):\", train_index[:10])\n",
    "    print(\"  Test indices (first 10):\", test_index[:10])\n",
    "    \n",
    "    # Extract the training and test sets using the indices with .iloc for positional indexing\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Here you can train your model on X_train, y_train and evaluate on X_test, y_test\n",
    "    # For demonstration, we'll just print the shapes of these sets.\n",
    "    print(\"  X_train shape:\", X_train.shape)\n",
    "    print(\"  X_test shape:\", X_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Example 2: Using StratifiedKFold\n",
    "# -------------------------------\n",
    "print(\"Accessing individual folds using StratifiedKFold:\\n\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate through each fold generated by StratifiedKFold\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(\"  Training indices (first 10):\", train_index[:10])\n",
    "    print(\"  Test indices (first 10):\", test_index[:10])\n",
    "    \n",
    "    # Extract training and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # For stratified folds, each test set will have a similar class distribution as the full dataset.\n",
    "    print(\"  X_train shape:\", X_train.shape)\n",
    "    print(\"  X_test shape:\", X_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also combine the cross-validations splits with the `cv_score` function from `sklearn`, which will automatically perform the cross-validation for you. This is a very useful function that allows you to evaluate the performance of a model using cross-validation without having to manually split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard 5-Fold CV (negative) log-loss: [-0.42081235 -0.41270505 -0.40442961 -0.41632738 -0.42678047]\n",
      "Mean (negative) log-loss: -0.41621097219701236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model = LogisticRegression(random_state=42,\n",
    "                           max_iter=1000)\n",
    "\n",
    "# KFold splits the data into 5 parts (folds) randomly. \n",
    "# It does not take the distribution of classes into account.\n",
    "# ----------------------------------------------------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate the model using cross_val_score which performs training\n",
    "# and validation in each fold and returns (negative) log-loss.\n",
    "scores_kf = cross_val_score(model, X_scaled, y, cv=kf, scoring='neg_log_loss')\n",
    "\n",
    "# Print the log-loss for each fold and the mean log-loss across all 5 folds.\n",
    "print(\"Standard 5-Fold CV (negative) log-loss:\", scores_kf)\n",
    "print(\"Mean (negative) log-loss:\", np.mean(scores_kf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified 5-Fold CV (negative) log-loss: [-0.42435809 -0.43003124 -0.40638417 -0.39803823 -0.41940113]\n",
      "Mean (negative) log-loss: -0.4156425732193837\n"
     ]
    }
   ],
   "source": [
    "# StratifiedKFold ensures that each fold has approximately the same \n",
    "# percentage of samples of each target class as the complete set.\n",
    "# This is particularly useful for binary or multi-class classification.\n",
    "# ----------------------------------------------------------------------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate the model using stratified cross-validation.\n",
    "scores_skf = cross_val_score(model, X_scaled, y, cv=skf, scoring='neg_log_loss')\n",
    "\n",
    "# Print the accuracy scores for each stratified fold and the mean accuracy.\n",
    "print(\"\\nStratified 5-Fold CV (negative) log-loss:\", scores_skf)\n",
    "print(\"Mean (negative) log-loss:\", np.mean(scores_skf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
