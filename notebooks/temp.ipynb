{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron Learning Algorithm\n",
    "\n",
    "The Perceptron Learning Algorithm is a fundamental method for training a simple binary classifier. It is an iterative method that updates the weights of a perceptron based on errors made in classification. Below is a detailed breakdown of how it works.\n",
    "\n",
    "#### 1. Setup and Notation\n",
    "\n",
    "We have:\n",
    "- **Input features**: $\\mathbf{x}$, where each sample $\\mathbf{x}_i$ has $n$ features.\n",
    "- **Binary labels**: $y$ (we use -1 and 1 instead of 0 and 1 for mathematical convenience).\n",
    "- **Weights**: $\\mathbf{w}$, initialized to zeros or small random values.\n",
    "- **Bias**: $b$, initialized to zero.\n",
    "\n",
    "A perceptron makes predictions using a linear decision rule:\n",
    "\n",
    "$$\\hat{y} = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{w} \\cdot \\mathbf{x}$ is the dot product of the weight vector and input features.\n",
    "- $b$ is the bias term (optional but useful).\n",
    "- The sign function returns +1 if the result is positive, and -1 otherwise.\n",
    "\n",
    "#### 2. Learning Algorithm\n",
    "\n",
    "**Step 1: Initialize Weights and Bias**\n",
    "- Set all weights $\\mathbf{w}$ to zero (or small random values).\n",
    "- Set bias $b$ to zero.\n",
    "\n",
    "**Step 2: Iterate Over the Dataset**\n",
    "\n",
    "For each training sample $(\\mathbf{x}_i, y_i)$:\n",
    "1. **Compute the Predicted Output**\n",
    "\n",
    "$$\\hat{y}_i = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x}_i + b)$$\n",
    "\n",
    "- If $\\hat{y}_i = y_i$, the prediction is correct, and we do nothing.\n",
    "- If $\\hat{y}_i \\neq y_i$, the prediction is incorrect, so we update the weights.\n",
    "\n",
    "2. **Update the Weights and Bias**\n",
    "\n",
    "When the perceptron makes a mistake, update the weights and bias using:\n",
    "\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y_i \\mathbf{x}_i$$\n",
    "$$b \\leftarrow b + \\eta y_i$$\n",
    "\n",
    "- $\\eta$ is the learning rate (typically a small value).\n",
    "\n",
    "**Step 3: Repeat Until Convergence**\n",
    "\n",
    "- Continue updating until all points are correctly classified or a maximum number of iterations is reached.\n",
    "\n",
    "#### 3. Intuition Behind Weight Updates\n",
    "\n",
    "Why does the update rule work?\n",
    "- If the perceptron misclassifies a point $(\\mathbf{x}_i, y_i)$, then the dot product $\\mathbf{w} \\cdot \\mathbf{x}_i$ has the wrong sign.\n",
    "- The update moves $\\mathbf{w}$ closer to the correct classification direction.\n",
    "- Over time, the updates shift the decision boundary until it correctly separates the data.\n",
    "\n",
    "#### 4. Example Walkthrough\n",
    "\n",
    "Imagine we have two features ($x_1$ and $x_2$) and a dataset with two classes (-1 and 1):\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$ |\n",
    "|-------|-------|-----|\n",
    "| 2     | 3     | 1   |\n",
    "| -1    | -2    | -1  |\n",
    "| 1     | 1     | 1   |\n",
    "| -2    | -1    | -1  |\n",
    "\n",
    "Initial Weights:\n",
    "\n",
    "Let's assume $\\mathbf{w} = [0, 0]$ and $b = 0$.\n",
    "\n",
    "First Sample (2,3), $y = 1$\n",
    "\n",
    "Prediction:\n",
    "$$\\hat{y} = \\text{sign}(0 \\cdot 2 + 0 \\cdot 3 + 0) = 0,$$\n",
    "and\n",
    "$\\hat{y} \\neq y,$ so update:\n",
    "\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y \\mathbf{x} = [0, 0] + 1 \\cdot 1 \\cdot [2, 3] = [2, 3]$$\n",
    "$$b \\leftarrow b + \\eta y = 0 + 1 \\cdot 1 = 1$$\n",
    "\n",
    "Second Sample (-1, -2), $y = -1$\n",
    "\n",
    "$$\\hat{y} = \\text{sign}(2 \\cdot -1 + 3 \\cdot -2 + 1) = \\text{sign}(-8) = -1$$\n",
    "Since $\\hat{y} = y$, no update.\n",
    "\n",
    "Continue Updating Until Convergence\n",
    "\n",
    "This process repeats until all points are correctly classified.\n",
    "\n",
    "#### 5. Convergence and Limitations\n",
    "\n",
    "- **Guarantees**: If the data is linearly separable, the perceptron will eventually find a separating hyperplane.\n",
    "- **Limitations**:\n",
    "    - The perceptron cannot solve non-linearly separable problems (e.g., XOR problem).\n",
    "    - If the data is not separable, it will never converge and keep updating indefinitely.\n",
    "    - The perceptron does not model probabilitiesâ€”it only outputs hard decisions (-1 or 1).\n",
    "\n",
    "#### 6. Summary\n",
    "1. Initialize weights and bias to zero.\n",
    "2. For each training sample:\n",
    "     - Compute prediction using the sign function.\n",
    "     - If misclassified, update weights and bias.\n",
    "3. Repeat until convergence or a maximum number of epochs.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
