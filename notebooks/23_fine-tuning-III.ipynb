{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0787e2e4",
   "metadata": {},
   "source": [
    "## Lecture 3: Full Evaluation, Augmentation, and Fine-Tuning\n",
    "\n",
    "**Recap from Lecture 2 & Homework:**\n",
    "* We successfully loaded the Oxford-IIIT Pet dataset with proper normalization and created DataLoaders.\n",
    "* We loaded pre-trained MobileNetV3 and ResNet18 models and adapted their final layers for our 37-class problem.\n",
    "* We implemented the \"Feature Extraction\" training strategy (training only the classifier head) and ran it for both MobileNetV3 (in lecture) and ResNet18 (in homework).\n",
    "* We visualized the basic training/validation loss and accuracy curves.\n",
    "\n",
    "**Goals for Today (Lecture 3):**\n",
    "1.  *Evaluation:* Calculate Precision, Recall, F1-score, and Confusion Matrix.\n",
    "2.  *Data Augmentation* to improve model generalization.\n",
    "3.  *Deeper fine-Tuning:* Unfreeze some pre-trained layers.\n",
    "4.  *Differential Learning Rates* for the optimizer.\n",
    "5.  Run a demonstration of fine-tuning with augmentation for one model.\n",
    "6.  Visualize and evaluate the results more thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8939862",
   "metadata": {},
   "source": [
    "### Comprehensive Evaluation Metrics\n",
    "\n",
    "There are several important metrics to evaluate a multi-class classification model like ours: First, recall the following:\n",
    "* **Accuracy:** The overall proportion of correct predictions across all classes. It is the simplest metric but can be misleading if the dataset is imbalanced.\n",
    "* **True Positives (TP):** The number of instances correctly predicted as a specific class.\n",
    "* **False Positives (FP):** The number of instances incorrectly predicted as a specific class (predicted as class A but actually class B).\n",
    "* **True Negatives (TN):** The number of instances correctly predicted as not being a specific class.\n",
    "* **False Negatives (FN):** The number of instances that were not predicted as a specific class but actually belong to that class.\n",
    "\n",
    "Using these, we can derive several key metrics for each class:\n",
    "\n",
    "* **Precision:** For a given class, what proportion of predictions for that class were actually correct? (TP / (TP + FP)). High precision means the model is trustworthy when it predicts that class.\n",
    "* **Recall (Sensitivity):** For a given class, what proportion of true instances of that class did the model correctly identify? (TP / (TP + FN)). High recall means the model finds most instances of that class.\n",
    "* **F1-Score:** The harmonic mean of Precision and Recall (2 * (Precision * Recall) / (Precision + Recall)). It provides a single score balancing both concerns. Often reported as a macro-average (unweighted mean across classes) or weighted-average (weighted by class support).\n",
    "* **Confusion Matrix:** A table showing the counts of true vs. predicted classes. The diagonal represents correct predictions, while off-diagonal elements show misclassifications (e.g., how many times 'Siamese' was predicted when the true label was 'Birman').\n",
    "\n",
    "We will use the `scikit-learn` library to easily compute these metrics. First, we need our validation function to return the raw predictions and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import time # Keep time for consistency if needed later\n",
    "\n",
    "# set device\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Fallback to CUDA then CPU\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265aba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Modify the validation helper function to return predictions and labels\n",
    "\n",
    "# We modify the previous 'validate' function to return lists of predictions and labels\n",
    "def validate_and_collect_preds(model, dataloader, criterion, device):\n",
    "    \"\"\"Performs one epoch of validation and returns metrics AND predictions/labels.\"\"\"\n",
    "    model.eval() # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # No gradients needed for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "            # Collect predictions and labels for detailed metrics\n",
    "            all_preds.extend(preds.cpu().numpy()) # Move preds to CPU and convert to numpy\n",
    "            all_labels.extend(labels.cpu().numpy()) # Move labels to CPU and convert to numpy\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects.double() / total_samples # Keep accuracy calculation here too\n",
    "\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels # Return collected lists\n",
    "\n",
    "print(\"Modified validation function 'validate_and_collect_preds' defined.\")\n",
    "# We will use this function in our training loop later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Define a function to calculate and print detailed metrics using scikit-learn\n",
    "\n",
    "import pandas as pd # For better confusion matrix display\n",
    "import seaborn as sns # For plotting confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_and_display_metrics(y_true, y_pred, class_names):\n",
    "    \"\"\"Calculates and displays accuracy, precision, recall, F1, and confusion matrix.\"\"\"\n",
    "\n",
    "    print(\"\\n--- Detailed Validation Metrics ---\")\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Overall Accuracy : {accuracy:.4f}\")\n",
    "\n",
    "    # Calculate precision, recall, F1-score (weighted average)\n",
    "    # 'weighted' accounts for class imbalance\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"Weighted Precision: {precision:.4f}\")\n",
    "    print(f\"Weighted Recall  : {recall:.4f}\")\n",
    "    print(f\"Weighted F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Optional: Print per-class metrics (can be long for 37 classes)\n",
    "    # report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n",
    "    # print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    # Calculate and display Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Display using pandas for better formatting (optional: use seaborn for heatmap)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # Print only if not too large, or consider saving to file / using heatmap\n",
    "    if len(class_names) <= 40: # Heuristic to avoid overwhelming output\n",
    "       # Print with adjusted display options\n",
    "       with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "            print(cm_df)\n",
    "       # Optional: Plot heatmap for better visualization\n",
    "       # plt.figure(figsize=(15, 12))\n",
    "       # sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "       # plt.ylabel('Actual')\n",
    "       # plt.xlabel('Predicted')\n",
    "       # plt.title('Confusion Matrix Heatmap')\n",
    "       # plt.show()\n",
    "    else:\n",
    "        print(\"(Confusion matrix is large, displaying as text might be truncated)\")\n",
    "        print(cm_df) # Still print, but acknowledge it might be hard to read\n",
    "\n",
    "print(\"Function 'calculate_and_display_metrics' defined.\")\n",
    "# We will call this function after the validation epoch in the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88791afc",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Training deep learning models often requires large amounts of data to prevent overfitting and improve generalization. When our specific dataset is moderately sized (like the Pet dataset), **Data Augmentation** becomes crucial.\n",
    "\n",
    "It involves applying random transformations to the training images during loading. This artificially expands the dataset, forcing the model to learn features that are robust to variations in appearance, position, lighting, etc.\n",
    "\n",
    "Common Augmentation Techniques for Images:\n",
    "* **Random Resized Crop:** Crops a random part of the image and resizes it. Helps model focus on different parts and handle scale variations.\n",
    "* **Random Horizontal Flip:** Flips the image horizontally with a 50% probability. Useful for many object classes (like pets) where left-right orientation doesn't change the class.\n",
    "* **Color Jitter:** Randomly changes brightness, contrast, saturation, and hue. Makes the model less sensitive to lighting conditions.\n",
    "* **Random Rotation:** Rotates the image by a random angle.\n",
    "* **Cutout / Random Erasing:** Randomly removes rectangular patches from the image, forcing the model to use more context.\n",
    "\n",
    "**Important:** Augmentations are typically applied *only* to the training data. Validation and test data should use fixed transformations (like resize and normalize) for consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Define transforms with augmentation and update datasets/dataloaders\n",
    "\n",
    "# (Re-import necessary libraries)\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision # Ensure torchvision is available\n",
    "\n",
    "# Use the same ImageNet stats as before\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "image_size = 224 # Standard size for these models\n",
    "\n",
    "# --- Define NEW Transforms including Augmentation for Training ---\n",
    "augmented_data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)), # Crop a random portion and resize\n",
    "        transforms.RandomHorizontalFlip(),   # Randomly flip images horizontally\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Optional: Add color jitter\n",
    "        transforms.RandomRotation(15), # Optional: Add random rotation\n",
    "        transforms.ToTensor(),               # Convert to Tensor MUST be before Normalize\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std) # Normalize\n",
    "    ]),\n",
    "    # Validation transform remains simple: Resize, ToTensor, Normalize\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)), # Ensure consistent size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std)\n",
    "    ]),\n",
    "}\n",
    "print(\"Defined data transforms with augmentation for training.\")\n",
    "\n",
    "# --- Reload Datasets and Create DataLoaders with Augmented Transforms ---\n",
    "data_root = './data'\n",
    "print(f\"\\nReloading datasets from {data_root} with new transforms...\")\n",
    "\n",
    "# Apply augmented transforms to the training dataset\n",
    "train_dataset_aug = torchvision.datasets.OxfordIIITPet(\n",
    "    root=data_root,\n",
    "    split='trainval',\n",
    "    download=False, # Should be downloaded already\n",
    "    transform=augmented_data_transforms['train'] # Use the augmented transforms\n",
    ")\n",
    "\n",
    "# Validation dataset uses non-augmented transforms\n",
    "val_dataset_aug = torchvision.datasets.OxfordIIITPet(\n",
    "    root=data_root,\n",
    "    split='test',\n",
    "    download=False,\n",
    "    transform=augmented_data_transforms['val'] # Use the simple validation transforms\n",
    ")\n",
    "\n",
    "print(f\"Augmented training dataset size: {len(train_dataset_aug)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset_aug)}\")\n",
    "num_classes = len(train_dataset_aug.classes) # Should still be 37\n",
    "class_names = train_dataset_aug.classes # Get class names for metric display later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate DataLoaders\n",
    "batch_size = 32 # Keep consistent or adjust if needed\n",
    "dataloaders_aug = {\n",
    "    'train': DataLoader(train_dataset_aug, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "    'val': DataLoader(val_dataset_aug, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "}\n",
    "dataset_sizes_aug = {'train': len(train_dataset_aug), 'val': len(val_dataset_aug)}\n",
    "\n",
    "print(f\"\\nDataLoaders recreated with augmented training data.\")\n",
    "# Now we'll use 'dataloaders_aug' for the fine-tuning training run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62fbbe",
   "metadata": {},
   "source": [
    "### Fine-Tuning Strategy: Unfreezing Layers\n",
    "\n",
    "Feature extraction (training only the head) gave us a good baseline. Now, we'll perform fine-tuning to potentially improve performance further.\n",
    "\n",
    "The idea is to unfreeze some of the later layers in the pre-trained base model and train them alongside the classifier head. These later layers often capture more complex and potentially dataset-specific features compared to the very general features (edges, textures) learned in early layers.\n",
    "\n",
    "**Which layers to unfreeze?**\n",
    "* It's common practice to unfreeze the last few blocks or layers. Freezing the earliest layers helps retain general feature knowledge and prevent \"catastrophic forgetting\". This is a phenomenon where the model forgets previously learned features when trained on new data (i.e. the pre-trained model's weights are overwritten too much).\n",
    "* For ResNet18, unfreezing `layer4` (the last residual block) is a typical starting point.\n",
    "* For MobileNetV3, the structure is different (inverted residual blocks). We might unfreeze the last few blocks within its `features` attribute.\n",
    "\n",
    "Let's choose one model (e.g., MobileNetV3 again for consistency, or ResNet18 if preferred) and demonstrate unfreezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ed79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Unfreeze later layers of a chosen model\n",
    "\n",
    "# Let's reload a fresh pre-trained model instance for fine-tuning\n",
    "model_to_finetune = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model_name = \"ResNet18_FineTune\"\n",
    "\n",
    "print(f\"Reloaded {model_name} for fine-tuning.\")\n",
    "\n",
    "# First, freeze all layers (as done initially for feature extraction)\n",
    "for param in model_to_finetune.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier head (must be done again on the fresh model instance)\n",
    "num_ftrs = model_to_finetune.fc.in_features\n",
    "model_to_finetune.fc = nn.Linear(num_ftrs, num_classes)\n",
    "print(\"Replaced ResNet18 classifier head (fc layer).\")\n",
    "# Unfreeze parameters in the final residual block (layer4) and the fc layer\n",
    "print(\"  Unfreezing ResNet18 layer4...\")\n",
    "for param in model_to_finetune.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "# Ensure the new fc layer is trainable (it is by default)\n",
    "for param in model_to_finetune.fc.parameters():\n",
    "        param.requires_grad = True # This should already be true for the new layer}\n",
    "\n",
    "# # Uncomment the following code to fine-tune MobileNetV3 instead\n",
    "# model_to_finetune = torchvision.models.mobilenet_v3_large(weights=torchvision.models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "# model_name = \"MobileNetV3_FineTune\"\n",
    "# num_ftrs = model_to_finetune.classifier[-1].in_features\n",
    "# model_to_finetune.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "# print(\"Replaced MobileNetV3 classifier head.\")\n",
    "# # Unfreeze parameters in the last few blocks of MobileNetV3's features\n",
    "# # Example: Unfreeze from block 13 onwards (MobileNetV3-Large has 16 blocks in features)\n",
    "# for i, block in enumerate(model_to_finetune.features):\n",
    "#     if i >= 13:\n",
    "#         print(f\"  Unfreezing MobileNetV3 features block {i}\")\n",
    "#         for param in block.parameters():\n",
    "#             param.requires_grad = True\n",
    "# # Also ensure the classifier itself is trainable (it is by default, but good practice)\n",
    "# for param in model_to_finetune.classifier.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "\n",
    "# --- Verify trainable parameters again ---\n",
    "print(f\"\\nTrainable parameters in {model_name}:\")\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "for name, param in model_to_finetune.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        # print(f\"  - {name}\") # Uncomment to see all trainable param names (can be long)\n",
    "        trainable_params += param.numel() \n",
    "\n",
    "print(f\"\\nTotal parameters in {model_name}: {total_params:,}\")\n",
    "print(f\"Trainable parameters (unfrozen base + head): {trainable_params:,}\")\n",
    "# This number should be significantly larger than for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b1311",
   "metadata": {},
   "source": [
    "### Differential Learning Rates\n",
    "\n",
    "When fine-tuning, the newly added classifier head is randomly initialized (or initialized from the feature extraction phase) and needs to learn the task from scratch. In contrast, the unfrozen pre-trained layers already contain useful knowledge that we only want to adapt slightly.\n",
    "\n",
    "If we use the same learning rate for both, a large learning rate might destroy the pre-trained features in the base layers (\"catastrophic forgetting\"), while a small learning rate might cause the new head to learn too slowly.\n",
    "\n",
    "The solution is **Differential Learning Rates**:\n",
    "* Use a **small** learning rate (e.g., `1e-5` or `1e-4`) for the parameters in the unfrozen pre-trained layers.\n",
    "* Use a **larger** learning rate (e.g., `1e-3` or `1e-4`) for the parameters in the newly added classifier head.\n",
    "\n",
    "We achieve this in PyTorch by passing different parameter groups (each with its own learning rate) to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a20b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Define the optimizer with differential learning rates\n",
    "\n",
    "# We need to separate parameters into two groups:\n",
    "# 1. Parameters of the unfrozen base layers.\n",
    "# 2. Parameters of the classifier head.\n",
    "\n",
    "# Assume 'model_to_finetune' is the model instance from the previous cell\n",
    "\n",
    "params_to_update = []\n",
    "base_params = []\n",
    "head_params = []\n",
    "\n",
    "# Identify parameters based on their names or module hierarchy\n",
    "print(\"Separating parameters for differential learning rates...\")\n",
    "for name, param in model_to_finetune.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if model_name == \"MobileNetV3_FineTune\" and name.startswith(\"classifier\"):\n",
    "            head_params.append(param)\n",
    "            print(f\"  - Head Param: {name}\")\n",
    "        elif model_name == \"ResNet18_FineTune\" and name.startswith(\"fc\"):\n",
    "            head_params.append(param)\n",
    "            print(f\"  - Head Param: {name}\")\n",
    "        else:\n",
    "            base_params.append(param)\n",
    "            # print(f\"  - Base Param: {name}\") # Uncomment to see base params\n",
    "\n",
    "# Define different learning rates\n",
    "lr_base = 1e-5 # Very small LR for pre-trained layers\n",
    "lr_head = 1e-4 # Larger LR for the new head (adjust as needed)\n",
    "\n",
    "print(f\"\\nLearning Rates - Base: {lr_base}, Head: {lr_head}\")\n",
    "\n",
    "# Create parameter groups for the optimizer\n",
    "param_groups = [\n",
    "    {'params': base_params, 'lr': lr_base},\n",
    "    {'params': head_params, 'lr': lr_head}\n",
    "]\n",
    "\n",
    "# Define the optimizer (Adam is common, SGD with momentum also works)\n",
    "optimizer_finetune = optim.Adam(param_groups)\n",
    "# Alternatively: optimizer_finetune = optim.SGD(param_groups, momentum=0.9)\n",
    "\n",
    "print(\"\\nOptimizer created with differential learning rates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Loss function (CrossEntropyLoss) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8653cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Implement the fine-tuning training loop\n",
    "\n",
    "# Use the model with unfrozen layers\n",
    "model = model_to_finetune.to(device) # Ensure model is on the correct device\n",
    "\n",
    "# Use the optimizer with differential LRs\n",
    "optimizer = optimizer_finetune\n",
    "\n",
    "# Use the dataloaders with augmentation\n",
    "dataloaders = dataloaders_aug # Use the augmented dataloaders defined earlier\n",
    "dataset_sizes = dataset_sizes_aug\n",
    "\n",
    "# Use the validation function that returns preds/labels\n",
    "validate_func = validate_and_collect_preds\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs_finetune = 10 # Train for more epochs for fine-tuning (adjust as needed)\n",
    "print(f\"\\nStarting Fine-Tuning training for {num_epochs_finetune} epochs...\")\n",
    "print(f\"Using model: {model_name}\")\n",
    "\n",
    "start_time_finetune = time.time()\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "ft_train_losses = []\n",
    "ft_val_losses = []\n",
    "ft_train_accs = []\n",
    "ft_val_accs = []\n",
    "\n",
    "# Variables to track best model weights\n",
    "best_val_acc = 0.0\n",
    "best_model_wts = model.state_dict() # Store initial weights\n",
    "\n",
    "for epoch in range(num_epochs_finetune):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs_finetune}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    model.train() # Set model to training mode\n",
    "    # (We can reuse train_one_epoch function from L2 if it's defined in the notebook)\n",
    "    # If not defined earlier in this L3 notebook, copy it here or re-run L2's Cell 7\n",
    "    # Assuming train_one_epoch is available:\n",
    "    try:\n",
    "        train_loss, train_acc = train_one_epoch(model, dataloaders['train'], criterion, optimizer, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
    "    except NameError:\n",
    "         print(\"Error: 'train_one_epoch' function not defined. Please define or run the cell from L2.\")\n",
    "         break # Stop if function is missing\n",
    "\n",
    "    # Store training metrics\n",
    "    ft_train_losses.append(train_loss)\n",
    "    # Ensure accuracy value is scalar before appending\n",
    "    ft_train_accs.append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
    "\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() # Set model to evaluate mode\n",
    "    # Use the validation function that returns predictions and labels\n",
    "    val_loss, val_acc, y_pred, y_true = validate_func(model, dataloaders['val'], criterion, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Store validation metrics\n",
    "    ft_val_losses.append(val_loss)\n",
    "     # Ensure accuracy value is scalar before appending\n",
    "    current_val_acc = val_acc.item() if isinstance(val_acc, torch.Tensor) else val_acc\n",
    "    ft_val_accs.append(current_val_acc)\n",
    "\n",
    "    # --- Calculate and display detailed metrics for this epoch ---\n",
    "    # (Assuming calculate_and_display_metrics function and class_names list are available)\n",
    "    calculate_and_display_metrics(y_true, y_pred, class_names)\n",
    "\n",
    "    # --- Checkpointing: Save best model weights ---\n",
    "    if current_val_acc > best_val_acc:\n",
    "        print(f\"Validation accuracy improved ({best_val_acc:.4f} --> {current_val_acc:.4f}). Saving model weights...\")\n",
    "        best_val_acc = current_val_acc\n",
    "        best_model_wts = model.state_dict()\n",
    "        # Optional: Save weights to a file\n",
    "        # torch.save(model.state_dict(), f'{model_name}_best_weights.pth')\n",
    "\n",
    "\n",
    "# --- End of Training Loop ---\n",
    "end_time_finetune = time.time()\n",
    "finetune_duration = end_time_finetune - start_time_finetune\n",
    "print(f\"\\nFine-tuning finished in {finetune_duration // 60:.0f}m {finetune_duration % 60:.0f}s\")\n",
    "print(f\"Best validation accuracy achieved: {best_val_acc:.4f}\")\n",
    "\n",
    "# Optional: Load best model weights back into the model\n",
    "# model.load_state_dict(best_model_wts)\n",
    "\n",
    "# --- The variables ft_train_losses, ft_val_losses, etc. now hold the history ---\n",
    "print(\"\\nFine-tuning metric history collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39af517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Visualize the fine-tuning training history\n",
    "\n",
    "# (Reuse the plotting code, just adapt variable names if needed)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the number of epochs run for fine-tuning\n",
    "epochs_ran = len(ft_train_losses)\n",
    "epochs_range_ft = range(1, epochs_ran + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range_ft, ft_train_losses, 'o-', label='Training Loss')\n",
    "plt.plot(epochs_range_ft, ft_val_losses, 'o-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'{model_name} - Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Training & Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range_ft, ft_train_accs, 'o-', label='Training Accuracy')\n",
    "plt.plot(epochs_range_ft, ft_val_accs, 'o-', label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1) # Set y-axis limits for accuracy between 0 and 1\n",
    "plt.title(f'{model_name} - Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d60c9",
   "metadata": {},
   "source": [
    "### Interpreting Fine-Tuning Results\n",
    "\n",
    "After running the fine-tuning process, compare the results to the Feature Extraction baseline from Lecture 2:\n",
    "\n",
    "* **Performance:** Did the validation accuracy improve compared to feature extraction? Often, fine-tuning provides a boost, especially with sufficient data and augmentation.\n",
    "* **Curves:** Observe the loss and accuracy curves. Does the gap between training and validation curves suggest overfitting? Data augmentation should help mitigate this. Fine-tuning might take longer to converge than feature extraction initially.\n",
    "* **Detailed Metrics:** Look at the precision, recall, F1-scores, and confusion matrix. Did fine-tuning help improve performance on specific classes that were previously difficult? The confusion matrix is key here.\n",
    "* **Training Time:** Fine-tuning involves more gradient calculations, so each epoch will typically take longer than in the feature extraction phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57614971",
   "metadata": {},
   "source": [
    "### Homework: Fine-Tuning the Second Model (to be completed by Monday, May 5th)\n",
    "\n",
    "**Goal:** This homework directs you to consolidate the code and concepts from Lectures 1, 2, and 3, and your previous homeworks, into a structured project format. You will implement the full fine-tuning pipeline for both required models (MobileNetV3 and ResNet18), incorporate early stopping, run extended training, and generate the core results needed for your final project write-up.\n",
    "\n",
    "Organize your work into two separate Jupyter Notebooks, and update the project repo with these:\n",
    "\n",
    "1.  `dataset_Exploration.ipynb`\n",
    "2.  `model_FineTuning.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions for `dataset_Exploration.ipynb`:**\n",
    "\n",
    "* **Purpose:** This notebook should focus entirely on loading, understanding, preparing, and visualizing the Oxford-IIIT Pet dataset.\n",
    "* **Content:**\n",
    "    * Include code and explanations for loading the dataset using `torchvision.datasets.OxfordIIITPet`.\n",
    "    * Implement and explain the necessary data transformations:\n",
    "        * Basic transforms (Resize, ToTensor).\n",
    "        * ImageNet Normalization (explain why it's needed).\n",
    "        * Data Augmentation for the training set (e.g., RandomResizedCrop, RandomHorizontalFlip, etc. - explain your choices).\n",
    "    * Include code to create `DataLoader` instances for training and validation sets using the appropriate transforms.\n",
    "    * Add the dataset exploration code from Homework 1: Calculate and display per-breed image counts and visualize sample images from several different breeds.\n",
    "* **Outcome:** This notebook should be runnable and clearly demonstrate how the data is prepared for training.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions for `Model_FineTuning.ipynb`:**\n",
    "\n",
    "* **Purpose:** This notebook will handle model loading, adaptation, the complete fine-tuning process (with early stopping), evaluation, and results visualization for *both* MobileNetV3 and ResNet18.\n",
    "* **Content - Part 1: Setup:**\n",
    "    * Import necessary libraries (torch, torchvision, optim, nn, sklearn.metrics, matplotlib, etc.).\n",
    "    * Define helper functions:\n",
    "        * Include the `validate_and_collect_preds` function (from L3) to get predictions/labels during validation.\n",
    "        * Include the `calculate_and_display_metrics` function (from L3) for detailed evaluation.\n",
    "        * You might want to refactor the `train_one_epoch` function (from L2) if helpful.\n",
    "    * Load *both* pre-trained MobileNetV3 (`MobileNet_V3_Large_Weights`) and ResNet18 (`ResNet18_Weights`) models.\n",
    "    * For *each* model: Adapt the classifier head for 37 classes and freeze/unfreeze layers according to the fine-tuning strategy discussed in Lecture 3 (e.g., unfreeze last few blocks of MobileNetV3, unfreeze `layer4` + `fc` of ResNet18). Keep track of both adapted model instances.\n",
    "    * Define the loss function (`nn.CrossEntropyLoss`).\n",
    "\n",
    "* **Content - Part 2: Training Loop with Early Stopping:**\n",
    "    * Design a training loop function or structure that can train a given model for a specified number of epochs.\n",
    "    * **Implement Early Stopping:**\n",
    "        * Add parameters for `patience` (e.g., `patience = 5`) – how many epochs to wait for improvement before stopping.\n",
    "        * Inside the loop, track the best validation accuracy achieved so far (`best_val_acc`).\n",
    "        * Keep a counter for epochs since the last improvement (`epochs_no_improve`).\n",
    "        * After each validation epoch:\n",
    "            * If `current_val_acc > best_val_acc`: Update `best_val_acc`, save the `model.state_dict()` (best weights), reset `epochs_no_improve` to 0.\n",
    "            * Else: Increment `epochs_no_improve`.\n",
    "            * If `epochs_no_improve >= patience`: Print an \"Early stopping\" message and break the loop.\n",
    "        * After the loop finishes (either by completing all epochs or early stopping), make sure to load the *best saved weights* back into the model (`model.load_state_dict(best_weights)`).\n",
    "    * Ensure the loop stores history (losses, accuracies) for plotting.\n",
    "    * Ensure the loop calls `validate_and_collect_preds` and `calculate_and_display_metrics` *after* the validation step for the current epoch (and potentially again after loading best weights at the end).\n",
    "\n",
    "* **Content - Part 3: Execution and Comparison:**\n",
    "    * Define the optimizer setup (using differential learning rates) specifically for MobileNetV3 (as shown in L3).\n",
    "    * Run the training loop (with early stopping, `max_epochs=20`, `patience=5`) for the adapted MobileNetV3 model. Store/display its best validation accuracy, final metrics (P, R, F1, Confusion Matrix) corresponding to the best epoch, and plot its training/validation curves.\n",
    "    * Define the optimizer setup (using differential learning rates) specifically for ResNet18 (as shown in L3).\n",
    "    * Run the training loop (with early stopping, `max_epochs=20`, `patience=5`) for the adapted ResNet18 model. Store/display its best validation accuracy, final metrics corresponding to the best epoch, and plot its training/validation curves.\n",
    "    * Add a final Markdown cell briefly summarizing the best validation accuracy achieved by each model.\n",
    "\n",
    "---\n",
    "\n",
    "**Deliverables:**\n",
    "\n",
    "Submit a zip file containing your two completed Jupyter Notebooks:\n",
    "1.  `Dataset_Exploration.ipynb`\n",
    "2.  `Model_FineTuning.ipynb`\n",
    "\n",
    "Ensure both notebooks run without errors and generate the required outputs (printouts, plots, metrics).\n",
    "\n",
    "---\n",
    "\n",
    "**Does this homework provide all material for the final write-up?**\n",
    "\n",
    "Upon completing this homework, you should have:\n",
    "* The fully implemented and documented code for data preparation and model fine-tuning.\n",
    "* The core quantitative results (best accuracies, detailed metrics, loss/accuracy plots) for both models.\n",
    "* Saved best model weights (implicitly, via the early stopping logic).\n",
    "\n",
    "The remaining part of the final project write-up would involve:\n",
    "* Writing the textual explanations and interpretations of your methods and results.\n",
    "* Structuring everything into a final report or comprehensive `README.md`.\n",
    "* Making sure your GitHub repository is well-organized and includes all necessary files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
