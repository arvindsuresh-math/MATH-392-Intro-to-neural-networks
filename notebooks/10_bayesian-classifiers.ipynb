{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classifiers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Outline\n",
    "In this notebook, we discuss a few different (but closely related) classifiers based on Baye's Theorems:\n",
    "1. Bayes Classifier\n",
    "2. Naive Bayes Classifier\n",
    "3. Linear Discriminant Analysis\n",
    "4. Quadratic Discriminant Analysis\n",
    "I refer to these as Bayesian classifiers because they are all based on Baye's Theorem, but differ in the assumptions they make about the distribution of the variables in the dataset. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of dataset\n",
    "In this notebook, we assume that we are given a labelled (training) dataset with the following format:\n",
    "- There are features $X_1,\\dotsc,X_n$ (continuous and/or discrete).\n",
    "- There is a categorical target $Y$ which takes values in some finite set $\\mathcal{C} = \\{c_1,\\dotsc,c_k\\}$. \n",
    "- There are $m$ instances in our dataset. \n",
    "- As usual, we denote by $\\mathbf{x}_i \\in \\mathbb{R}^m$ the column vector of features for the $i$-th feature, and by $\\mathbf{y} \\in \\mathcal{C}^m$ the column vector of true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Soft) Classifiers\n",
    "Our goal is to predict the label $c \\in \\mathcal{C}$ given a row of features $\\vec{x} \\in \\mathbb{R}^n$. Thus, we want to construct a **classifier**, i.e. a function\n",
    "\\begin{equation*}\n",
    "    F : \\mathbb{R}^n \\to \\mathcal{C},\n",
    "\\end{equation*}\n",
    "which assigns to any instance $\\vec{x} = (x_1,\\dotsc,x_n)$ of features a class (or label) $F(\\vec{x}) \\in \\mathcal{C}$. \n",
    "\n",
    "For this purpose, it is natural to first construct a **soft classifier**, which assigns to each instance $\\vec{x}$ an entire probability distribution over the classes $\\mathcal{C}$. Since we are in the discrete case, asking for a probability distribution over the classes is equivalent to asking for a **probability mass function** (PMF), i.e. a function\n",
    "\\begin{equation*}\n",
    "    p: \\mathcal{C} \\to [0,1],\n",
    "\\end{equation*}\n",
    "such that $\\sum_{c \\in \\mathcal{C}} p(c) = 1$. \n",
    "\n",
    "So, formally speaking, a soft classifier can be thought of as a function\n",
    "\\begin{align*}\n",
    "    \\mathbb{R}^n & \\to \\{ \\textup{probability distributions over } \\mathcal{C} \\}, \\\\\n",
    "    \\vec{x} & \\mapsto p_{\\vec{x}},\n",
    "\\end{align*}\n",
    "which assigns to each instance $\\vec{x}$ a probability distribution $p_{\\vec{x}}$ over the classes $\\mathcal{C}$. The hard classifier $F$ is then obtained by taking the class with the highest probability:\n",
    "\\begin{equation*}\n",
    "    F(\\vec{x}) = \\argmax_{c \\in \\mathcal{C}} \\; p_{\\vec{x}}(c).\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier\n",
    "\n",
    "### The most natural soft classifier\n",
    "Consider the problem of assigning to each instance $\\vec{x}$ a probability distribution over the classes $\\mathcal{C}$. Such a distribution needs to come (somehow) from the data. There is basically only one (natural) way to obtain such a distribution from the dataset, namely, we can use the **conditional probability** of the class given the features:\n",
    "\\begin{align*}\n",
    "    p_{\\vec{x}} : \\mathcal{C} & \\to [0,1], \\\\\n",
    "    c & \\mapsto p(c | \\vec{x}).\n",
    "\\end{align*}\n",
    "This is the **Bayes Classifier**. It is the most natural classifier, because the logic is very intuitive and natural: we look at the data and say \"in the training dataset, given that the features where $\\vec{x}$, the probability distribution of the labels was $p(c | \\vec{x})$, so I'm going to predict the same will also hold for future data in which the features are $\\vec{x}$\".\n",
    "\n",
    "To convert this soft classifier into a hard classifier, we take the class with the highest probability condition on the features being equalt to $\\vec{x}$:\n",
    "\\begin{equation*}\n",
    "    F(\\vec{x}) = \\argmax_{c \\in \\mathcal{C}} \\; p(c | \\vec{x}).\n",
    "\\end{equation*}\n",
    "\n",
    "### Implementing the Bayes Classifier\n",
    "Just to keep up with our ongoing tradition of implementing ML models from scratch, we also implement the Bayes Classifier below, with some caveats:\n",
    "- We assume that the features are discrete, so we can use a simple frequency count to estimate the conditional probabilities.\n",
    "- We assume that we have a small number of features and classes, so we can use a brute-force approach to compute the conditional probabilities.\n",
    "- In the case of unseen features, we assign the prior distribution of the classes to the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class MyBayesClassifier:\n",
    "    \"\"\"\n",
    "    MyBayesClassifier implements a simple Bayesian classifier for categorical data.\n",
    "    It computes class priors and conditional probabilities based on the training data,\n",
    "    and uses these estimates to compute posterior probabilities for new observations.\n",
    "    If an unseen combination of feature values is encountered during prediction, the\n",
    "    classifier defaults to the overall class priors.\n",
    "\n",
    "    Attributes:\n",
    "        X : pandas.DataFrame or array\n",
    "            The feature data used during training. Internally, X is stored as a DataFrame.\n",
    "        y : pandas.Series\n",
    "            The target (label) vector corresponding to the training examples.\n",
    "        classes : list\n",
    "            A sorted list of unique target class values extracted from y.\n",
    "        priors : dict\n",
    "            A mapping from each class label to its prior probability based on the training data.\n",
    "        cond_probs : dict\n",
    "            A dictionary mapping a tuple of feature values (representing a unique combination)\n",
    "            to a list of posterior probabilities for each class. The order of probabilities in\n",
    "            the list corresponds to the order of classes in the 'classes' attribute.\n",
    "\n",
    "    Methods:\n",
    "        fit(X, y):\n",
    "            Accepts a DataFrame or array of categorical features X and a vector y of targets.\n",
    "            Computes and stores the unique classes, the class prior probabilities, and the\n",
    "            conditional probabilities for each observed combination of feature values.\n",
    "        \n",
    "        predict_proba(X_test):\n",
    "            Given new observations (as a DataFrame or array), returns a DataFrame of\n",
    "            posterior probabilities for each class. For any row where the feature combination\n",
    "            is unseen in the training data, the overall class priors are returned.\n",
    "        \n",
    "        predict(X_test):\n",
    "            Uses predict_proba to calculate the posterior probabilities, and returns a Series\n",
    "            containing the class label with the highest probability for each observation.\n",
    "            \n",
    "        posteriors():\n",
    "            Returns a DataFrame listing every possible combination of feature values (based on the training data)\n",
    "            and the corresponding conditional distribution (posterior probabilities) of y. There is one column\n",
    "            per feature and one column per class.\n",
    "        \n",
    "        unseen_instances(X_test):\n",
    "            Given a new test dataset, returns a DataFrame containing the rows representing feature combinations\n",
    "            that were not observed in the training data.\n",
    "            \n",
    "    Example usage:\n",
    "        clf = MyBayesClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        proba = clf.predict_proba(X_test)\n",
    "        predictions = clf.predict(X_test)\n",
    "        posteriors = clf.posteriors()\n",
    "        new_instances = clf.unseen_instances(X_test)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "        self.cond_probs = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes classes, priors, and conditional probabilities for \n",
    "        each unique combination of feature values in X.\n",
    "        \n",
    "        Parameters:\n",
    "            X (pd.DataFrame or np.array): Features (categorical).\n",
    "            y (pd.Series or np.array): Column vector of targets.\n",
    "        \"\"\"\n",
    "        # If X is not a DataFrame, convert it.\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        self.X = X.copy()\n",
    "        self.y = pd.Series(y).copy()\n",
    "        \n",
    "        # Determine the list of classes.\n",
    "        self.classes = sorted(self.y.unique())\n",
    "        \n",
    "        # Compute class priors.\n",
    "        prior_series = self.y.value_counts(normalize=True)\n",
    "        self.priors = prior_series.to_dict()\n",
    "        \n",
    "        # Compute conditional probabilities for each observed combination.\n",
    "        # Concatenate X and y for grouping.\n",
    "        df_train = self.X.copy()\n",
    "        df_train[\"target\"] = self.y\n",
    "        \n",
    "        # Group by the feature columns.\n",
    "        grouped = df_train.groupby(list(self.X.columns))\n",
    "        \n",
    "        for combo, group in grouped:\n",
    "            # Ensure combo is a tuple even if single feature.\n",
    "            if not isinstance(combo, tuple):\n",
    "                combo = (combo,)\n",
    "                \n",
    "            # Count occurrences of each class for this feature combination.\n",
    "            counts = group[\"target\"].value_counts().to_dict()\n",
    "            total = sum(counts.values())\n",
    "            \n",
    "            # Build the posterior probability vector in the order of self.classes.\n",
    "            prob_vector = [counts.get(cls, 0) / total for cls in self.classes]\n",
    "            \n",
    "            self.cond_probs[combo] = prob_vector\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        For each row in X_test, returns the posterior probabilities over classes.\n",
    "        If a combination of feature values is unseen, the overall class priors are used.\n",
    "        \n",
    "        Parameters:\n",
    "            X_test (pd.DataFrame or np.array): Test features.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame of posterior probabilities with columns corresponding\n",
    "                          to the classes.\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if necessary.\n",
    "        if not isinstance(X_test, pd.DataFrame):\n",
    "            X_test = pd.DataFrame(X_test, columns=self.X.columns)\n",
    "        \n",
    "        # Define a lookup function for a row of features.\n",
    "        def lookup(row):\n",
    "            combo = tuple(row)\n",
    "            if combo in self.cond_probs:\n",
    "                return self.cond_probs[combo]\n",
    "            else:\n",
    "                # Return priors in the order of self.classes.\n",
    "                return [self.priors.get(cls, 0) for cls in self.classes]\n",
    "        \n",
    "        # Apply the lookup along axis=1.\n",
    "        proba_series = X_test.apply(lookup, axis=1)\n",
    "        proba_df = pd.DataFrame(proba_series.tolist(), \n",
    "                                index=X_test.index, \n",
    "                                columns=self.classes)\n",
    "        return proba_df\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Returns the predicted class label for each row in X_test.\n",
    "        \n",
    "        Parameters:\n",
    "            X_test (pd.DataFrame or np.array): Test features.\n",
    "        \n",
    "        Returns:\n",
    "            pd.Series: Predicted class for each row.\n",
    "        \"\"\"\n",
    "        proba_df = self.predict_proba(X_test)\n",
    "        # Return the class with maximum probability along each row.\n",
    "        predictions = proba_df.idxmax(axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    def posteriors(self):\n",
    "        \"\"\"\n",
    "        Returns a DataFrame with every possible combination of feature values from the training data \n",
    "        and the corresponding conditional distribution (posterior probabilities) of y.\n",
    "        \n",
    "        The resulting DataFrame contains one column per feature and one column per class.\n",
    "        For any combination not observed during training, the overall class priors are returned.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame where each row represents a unique combination of feature values,\n",
    "                          with feature columns (same names as in training data) and additional columns for \n",
    "                          each class (named using the class value).\n",
    "        \"\"\"\n",
    "        # For each feature column, get unique sorted values.\n",
    "        unique_vals = {col: sorted(self.X[col].unique()) for col in self.X.columns}\n",
    "        # Generate all possible combinations.\n",
    "        combos = list(itertools.product(*[unique_vals[col] for col in self.X.columns]))\n",
    "        \n",
    "        records = []\n",
    "        for combo in combos:\n",
    "            record = {col: combo[i] for i, col in enumerate(self.X.columns)}\n",
    "            # Use cond_probs if seen; otherwise, use overall priors.\n",
    "            if combo in self.cond_probs:\n",
    "                probs = self.cond_probs[combo]\n",
    "            else:\n",
    "                probs = [self.priors.get(cls, 0) for cls in self.classes]\n",
    "            for idx, cls in enumerate(self.classes):\n",
    "                record[cls] = probs[idx]\n",
    "            records.append(record)\n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    def unseen_instances(self, X_test):\n",
    "        \"\"\"\n",
    "        Given a test dataset, returns the sub-dataframe containing rows that are not found \n",
    "        in the training data (i.e. their feature combination was not observed during training).\n",
    "        \n",
    "        Parameters:\n",
    "            X_test (pd.DataFrame or np.array): Test features.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Sub-dataframe of X_test of the unseen instances.\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if necessary.\n",
    "        if not isinstance(X_test, pd.DataFrame):\n",
    "            X_test = pd.DataFrame(X_test, columns=self.X.columns)\n",
    "        \n",
    "        # Mark rows whose feature combination is not in cond_probs.\n",
    "        unseen_mask = X_test.apply(lambda row: tuple(row) not in self.cond_probs, axis=1)\n",
    "        return X_test[unseen_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the Bayes Classifier on the `car_evaluation` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the car evaluation datasets\n",
    "cars_train = pd.read_csv(\"../data/classification/car_evaluation/train.csv\")\n",
    "cars_test = pd.read_csv(\"../data/classification/car_evaluation/test.csv\")\n",
    "\n",
    "# Create X and y for train and test sets\n",
    "X_train = cars_train.drop(columns=[\"Y\"])\n",
    "y_train = cars_train[\"Y\"]\n",
    "X_test = cars_test.drop(columns=[\"Y\"])\n",
    "y_test = cars_test[\"Y\"]\n",
    "\n",
    "# Initialize and fit the classifier.\n",
    "clf = MyBayesClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and classes for the train and test set\n",
    "train_proba = clf.predict_proba(X_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_proba = clf.predict_proba(X_test)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "# append true and predicted values of train and test sets to the proba dataframes\n",
    "train_proba[\"Y_true\"] = y_train\n",
    "train_proba[\"Y_pred\"] = train_predictions\n",
    "test_proba[\"Y_true\"] = y_test\n",
    "test_proba[\"Y_pred\"] = test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the Bayes Classifier\n",
    "The Bayes Classifier is a very natural classifier, but it has a few limitations. We give two of them below:\n",
    "1. **Unseen features**: If we are given an instance $\\vec{x}$ which does not appear in the training set, then the Bayes classifier will not know how to assign a probability distribution to the classes. There are a couple of potential ways to deal with this issue, but they are all not very satisfying:\n",
    "    - We can assign a uniform distribution over the classes, i.e. \n",
    "    \\begin{equation*}\n",
    "        p(c | \\vec{x}) = \\frac{1}{|\\mathcal{C}|} \\textup{ for all } c \\in \\mathcal{C}.\n",
    "    \\end{equation*}\n",
    "    - We can assign the prior distribution of the classes, i.e. \n",
    "    \\begin{equation*}\n",
    "        p(c | \\vec{x}) = p(c) \\textup{ for all } c \\in \\mathcal{C}.\n",
    "    \\end{equation*}\n",
    "    - If the features are continuous (or more generally, if we have a way of distinguishing feature values \"near\" to $\\vec{x}$), then we  can consider a small sample of our training data near the point $\\vec{x} \\in \\mathbb{R}^n$ and use the conditional probability of the classes given this sample. \n",
    "2. **Computationally expensive**: The Bayes classifier requires us to compute the conditional probability of the classes given the features, which is computationally expensive. This is not a problem if we have a small number of features and classes, but it can be very expensive if we have many features and/or classes. Moreover, in general there aren't any particular assumptions we can make about the joint distributions which would allow us to compute the conditional probabilities in a more efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem recalled\n",
    "Recall from last class that Bayes Theorem allows us to re-write the posteriors as follows:\n",
    "\\begin{equation*}\n",
    "    p(c | \\vec{x}) = \\frac{p(\\vec{x} | c) p(c)}{p(\\vec{x})}.\n",
    "\\end{equation*}\n",
    "Here, we have the following terminology and concepts:\n",
    "- **Prior** $P(c)$:\n",
    "    \n",
    "    This is the probability $P(Y=c)$ that the target is $y$ before we have observed the features. We either make a reasonable assumption about this probability, or we estimate it from the data (e.g. by taking it to be the fraction of the training data that has target $y$ in the discrete case, or by computing an approximation to the density of the target at $y$ in the continuous case).\n",
    "- **Likelihood** $P(\\vec{x}|c)$:\n",
    "\n",
    "    This is the condition probability $P(\\vec{X} = \\vec{x}|Y=c)$ of observing the features $\\vec{x}$ given that the target is $y$.\n",
    "\n",
    "- **Evidence** $P(\\vec{x})$:\n",
    "\n",
    "    This is the probability $P(\\vec{X} = \\vec{x})$ of observing the features $\\vec{x}$, regardless of the target. \n",
    "\n",
    "- **Posterior** $P(c|\\vec{x})$:\n",
    "\n",
    "    This is the probability that the target is $y$ given that we have observed the features $\\vec{x}$. \n",
    "\n",
    "Thus, the (soft) Bayes Classifier can be re-written as the mapping\n",
    "\\begin{align*}\n",
    "    \\mathbb{R}^n & \\to \\{ \\textup{probability distributions over } \\mathcal{C} \\}, \\\\\n",
    "    \\vec{x} & \\mapsto \\left( p_{\\vec{x}}(c) = \\frac{p(\\vec{x} | c) p(c)}{p(\\vec{x})} \\right).\n",
    "\\end{align*}\n",
    "The hard classifier is then obtained by taking the class with the highest probability (which allows us to drop the denominator term $p(\\vec{x})$):\n",
    "\\begin{align*}\n",
    "    F(\\vec{x}) & = \\argmax_{c \\in \\mathcal{C}} \\; p(c | \\vec{x}) \\\\\n",
    "    & = \\argmax_{c \\in \\mathcal{C}} \\; p(\\vec{x} | c) p(c).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Bayes Classifier to Bayesian Classifiers\n",
    "Although the Bayes Classifier is not useful from the point of view of practical applications, it is still theoretically very useful because it is in (a sense which can be made precise) the \"best possible\" classifier that can be constructed from the data. Thus, any soft classifier can be thought of as a weakening or approximation of the Bayes Classifier, in which certain assumptions are made about likelihoods $p(\\vec{x} \\mid c)$ in order to make the computation of the posterior $p(c \\mid \\vec{x})$ more tractable. \n",
    "\n",
    "The priors $p(c)$ need only be computed once, and can be used for all instances $\\vec{x}$. Thus, the main easing of computational burden comes from the fact that we estimate likelihoods rather than posteriors. The trade-off is that instead of constructing a PMF over the classes for each instance $\\vec{x}$, we construct a PMF (or PDF) of the features given each class (which is \"only\" $|\\mathcal{C}|$-many distributions). \n",
    "\n",
    "Three different assumptions lead to three different Bayesian classifiers:\n",
    "1. Assuming conditional independence of the features given the class leads to the **Naive Bayes Classifier**.\n",
    "2. Assuming that the likelihoods are Gaussian (with identical class-conditional covariances) leads to **Linear Discriminant Analysis**.\n",
    "3. Assuming that the likelihoods are Gaussian (with different class-conditional covariances) leads to **Quadratic Discriminant Analysis**.\n",
    "In the next sections, we will discuss each of these classifiers in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "### Conditional independence\n",
    "In NB, we will assume that the features $X_1,\\dotsc,X_n$ are conditionally independent given $Y$. This means that the likelihoods factorize as\n",
    "\\begin{equation*}\n",
    "    p(\\vec{x} | c) = \\prod_{i=1}^n p(x_i | c).\n",
    "\\end{equation*}\n",
    "This is a very strong (some might say, *Naive*) assumption which very rarely holds in real life, but it is a very useful assumption because it allows us to compute the likelihoods in a much more efficient way. Note that for each class $c \\in \\mathcal{C}$, there are $n$ conditional distributions $p(x_i | c)$ to estimate, rather than a single joint distribution $p(\\vec{x} | c)$, so there is a trade-off between computing many low-dimensional distributions rather than a single high-dimensional distribution rather.\n",
    "\n",
    "Now, the computations of the likelihoods depends on whether the features are discrete or continuous. We will discuss both cases below separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Naive Bayes\n",
    "Fix a class $c \\in \\mathcal{C}$, and assume that the features are all categorical (i.e. discrete). Then, we can estimate the conditional distributions $p(X_i = x_i |Y= c)$ by computing the corresponding fractions in the dataset:\n",
    "\\begin{equation*}\n",
    "    p(x_i \\mid c) = \\frac{\\textup{number of instances with } X_i = x_i \\textup{ and } Y = c}{\\textup{number of instances with } Y = c}.\n",
    "\\end{equation*}\n",
    "To implement this, we can use the `CategoricalNB` class from `sklearn.naive_bayes`. We illustrate this on the `car_evaluation` dataset, which is a dataset of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run sklearn's CategoricalNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "# To compute accuracy of our predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# To visualize the results in a confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# read in car evaluation datasets\n",
    "cars_train = pd.read_csv(\"../data/classification/car_evaluation/train.csv\")\n",
    "cars_test = pd.read_csv(\"../data/classification/car_evaluation/test.csv\")\n",
    "\n",
    "# Create X and y for train and test sets\n",
    "X_train = cars_train.drop(columns=[\"Y\"])\n",
    "y_train = cars_train[\"Y\"]\n",
    "X_test = cars_test.drop(columns=[\"Y\"])\n",
    "y_test = cars_test[\"Y\"]\n",
    "\n",
    "# Convert categorical features into integer codes.\n",
    "encoder = OrdinalEncoder()\n",
    "X_train_enc = encoder.fit_transform(X_train)\n",
    "X_test_enc = encoder.transform(X_test)\n",
    "\n",
    "# Convert target labels into integer codes.\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_test_enc = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# Create and train the CategoricalNB classifier.\n",
    "# Create the classifier with the number of categories for each feature\n",
    "n_categories = [len(encoder.categories_[i]) for i in range(len(encoder.categories_))]\n",
    "clf = CategoricalNB(min_categories=n_categories)\n",
    "clf.fit(X_train_enc, y_train_enc)\n",
    "\n",
    "# Predict probabilities and classes for the train and test set\n",
    "train_proba = clf.predict_proba(X_train_enc)\n",
    "train_predictions = clf.predict(X_train_enc)\n",
    "test_proba = clf.predict_proba(X_test_enc)\n",
    "test_predictions = clf.predict(X_test_enc)\n",
    "\n",
    "# Compute and print accuracy of our predictions (as a percentage) on train and test sets\n",
    "train_accuracy = accuracy_score(y_train_enc, train_predictions)\n",
    "test_accuracy = accuracy_score(y_test_enc, test_predictions)\n",
    "print(f\"Train accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the class priors for the train and test set\n",
    "print(\"Class priors in train set:\")\n",
    "print(cars_train[\"Y\"].value_counts(normalize=True))\n",
    "print(\"Class priors in test set:\")\n",
    "print(cars_test[\"Y\"].value_counts(normalize=True))\n",
    "\n",
    "# Convert numeric predictions back to original categorical labels.\n",
    "train_predictions_orig = label_encoder.inverse_transform(train_predictions)\n",
    "test_predictions_orig = label_encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Compute confusion matrices using the true labels (already categorical) and the converted predictions.\n",
    "train_cm = confusion_matrix(y_train, train_predictions_orig, labels=label_encoder.classes_)\n",
    "test_cm = confusion_matrix(y_test, test_predictions_orig, labels=label_encoder.classes_)\n",
    "\n",
    "# Convert the confusion matrices into DataFrames for prettier output, with the original categories as row and column labels.\n",
    "train_cm_df = pd.DataFrame(train_cm, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "test_cm_df = pd.DataFrame(test_cm, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "\n",
    "# Compute fraction matrices: each element divided by the overall total of the confusion matrix\n",
    "train_frac = (train_cm_df / train_cm_df.to_numpy().sum())*100\n",
    "test_frac = (test_cm_df / test_cm_df.to_numpy().sum())*100\n",
    "\n",
    "# Compute row-normalized matrices: each row divided by its total (in percentage)\n",
    "train_row_norm = (train_cm_df.div(train_cm_df.sum(axis=1), axis=0)) * 100\n",
    "test_row_norm = (test_cm_df.div(test_cm_df.sum(axis=1), axis=0)) * 100\n",
    "\n",
    "# Create a figure with 3 rows and 2 columns of subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(8,8))\n",
    "\n",
    "# First row: standard confusion matrices\n",
    "sns.heatmap(train_cm_df, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title('Train predictions')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('True')\n",
    "\n",
    "sns.heatmap(test_cm_df, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test predictions')\n",
    "axes[0,1].set_xlabel('Predicted')\n",
    "axes[0,1].set_ylabel('True')\n",
    "\n",
    "# Second row: fraction matrices\n",
    "sns.heatmap(train_frac, annot=True, fmt='.2f', cmap='Greens', ax=axes[1,0])\n",
    "axes[1,0].set_title('Train prediction %')\n",
    "axes[1,0].set_xlabel('Predicted')\n",
    "axes[1,0].set_ylabel('True')\n",
    "\n",
    "sns.heatmap(test_frac, annot=True, fmt='.2f', cmap='Greens', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test prediction %')\n",
    "axes[1,1].set_xlabel('Predicted')\n",
    "axes[1,1].set_ylabel('True')\n",
    "\n",
    "# Third row: row-normalized matrices\n",
    "sns.heatmap(train_row_norm, annot=True, fmt='.2f', cmap='Oranges', ax=axes[2,0])\n",
    "axes[2,0].set_title('Train prediction % per true label')\n",
    "axes[2,0].set_xlabel('Predicted')\n",
    "axes[2,0].set_ylabel('True')\n",
    "\n",
    "sns.heatmap(test_row_norm, annot=True, fmt='.2f', cmap='Oranges', ax=axes[2,1])\n",
    "axes[2,1].set_title('Test prediction % per true label')\n",
    "axes[2,1].set_xlabel('Predicted')\n",
    "axes[2,1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "Assume now that the features are all continuous, and fix a class $c \\in \\mathcal{C}$. In order to estimate the conditional distribution of $X_i$ given $Y=c$, we need to basically approximate the PDF. There are many ways to do this, but a very common and simple way is to assume that the features are **Gaussian** (or follow a **normal distribution**). The graph of the PDF is a bell-shaped curve, and it is in fact *the* Bell Curve.  Recall that the PDF of a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ is given by\n",
    "\\begin{equation*}\n",
    "    p(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left( {-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\right).\n",
    "\\end{equation*}\n",
    "Thus, we can estimate the conditional distribution of $X_i$ given $Y=c$ by computing the mean $\\mu_c$ and variance $\\sigma_c^2$ of $X_i$ for all instances in the training set with $Y=c$. So, if there are $m_c$ instances in the training set with $Y=c$, then we have\n",
    "\\begin{align*}\n",
    "    \\mu_c & = \\frac{1}{m_c} \\sum_{i=1}^{m_c} x_i, \\\\\n",
    "    \\sigma_c^2 & = \\frac{1}{m_c} \\sum_{i=1}^{m_c} (x_i - \\mu_c)^2.\n",
    "\\end{align*}\n",
    "We can then use these estimates to compute the conditional distribution of $X_i$ given $Y=c$ as\n",
    "\\begin{equation*}\n",
    "    p(x_i \\mid c) = \\frac{1}{\\sqrt{2 \\pi} \\sigma_c} \\exp \\left( -\\frac{(x_i - \\mu_c)^2}{2 \\sigma_c^2} \\right).\n",
    "\\end{equation*}\n",
    "Thus, the formula for the posterior is given by\n",
    "\\begin{align*}\n",
    "    p(c \\mid \\vec{x}) & = \\frac{p(c)}{p(\\vec{x})} \\cdot \\prod_{i=1}^n p(x_i \\mid c) \\\\\n",
    "    & = \\frac{p(c)}{p(\\vec{x})} \\cdot \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi} \\sigma_c} \\exp \\left( {-\\frac{(x_i - \\mu_c)^2}{2 \\sigma_c^2}} \\right).\n",
    "\\end{align*}\n",
    "It is convenient to take the log of both sides to get the **log posterior**:\n",
    "\\begin{align*}\n",
    "    \\log p(c \\mid \\vec{x}) & = \\log p(c) - \\log p(\\vec{x}) + \\sum_{i=1}^n \\left( -\\log(\\sqrt{2 \\pi} \\sigma_c) - \\frac{(x_i - \\mu_c)^2}{2 \\sigma_c^2} \\right) \\\\\n",
    "    & = \\log p(c) - \\log p(\\vec{x}) + -n\\log(\\sqrt{2 \\pi} \\sigma_c) - \\frac{1}{2\\sigma_c^2} \\sum_{i=1}^n (x_i - \\mu_c)^2.\n",
    "\\end{align*}\n",
    "The hard classifier is then obtained by taking the class with the highest log posterior:\n",
    "\\begin{align*}\n",
    "    F_{\\textup{NB}}(\\vec{x}) & = \\argmax_{c \\in \\mathcal{C}} \\; \\log p(c | \\vec{x})\\\\\n",
    "    & = \\argmax_{c \\in \\mathcal{C}} \\; \\left( \\log p(c) - \\log p(\\vec{x}) + -n\\log(\\sqrt{2 \\pi} \\sigma_c) - \\frac{1}{2\\sigma_c^2} \\sum_{i=1}^n (x_i - \\mu_c)^2 \\right).\n",
    "\\end{align*}\n",
    "So, if we put $A_c = n\\log(\\sqrt{2 \\pi} \\sigma_c) - \\log p(c)$, then we have\n",
    "\\begin{align*}\n",
    "    F_{\\textup{NB}}(\\vec{x}) & = \\argmin_{c \\in \\mathcal{C}} \\; \\left( \\frac{1}{2\\sigma_c^2} \\sum_{i=1}^n (x_i - \\mu_c)^2 + A_c \\right).\n",
    "\\end{align*}\n",
    "(NOTE: each $x_i$ above is an entry in a row vector $\\vec{x}$, so we are summing over the entries of $\\vec{x}$).\n",
    "\n",
    "### Implementing Gaussian Naive Bayes\n",
    "We can implement Gaussian Naive Bayes using the `GaussianNB` class from `sklearn.naive_bayes`. We illustrate this on the `iris` dataset, which is a dataset of continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Gaussian NB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# read in iris datasets\n",
    "iris_train = pd.read_csv(\"../data/classification/iris/train.csv\")\n",
    "iris_test = pd.read_csv(\"../data/classification/iris/test.csv\")\n",
    "\n",
    "# Create X and y for train and test sets\n",
    "X_train = iris_train.drop(columns=[\"Y\"])\n",
    "y_train = iris_train[\"Y\"]\n",
    "X_test = iris_test.drop(columns=[\"Y\"])\n",
    "y_test = iris_test[\"Y\"]\n",
    "\n",
    "# Print the class priors for the train and test set\n",
    "print(\"Class priors in train set:\")\n",
    "print(iris_train[\"Y\"].value_counts(normalize=True))\n",
    "print(\"Class priors in test set:\")\n",
    "print(iris_test[\"Y\"].value_counts(normalize=True))\n",
    "\n",
    "# Initialize and fit the classifier.\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and classes for the train and test set\n",
    "train_proba = clf.predict_proba(X_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_proba = clf.predict_proba(X_test)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "# Compute and print accuracy of our predictions (as a percentage) on train and test sets\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Train accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute confusion matrices for train and test sets\n",
    "train_cm = confusion_matrix(y_train, train_predictions, labels=clf.classes_)\n",
    "test_cm = confusion_matrix(y_test, test_predictions, labels=clf.classes_)\n",
    "\n",
    "# Compute percentage matrices: each element divided by the overall total of the confusion matrix\n",
    "train_frac = (train_cm / train_cm.sum()) * 100\n",
    "test_frac = (test_cm / test_cm.sum()) * 100\n",
    "\n",
    "# Compute row-normalized matrices: each row divided by its total (in percentage)\n",
    "train_row_norm = (train_cm / train_cm.sum(axis=1, keepdims=True)) * 100\n",
    "test_row_norm = (test_cm / test_cm.sum(axis=1, keepdims=True)) * 100\n",
    "\n",
    "# Create a figure with 3 rows and 2 columns of subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(6,6))\n",
    "# First row: standard confusion matrices\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title('Train predictions')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('True')\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test predictions')\n",
    "axes[0,1].set_xlabel('Predicted')\n",
    "axes[0,1].set_ylabel('True')\n",
    "\n",
    "# Second row: fraction matrices\n",
    "sns.heatmap(train_frac, annot=True, fmt='.2f', cmap='Greens', ax=axes[1,0])\n",
    "axes[1,0].set_title('Train prediction %')\n",
    "axes[1,0].set_xlabel('Predicted')\n",
    "axes[1,0].set_ylabel('True')\n",
    "sns.heatmap(test_frac, annot=True, fmt='.2f', cmap='Greens', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test prediction %')\n",
    "axes[1,1].set_xlabel('Predicted')\n",
    "axes[1,1].set_ylabel('True')\n",
    "\n",
    "# Third row: row-normalized matrices\n",
    "sns.heatmap(train_row_norm, annot=True, fmt='.2f', cmap='Oranges', ax=axes[2,0])\n",
    "axes[2,0].set_title('Train prediction % per true label')\n",
    "axes[2,0].set_xlabel('Predicted')\n",
    "axes[2,0].set_ylabel('True')\n",
    "sns.heatmap(test_row_norm, annot=True, fmt='.2f', cmap='Oranges', ax=axes[2,1])\n",
    "axes[2,1].set_title('Test prediction % per true label')\n",
    "axes[2,1].set_xlabel('Predicted')\n",
    "axes[2,1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That went quite well! Below, we visualize the individual estimated Gaussian distributions (blue curve) for each feature conditional on the class, and we overlay it with a kdeplot of the true distribution for comparison (red curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Get feature names and class labels\n",
    "features = X_train.columns  # iris features\n",
    "classes = clf.classes_\n",
    "theta = clf.theta_         # shape (n_classes, n_features)\n",
    "var = clf.var_            # shape (n_classes, n_features)\n",
    "\n",
    "n_features = len(features)\n",
    "n_classes = len(classes)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a grid of subplots: one row per feature and one column per class.\n",
    "fig, axes = plt.subplots(n_features, n_classes, figsize=(4*n_classes, 3*n_features), sharex=False, sharey=False)\n",
    "\n",
    "for i, feat in enumerate(features):\n",
    "    # Define an x-axis range based on the overall training data for this feature.\n",
    "    feat_data = X_train[feat]\n",
    "    x_min, x_max = feat_data.min(), feat_data.max()\n",
    "    x_values = np.linspace(x_min, x_max, 200)\n",
    "    \n",
    "    for j, cls in enumerate(classes):\n",
    "        # Get the correct subplot axis.\n",
    "        ax = axes[i, j] if n_features > 1 and n_classes > 1 else (axes[j] if n_features==1 else axes[i])\n",
    "        \n",
    "        # Filter the training data for the current class.\n",
    "        data_cls = X_train.loc[y_train == cls, feat]\n",
    "        # Plot the true distribution as a KDE (use a reddish color).\n",
    "        sns.kdeplot(data_cls, ax=ax, color='darkred', label=\"True KDE\", fill=True, alpha=0.2)\n",
    "        \n",
    "        # Extract the estimated Gaussian parameters for this feature in this class.\n",
    "        mu = theta[j, i]\n",
    "        std = np.sqrt(var[j, i])\n",
    "        # Compute the Gaussian pdf at the x values.\n",
    "        pdf = norm.pdf(x_values, loc=mu, scale=std)\n",
    "        # Plot the estimated Gaussian (using a blue color).\n",
    "        ax.plot(x_values, pdf, color='steelblue', label=\"Predicted Gaussian\")\n",
    "        \n",
    "        ax.set_title(f\"{feat} | Class {cls}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis\n",
    "\n",
    "### Multi-variate Gaussian\n",
    "Given continuous features $X_1,\\dotsc,X_n$, we can assume that the joint distribution of the features given the class is a **multi-variate Gaussian**; this is a higher-dimensional analogue of the one-dimensional normal distribution. So, if there are two features, then it looks like a bell-shaped surface over the $X_1,X_2$ plane, and if there are three features, then it looks like a bell-shaped volume over the $X_1,X_2,X_3$ space (whatever the heck that is). \n",
    "\n",
    "The multi-variate Gaussian distribution is parameterized by a mean vector $\\mu \\in \\mathbb{R}^n$ and a covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$, and its PDF is given (for each $\\vec{x} \\in \\mathbb{R}^n$) by\n",
    "\\begin{equation*}\n",
    "    p(\\vec{x}) = \\frac{1}{\\sqrt{(2 \\pi)^n |\\Sigma|}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\mu)^\\top \\Sigma^{-1} (\\vec{x} - \\mu) \\right),\n",
    "\\end{equation*}\n",
    "where $|\\Sigma|$ is the determinant of the (sample) covariance matrix $\\Sigma = [\\textup{Cov}(\\mathbf{x}_i,\\mathbf{x}_j)]$. The determinant of a matrix is a scalar, and it is a measure of how \"spread out\" the data is. If the determinant is small, then the data is tightly clustered around the mean, and if the determinant is large, then the data is more spread out.\n",
    "The inverse of the covariance matrix is called the **precision matrix**, and it is a measure of how \"sharp\" the data is. If the precision matrix is large, then the data is tightly clustered around the mean, and if the precision matrix is small, then the data is more spread out.\n",
    "\n",
    "### QDA assumptions\n",
    "In QDA, we assume that the joint distribution of the features given the class is a multi-variate Gaussian distribution with mean vector $\\mu_c \\in \\mathbb{R}^n$ and covariance matrix $\\Sigma_c \\in \\mathbb{R}^{n \\times n}$, i.e.\n",
    "\\begin{equation*}\n",
    "    p(\\vec{x} | c) = \\frac{1}{\\sqrt{(2 \\pi)^n |\\Sigma_c|}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\mu_c)^\\top \\Sigma_c^{-1} (\\vec{x} - \\mu_c) \\right).\n",
    "\\end{equation*}\n",
    "NOTE: we have dropped the assumption that the features are independent given the class! So, for each class $c \\in \\mathcal{C}$, we have to now estimate a single multi-variate Gaussian distribution $p(\\vec{x} | c)$, rather than $n$ independent distributions $p(x_i | c)$. This approach allows us to capture the correlations between the features (conditioned on the class).\n",
    "\n",
    "Then, the posterior is given by\n",
    "\\begin{align*}\n",
    "    p(c | \\vec{x}) & = \\frac{p(c)}{p(\\vec{x})} \\cdot p(\\vec{x} | c) \\\\\n",
    "    & = \\frac{1}{p(\\vec{x})} \\cdot \\frac{p(c)}{\\sqrt{(2 \\pi)^n |\\Sigma_c|}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\mu_c)^\\top \\Sigma_c^{-1} (\\vec{x} - \\mu_c) \\right).\n",
    "\\end{align*}\n",
    "\n",
    "Taking the log of both sides and setting $$A_c = \\log p(c) - \\frac{n}{2} \\log(2 \\pi) - \\frac{1}{2} \\log(|\\Sigma_c|),$$ we get the following formula for the log posterior:\n",
    "\\begin{equation*}\n",
    "    \\log p(c \\mid \\vec{x}) = A_c - \\frac{1}{2} (\\vec{x} - \\mu_c)^\\top \\Sigma_c^{-1} (\\vec{x} - \\mu_c) - \\frac{1}{2} \\log p(\\vec{x}).\n",
    "\\end{equation*}\n",
    "The hard classifier is then obtained by taking the class with the highest log posterior:\n",
    "\\begin{align*}\n",
    "    F_{\\textup{QDA}}(\\vec{x}) & = \\argmax_{c \\in \\mathcal{C}} \\; \\log p(c | \\vec{x})\\\\\n",
    "    & = \\argmax_{c \\in \\mathcal{C}} \\; \\left( A_c - \\frac{1}{2} (\\vec{x} - \\mu_c)^\\top \\Sigma_c^{-1} (\\vec{x} - \\mu_c) - \\frac{1}{2} \\log p(\\vec{x}) \\right)\\\\\n",
    "    & = \\argmin_{c \\in \\mathcal{C}} \\; \\left( (\\vec{x} - \\mu_c)^\\top \\Sigma_c^{-1} (\\vec{x} - \\mu_c) - A_c \\right),\n",
    "\\end{align*}\n",
    "where we have dropped the term $-\\frac{1}{2} \\log p(\\vec{x})$ because it does not depend on $c$.\n",
    "\n",
    "NOTE: If you look at the quantity being minimized above, you will see that it is a quadratic form in $\\vec{x}$, which is a measure of how far the instance $\\vec{x}$ is from the mean $\\mu_c$ of the class $c$, weighted by the precision matrix $\\Sigma_c^{-1}$. Thus, the hard classifier is essentially assigning the class $c$ to the instance $\\vec{x}$ which is closest to the mean $\\mu_c$ of the class $c$, weighted by the precision matrix $\\Sigma_c^{-1}$. Since the function is quadratic, the classifier will produce quadratic decision boundaries between the classes (hence the name **Quadratic Discriminant Analysis**).\n",
    "\n",
    "### Implementing QDA\n",
    "We illustrate this on the `wine` dataset, which is a dataset of continuous features. We can implement QDA using the `QuadraticDiscriminantAnalysis` class from `sklearn.discriminant_analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "#read in wine datasets\n",
    "wine_train = pd.read_csv(\"../data/classification/wine/train.csv\")\n",
    "wine_test = pd.read_csv(\"../data/classification/wine/test.csv\")\n",
    "\n",
    "# Create X and y for train and test sets\n",
    "X_train = wine_train.drop(columns=[\"Y\"])\n",
    "y_train = wine_train[\"Y\"]\n",
    "X_test = wine_test.drop(columns=[\"Y\"])\n",
    "y_test = wine_test[\"Y\"]\n",
    "\n",
    "# Print the class priors for the train and test set\n",
    "print(\"Class priors in train set:\")\n",
    "print(wine_train[\"Y\"].value_counts(normalize=True))\n",
    "print(\"Class priors in test set:\")\n",
    "print(wine_test[\"Y\"].value_counts(normalize=True))\n",
    "\n",
    "# Initialize and fit the classifier.\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and classes for the train and test set\n",
    "train_proba = clf.predict_proba(X_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_proba = clf.predict_proba(X_test)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "# Compute and print accuracy of our predictions (as a percentage) on train and test sets\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Train accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute confusion matrices for train and test sets\n",
    "train_cm = confusion_matrix(y_train, train_predictions, labels=clf.classes_)\n",
    "test_cm = confusion_matrix(y_test, test_predictions, labels=clf.classes_)\n",
    "# Compute percentage matrices: each element divided by the overall total of the confusion matrix\n",
    "train_frac = (train_cm / train_cm.sum()) * 100\n",
    "test_frac = (test_cm / test_cm.sum()) * 100\n",
    "# Compute row-normalized matrices: each row divided by its total (in percentage)\n",
    "train_row_norm = (train_cm / train_cm.sum(axis=1, keepdims=True)) * 100\n",
    "test_row_norm = (test_cm / test_cm.sum(axis=1, keepdims=True)) * 100\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Create a figure with 3 rows and 2 columns of subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(6,6))\n",
    "# First row: standard confusion matrices\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title('Train predictions')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('True')\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test predictions')\n",
    "axes[0,1].set_xlabel('Predicted')\n",
    "axes[0,1].set_ylabel('True')\n",
    "\n",
    "# Second row: fraction matrices\n",
    "sns.heatmap(train_frac, annot=True, fmt='.2f', cmap='Greens', ax=axes[1,0])\n",
    "axes[1,0].set_title('Train prediction %')\n",
    "axes[1,0].set_xlabel('Predicted')\n",
    "axes[1,0].set_ylabel('True')\n",
    "sns.heatmap(test_frac, annot=True, fmt='.2f', cmap='Greens', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test prediction %')\n",
    "axes[1,1].set_xlabel('Predicted')\n",
    "axes[1,1].set_ylabel('True')\n",
    "\n",
    "# Third row: row-normalized matrices\n",
    "sns.heatmap(train_row_norm, annot=True, fmt='.2f', cmap='Oranges', ax=axes[2,0])\n",
    "axes[2,0].set_title('Train prediction % per true label')\n",
    "axes[2,0].set_xlabel('Predicted')\n",
    "axes[2,0].set_ylabel('True')\n",
    "sns.heatmap(test_row_norm, annot=True, fmt='.2f', cmap='Oranges', ax=axes[2,1])\n",
    "axes[2,1].set_title('Test prediction % per true label')\n",
    "axes[2,1].set_xlabel('Predicted')\n",
    "axes[2,1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "### LDA assumptions\n",
    "In LDA, we assume that the joint distribution of the features given the class is a multi-variate Gaussian distribution with mean vector $\\mu_c \\in \\mathbb{R}^n$, but we assume that the covariance matrix $\\Sigma_c$ is the same for all classes, i.e. $\\Sigma_c = \\Sigma$ for all $c \\in \\mathcal{C}$. This means that we are assuming that the features are independent given the class, but we are also assuming that the covariance matrix is the same for all classes. In other words, the features are not assumed to be independent given the class, rather, they are assumed to be correlated in the same way across all classes. This is a weaker assumption than QDA, and it allows us to estimate a single covariance matrix $\\Sigma$ for all classes, rather than a separate covariance matrix for each class.\n",
    "\n",
    "The posterior is given by\n",
    "\\begin{align*}\n",
    "    p(c | \\vec{x}) & = \\frac{p(c)}{p(\\vec{x})} \\cdot p(\\vec{x} | c) \\\\\n",
    "    & = \\frac{1}{p(\\vec{x})} \\cdot \\frac{p(c)}{\\sqrt{(2 \\pi)^n |\\Sigma|}} \\exp \\left( -\\frac{1}{2} (\\vec{x} - \\mu_c)^\\top \\Sigma^{-1} (\\vec{x} - \\mu_c) \\right).\n",
    "\\end{align*}\n",
    "\n",
    "Taking the log of both sides and setting $$A_c = \\log p(c) - \\frac{n}{2} \\log(2 \\pi) - \\frac{1}{2} \\log(|\\Sigma|),$$ we get the following formula for the log posterior:\n",
    "\\begin{equation*}\n",
    "    \\log p(c \\mid \\vec{x}) = A_c - \\frac{1}{2} (\\vec{x} - \\mu_c)^\\top \\Sigma^{-1} (\\vec{x} - \\mu_c) - \\frac{1}{2} \\log p(\\vec{x}).\n",
    "\\end{equation*}\n",
    "\n",
    "The hard classifier is then obtained by taking the class with the highest log posterior:\n",
    "\\begin{align*}\n",
    "    F_{\\textup{LDA}}(\\vec{x}) & = \\argmax_{c \\in \\mathcal{C}} \\; \\log p(c | \\vec{x})\\\\\n",
    "    & = \\argmax_{c \\in \\mathcal{C}} \\; \\left( A_c - \\frac{1}{2} (\\vec{x} - \\mu_c)^\\top \\Sigma^{-1} (\\vec{x} - \\mu_c) - \\frac{1}{2} \\log p(\\vec{x}) \\right)\\\\\n",
    "    & = \\argmin_{c \\in \\mathcal{C}} \\; \\left( (\\vec{x} - \\mu_c)^\\top \\Sigma^{-1} (\\vec{x} - \\mu_c) - A_c \\right),\n",
    "\\end{align*}\n",
    "where we have dropped the term $-\\frac{1}{2} \\log p(\\vec{x})$ because it does not depend on $c$.\n",
    "\n",
    "### Implementing LDA\n",
    "We illustrate this on the `wine` dataset, which is a dataset of continuous features. We can implement LDA using the `LinearDiscriminantAnalysis` class from `sklearn.discriminant_analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#read in wine datasets\n",
    "wine_train = pd.read_csv(\"../data/classification/wine/train.csv\")\n",
    "wine_test = pd.read_csv(\"../data/classification/wine/test.csv\")\n",
    "\n",
    "# Create X and y for train and test sets\n",
    "X_train = wine_train.drop(columns=[\"Y\"])\n",
    "y_train = wine_train[\"Y\"]\n",
    "X_test = wine_test.drop(columns=[\"Y\"])\n",
    "y_test = wine_test[\"Y\"]\n",
    "\n",
    "# Print the class priors for the train and test set\n",
    "print(\"Class priors in train set:\")\n",
    "print(wine_train[\"Y\"].value_counts(normalize=True))\n",
    "print(\"Class priors in test set:\")\n",
    "print(wine_test[\"Y\"].value_counts(normalize=True))\n",
    "\n",
    "# Initialize and fit the classifier.\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and classes for the train and test set\n",
    "train_proba = clf.predict_proba(X_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_proba = clf.predict_proba(X_test)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "# Compute and print accuracy of our predictions (as a percentage) on train and test sets\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Train accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Compute confusion matrices for train and test sets\n",
    "train_cm = confusion_matrix(y_train, train_predictions, labels=clf.classes_)\n",
    "test_cm = confusion_matrix(y_test, test_predictions, labels=clf.classes_)\n",
    "# Compute percentage matrices: each element divided by the overall total of the confusion matrix\n",
    "train_frac = (train_cm / train_cm.sum()) * 100\n",
    "test_frac = (test_cm / test_cm.sum()) * 100\n",
    "# Compute row-normalized matrices: each row divided by its total (in percentage)\n",
    "train_row_norm = (train_cm / train_cm.sum(axis=1, keepdims=True)) * 100\n",
    "test_row_norm = (test_cm / test_cm.sum(axis=1, keepdims=True)) * 100\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Create a figure with 3 rows and 2 columns of subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(6,6))\n",
    "# First row: standard confusion matrices\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title('Train predictions')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('True')\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test predictions')\n",
    "axes[0,1].set_xlabel('Predicted')\n",
    "axes[0,1].set_ylabel('True')\n",
    "\n",
    "# Second row: fraction matrices\n",
    "sns.heatmap(train_frac, annot=True, fmt='.2f', cmap='Greens', ax=axes[1,0])\n",
    "axes[1,0].set_title('Train prediction %')\n",
    "axes[1,0].set_xlabel('Predicted')\n",
    "axes[1,0].set_ylabel('True')\n",
    "sns.heatmap(test_frac, annot=True, fmt='.2f', cmap='Greens', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test prediction %')\n",
    "axes[1,1].set_xlabel('Predicted')\n",
    "axes[1,1].set_ylabel('True')\n",
    "\n",
    "# Third row: row-normalized matrices\n",
    "sns.heatmap(train_row_norm, annot=True, fmt='.2f', cmap='Oranges', ax=axes[2,0])\n",
    "axes[2,0].set_title('Train prediction % per true label')\n",
    "axes[2,0].set_xlabel('Predicted')\n",
    "axes[2,0].set_ylabel('True')\n",
    "sns.heatmap(test_row_norm, annot=True, fmt='.2f', cmap='Oranges', ax=axes[2,1])\n",
    "axes[2,1].set_title('Test prediction % per true label')\n",
    "axes[2,1].set_xlabel('Predicted')\n",
    "axes[2,1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
