{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classifiers\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Outline\n",
    "In this notebook, we discuss a few different (but closely related) classifiers based on Baye's Theorems:\n",
    "1. Bayes Classifier\n",
    "2. Naive Bayes Classifier\n",
    "3. Linear Discriminant Analysis\n",
    "4. Quadratic Discriminant Analysis\n",
    "I refer to these as Bayesian classifiers because they are all based on Baye's Theorem, but differ in the assumptions they make about the distribution of the variables in the dataset. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of dataset\n",
    "In this notebook, we assume that we are given a labelled (training) dataset with the following format:\n",
    "- There are features $X_1,\\dotsc,X_n$ (continuous and/or discrete).\n",
    "- There is a categorical target $Y$ which takes values in some finite set $\\mathcal{C} = \\{c_1,\\dotsc,c_k\\}$. \n",
    "- There are $m$ instances in our dataset. \n",
    "- As usual, we denote by $\\mathbf{x}_i \\in \\mathbb{R}^m$ the column vector of features for the $i$-th feature, and by $\\mathbf{y} \\in \\mathcal{C}^m$ the column vector of true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Soft) Classifiers\n",
    "Our goal is to predict the label $c \\in \\mathcal{C}$ given a row of features $\\vec{x} \\in \\mathbb{R}^n$. Thus, we want to construct a **classifier**, i.e. a function\n",
    "\\begin{equation*}\n",
    "    F : \\mathbb{R}^n \\to \\mathcal{C},\n",
    "\\end{equation*}\n",
    "which assigns to any instance $\\vec{x} = (x_1,\\dotsc,x_n)$ of features a class (or label) $F(\\vec{x}) \\in \\mathcal{C}$. \n",
    "\n",
    "For this purpose, it is natural to first construct a **soft classifier**, which assigns to each instance $\\vec{x}$ an entire probability distribution over the classes $\\mathcal{C}$. Since we are in the discrete case, asking for a probability distribution over the classes is equivalent to asking for a **probability mass function** (PMF), i.e. a function\n",
    "\\begin{equation*}\n",
    "    p: \\mathcal{C} \\to [0,1],\n",
    "\\end{equation*}\n",
    "such that $\\sum_{c \\in \\mathcal{C}} p(c) = 1$. \n",
    "\n",
    "So, formally speaking, a soft classifier can be thought of as a function\n",
    "\\begin{align*}\n",
    "    \\mathbb{R}^n & \\to \\{ \\textup{probability distributions over } \\mathcal{C} \\}, \\\\\n",
    "    \\vec{x} & \\mapsto p_{\\vec{x}},\n",
    "\\end{align*}\n",
    "which assigns to each instance $\\vec{x}$ a probability distribution $p_{\\vec{x}}$ over the classes $\\mathcal{C}$. The hard classifier $F$ is then obtained by taking the class with the highest probability:\n",
    "\\begin{equation*}\n",
    "    F(\\vec{x}) = \\argmax_{c \\in \\mathcal{C}} \\; p_{\\vec{x}}(c).\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier\n",
    "\n",
    "### The most natural soft classifier\n",
    "Consider the problem of assigning to each instance $\\vec{x}$ a probability distribution over the classes $\\mathcal{C}$. Such a distribution needs to come (somehow) from the data. There is basically only one (natural) way to obtain such a distribution from the dataset, namely, we can use the **conditional probability** of the class given the features:\n",
    "\\begin{align*}\n",
    "    p_{\\vec{x}} : \\mathcal{C} & \\to [0,1], \\\\\n",
    "    c & \\mapsto p(c | \\vec{x}).\n",
    "\\end{align*}\n",
    "This is the **Bayes Classifier**. It is the most natural classifier, because the logic is very intuitive and natural: we look at the data and say \"in the training dataset, given that the features where $\\vec{x}$, the probability distribution of the labels was $p(c | \\vec{x})$, so I'm going to predict the same will also hold for future data in which the features are $\\vec{x}$\".\n",
    "\n",
    "To convert this soft classifier into a hard classifier, we take the class with the highest probability condition on the features being equalt to $\\vec{x}$:\n",
    "\\begin{equation*}\n",
    "    F(\\vec{x}) = \\argmax_{c \\in \\mathcal{C}} \\; p(c | \\vec{x}).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of the Bayes Classifier\n",
    "The Bayes Classifier is a very natural classifier, but it has a few limitations. We give two of them below:\n",
    "1. **Unseen features**: If we are given an instance $\\vec{x}$ which does not appear in the training set, then the Bayes classifier will not know how to assign a probability distribution to the classes. There are a couple of potential ways to deal with this issue, but they are all not very satisfying:\n",
    "    - We can assign a uniform distribution over the classes, i.e. \n",
    "    \\begin{equation*}\n",
    "        p(c | \\vec{x}) = \\frac{1}{|\\mathcal{C}|} \\textup{ for all } c \\in \\mathcal{C}.\n",
    "    \\end{equation*}\n",
    "    - We can assign the prior distribution of the classes, i.e. \n",
    "    \\begin{equation*}\n",
    "        p(c | \\vec{x}) = p(c) \\textup{ for all } c \\in \\mathcal{C}.\n",
    "    \\end{equation*}\n",
    "    - If the features are continuous (or more generally, if we have a way of distinguishing feature values \"near\" to $\\vec{x}$), then we  can consider a small sample of our training data near the point $\\vec{x} \\in \\mathbb{R}^n$ and use the conditional probability of the classes given this sample. \n",
    "2. **Computationally expensive**: The Bayes classifier requires us to compute the conditional probability of the classes given the features, which is computationally expensive. This is not a problem if we have a small number of features and classes, but it can be very expensive if we have many features and/or classes. Moreover, in general there aren't any particular assumptions we can make about the joint distributions which would allow us to compute the conditional probabilities in a more efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Bayes Classifier\n",
    "Just to keep up with our ongoing tradition of implementing ML models from scratch, we also implement the Bayes Classifier below, with some caveats:\n",
    "- We assume that the features are discrete, so we can use a simple frequency count to estimate the conditional probabilities.\n",
    "- We assume that we have a small number of features and classes, so we can use a brute-force approach to compute the conditional probabilities.\n",
    "- In the case of unseen features, we assign the prior distribution of the classes to the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class MyBayesClassifier:\n",
    "    \"\"\"\n",
    "    MyBayesClassifier implements a simple Bayesian classifier for categorical data.\n",
    "    It computes class priors and conditional probabilities based on the training data,\n",
    "    and uses these estimates to compute posterior probabilities for new observations.\n",
    "    If an unseen combination of feature values is encountered during prediction, the\n",
    "    classifier defaults to the overall class priors.\n",
    "\n",
    "    Attributes:\n",
    "        X : pandas.DataFrame or array\n",
    "            The feature data used during training. Internally, X is stored as a DataFrame.\n",
    "        y : pandas.Series\n",
    "            The target (label) vector corresponding to the training examples.\n",
    "        classes : list\n",
    "            A sorted list of unique target class values extracted from y.\n",
    "        priors : dict\n",
    "            A mapping from each class label to its prior probability based on the training data.\n",
    "        cond_probs : dict\n",
    "            A dictionary mapping a tuple of feature values (representing a unique combination)\n",
    "            to a list of posterior probabilities for each class. The order of probabilities in\n",
    "            the list corresponds to the order of classes in the 'classes' attribute.\n",
    "\n",
    "    Methods:\n",
    "        fit(X, y):\n",
    "            Accepts a DataFrame or array of categorical features X and a vector y of targets.\n",
    "            Computes and stores the unique classes, the class prior probabilities, and the\n",
    "            conditional probabilities for each observed combination of feature values.\n",
    "        \n",
    "        predict_proba(X_test):\n",
    "            Given new observations (as a DataFrame or array), returns a DataFrame of\n",
    "            posterior probabilities for each class. For any row where the feature combination\n",
    "            is unseen in the training data, the overall class priors are returned.\n",
    "        \n",
    "        predict(X_test):\n",
    "            Uses predict_proba to calculate the posterior probabilities, and returns a Series\n",
    "            containing the class label with the highest probability for each observation.\n",
    "            \n",
    "        get_posteriors():\n",
    "            Returns a DataFrame listing every possible combination of feature values (based on the training data)\n",
    "            and the corresponding conditional distribution (posterior probabilities) of y. There is one column\n",
    "            per feature and one column per class.\n",
    "        \n",
    "        unseen_instances(X_test):\n",
    "            Given a new test dataset, returns a DataFrame containing the rows representing feature combinations\n",
    "            that were not observed in the training data.\n",
    "            \n",
    "    Example usage:\n",
    "        clf = MyBayesClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        proba = clf.predict_proba(X_test)\n",
    "        predictions = clf.predict(X_test)\n",
    "        all_posteriors = clf.get_posteriors()\n",
    "        new_instances = clf.unseen_instances(X_test)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "        self.cond_probs = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes classes, priors, and conditional probabilities for \n",
    "        each unique combination of feature values in X.\n",
    "        \n",
    "        Parameters:\n",
    "            X (pd.DataFrame or np.array): Features (categorical).\n",
    "            y (pd.Series or np.array): Column vector of targets.\n",
    "        \"\"\"\n",
    "        # If X is not a DataFrame, convert it.\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        self.X = X.copy()\n",
    "        self.y = pd.Series(y).copy()\n",
    "        \n",
    "        # Determine the list of classes.\n",
    "        self.classes = sorted(self.y.unique())\n",
    "        \n",
    "        # Compute class priors.\n",
    "        prior_series = self.y.value_counts(normalize=True)\n",
    "        self.priors = prior_series.to_dict()\n",
    "        \n",
    "        # Compute conditional probabilities for each observed combination.\n",
    "        # Concatenate X and y for grouping.\n",
    "        df_train = self.X.copy()\n",
    "        df_train[\"target\"] = self.y\n",
    "        \n",
    "        # Group by the feature columns.\n",
    "        grouped = df_train.groupby(list(self.X.columns))\n",
    "        \n",
    "        for combo, group in grouped:\n",
    "            # Ensure combo is a tuple even if single feature.\n",
    "            if not isinstance(combo, tuple):\n",
    "                combo = (combo,)\n",
    "                \n",
    "            # Count occurrences of each class for this feature combination.\n",
    "            counts = group[\"target\"].value_counts().to_dict()\n",
    "            total = sum(counts.values())\n",
    "            \n",
    "            # Build the posterior probability vector in the order of self.classes.\n",
    "            prob_vector = [counts.get(cls, 0) / total for cls in self.classes]\n",
    "            \n",
    "            self.cond_probs[combo] = prob_vector\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        For each row in X_test, returns the posterior probabilities over classes.\n",
    "        If a combination of feature values is unseen, the overall class priors are used.\n",
    "        \n",
    "        Parameters:\n",
    "            X_test (pd.DataFrame or np.array): Test features.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame of posterior probabilities with columns corresponding\n",
    "                          to the classes.\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if necessary.\n",
    "        if not isinstance(X_test, pd.DataFrame):\n",
    "            X_test = pd.DataFrame(X_test, columns=self.X.columns)\n",
    "        \n",
    "        # Define a lookup function for a row of features.\n",
    "        def lookup(row):\n",
    "            combo = tuple(row)\n",
    "            if combo in self.cond_probs:\n",
    "                return self.cond_probs[combo]\n",
    "            else:\n",
    "                # Return priors in the order of self.classes.\n",
    "                return [self.priors.get(cls, 0) for cls in self.classes]\n",
    "        \n",
    "        # Apply the lookup along axis=1.\n",
    "        proba_series = X_test.apply(lookup, axis=1)\n",
    "        proba_df = pd.DataFrame(proba_series.tolist(), \n",
    "                                index=X_test.index, \n",
    "                                columns=self.classes)\n",
    "        return proba_df\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Returns the predicted class label for each row in X_test.\n",
    "        \n",
    "        Parameters:\n",
    "            X_test (pd.DataFrame or np.array): Test features.\n",
    "        \n",
    "        Returns:\n",
    "            pd.Series: Predicted class for each row.\n",
    "        \"\"\"\n",
    "        proba_df = self.predict_proba(X_test)\n",
    "        # Return the class with maximum probability along each row.\n",
    "        predictions = proba_df.idxmax(axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    def get_posteriors(self):\n",
    "        \"\"\"\n",
    "        Returns a DataFrame with every possible combination of feature values from the training data \n",
    "        and the corresponding conditional distribution (posterior probabilities) of y.\n",
    "        \n",
    "        The resulting DataFrame contains one column per feature and one column per class.\n",
    "        For any combination not observed during training, the overall class priors are returned.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame where each row represents a unique combination of feature values,\n",
    "                          with feature columns (same names as in training data) and additional columns for \n",
    "                          each class (named using the class value).\n",
    "        \"\"\"\n",
    "        # For each feature column, get unique sorted values.\n",
    "        unique_vals = {col: sorted(self.X[col].unique()) for col in self.X.columns}\n",
    "        # Generate all possible combinations.\n",
    "        combos = list(itertools.product(*[unique_vals[col] for col in self.X.columns]))\n",
    "        \n",
    "        records = []\n",
    "        for combo in combos:\n",
    "            record = {col: combo[i] for i, col in enumerate(self.X.columns)}\n",
    "            # Use cond_probs if seen; otherwise, use overall priors.\n",
    "            if combo in self.cond_probs:\n",
    "                probs = self.cond_probs[combo]\n",
    "            else:\n",
    "                probs = [self.priors.get(cls, 0) for cls in self.classes]\n",
    "            for idx, cls in enumerate(self.classes):\n",
    "                record[cls] = probs[idx]\n",
    "            records.append(record)\n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    def unseen_instances(self, X_test):\n",
    "        \"\"\"\n",
    "        Given a test dataset, returns the sub-dataframe containing rows that are not found \n",
    "        in the training data (i.e. their feature combination was not observed during training).\n",
    "        \n",
    "        Parameters:\n",
    "            X_test (pd.DataFrame or np.array): Test features.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Sub-dataframe of X_test of the unseen instances.\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if necessary.\n",
    "        if not isinstance(X_test, pd.DataFrame):\n",
    "            X_test = pd.DataFrame(X_test, columns=self.X.columns)\n",
    "        \n",
    "        # Mark rows whose feature combination is not in cond_probs.\n",
    "        unseen_mask = X_test.apply(lambda row: tuple(row) not in self.cond_probs, axis=1)\n",
    "        return X_test[unseen_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_train = pd.read_csv(\"../data/classification/car_evaluation/train.csv\")\n",
    "cars_test = pd.read_csv(\"../data/classification/car_evaluation/test.csv\")\n",
    "# Combine train and test data for consistent preprocessing.\n",
    "X_train = cars_train.drop(columns=[\"Y\"])\n",
    "y_train = cars_train[\"Y\"]\n",
    "X_test = cars_test.drop(columns=[\"Y\"])\n",
    "y_test = cars_test[\"Y\"]\n",
    "\n",
    "# Initialize and fit the classifier.\n",
    "clf = MyBayesClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and classes for the train and test set\n",
    "train_proba = clf.predict_proba(X_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_proba = clf.predict_proba(X_test)\n",
    "test_predictions = clf.predict(X_test)\n",
    "\n",
    "# append true and predicted values of train and test sets to the proba dataframes\n",
    "train_proba[\"Y_true\"] = y_train\n",
    "train_proba[\"Y_pred\"] = train_predictions\n",
    "test_proba[\"Y_true\"] = y_test\n",
    "test_proba[\"Y_pred\"] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>big</td>\n",
       "      <td>high</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>big</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>big</td>\n",
       "      <td>med</td>\n",
       "      <td>0.222142</td>\n",
       "      <td>0.039797</td>\n",
       "      <td>0.700434</td>\n",
       "      <td>0.037627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>high</td>\n",
       "      <td>0.222142</td>\n",
       "      <td>0.039797</td>\n",
       "      <td>0.700434</td>\n",
       "      <td>0.037627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1728 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1     X2     X3    X4     X5    X6       acc      good     unacc  \\\n",
       "0      high   high      2     2    big  high  0.000000  0.000000  1.000000   \n",
       "1      high   high      2     2    big   low  0.000000  0.000000  1.000000   \n",
       "2      high   high      2     2    big   med  0.222142  0.039797  0.700434   \n",
       "3      high   high      2     2    med  high  0.222142  0.039797  0.700434   \n",
       "4      high   high      2     2    med   low  0.000000  0.000000  1.000000   \n",
       "...     ...    ...    ...   ...    ...   ...       ...       ...       ...   \n",
       "1723  vhigh  vhigh  5more  more    med   low  0.000000  0.000000  1.000000   \n",
       "1724  vhigh  vhigh  5more  more    med   med  0.000000  0.000000  1.000000   \n",
       "1725  vhigh  vhigh  5more  more  small  high  0.000000  0.000000  1.000000   \n",
       "1726  vhigh  vhigh  5more  more  small   low  0.000000  0.000000  1.000000   \n",
       "1727  vhigh  vhigh  5more  more  small   med  0.000000  0.000000  1.000000   \n",
       "\n",
       "         vgood  \n",
       "0     0.000000  \n",
       "1     0.000000  \n",
       "2     0.037627  \n",
       "3     0.037627  \n",
       "4     0.000000  \n",
       "...        ...  \n",
       "1723  0.000000  \n",
       "1724  0.000000  \n",
       "1725  0.000000  \n",
       "1726  0.000000  \n",
       "1727  0.000000  \n",
       "\n",
       "[1728 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posteriors = clf.get_posteriors()\n",
    "posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>good</th>\n",
       "      <th>unacc</th>\n",
       "      <th>vgood</th>\n",
       "      <th>Y_true</th>\n",
       "      <th>Y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>acc</td>\n",
       "      <td>acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unacc</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>acc</td>\n",
       "      <td>acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unacc</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unacc</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unacc</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unacc</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unacc</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1382 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      acc  good  unacc  vgood Y_true Y_pred\n",
       "0     1.0   0.0    0.0    0.0    acc    acc\n",
       "1     0.0   0.0    1.0    0.0  unacc  unacc\n",
       "2     1.0   0.0    0.0    0.0    acc    acc\n",
       "3     0.0   1.0    0.0    0.0   good   good\n",
       "4     0.0   0.0    1.0    0.0  unacc  unacc\n",
       "...   ...   ...    ...    ...    ...    ...\n",
       "1377  0.0   0.0    1.0    0.0  unacc  unacc\n",
       "1378  0.0   0.0    1.0    0.0  unacc  unacc\n",
       "1379  0.0   0.0    1.0    0.0  unacc  unacc\n",
       "1380  0.0   0.0    1.0    0.0  unacc  unacc\n",
       "1381  0.0   1.0    0.0    0.0   good   good\n",
       "\n",
       "[1382 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>4</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>med</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>big</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>5more</td>\n",
       "      <td>4</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "      <td>3</td>\n",
       "      <td>more</td>\n",
       "      <td>big</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>med</td>\n",
       "      <td>3</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>low</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>big</td>\n",
       "      <td>med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>big</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X1     X2     X3    X4     X5    X6\n",
       "0      low    low      4  more    med   med\n",
       "1      med  vhigh      3     4    big   low\n",
       "2     high  vhigh  5more     4  small  high\n",
       "3      med    med      2     2  small   low\n",
       "4     high   high      3  more    big  high\n",
       "..     ...    ...    ...   ...    ...   ...\n",
       "341    low    low      3     4    med   low\n",
       "342  vhigh    med      3  more    med  high\n",
       "343  vhigh    low      4     4  small   low\n",
       "344    med    med      3     2    big   med\n",
       "345    low   high      2     4    big   low\n",
       "\n",
       "[346 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.unseen_instances(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baye's Theorem recalled\n",
    " \n",
    "\n",
    "Recall from last class that we denote a row of features as $\\vec{x} = (x_1,\\dotsc,x_n)$, and we write $\\vec{X} = \\vec{x}$ to mean that $X_i = x_i$ for $i=1,\\dotsc,n$. Then, Bayes' theorem states that for any possible values $y$ and $\\vec{x}$ of $Y$ and $\\vec{X}$, we have\n",
    "\\begin{equation*}\n",
    "    P(c|\\vec{x}) = \\frac{P(\\vec{x}|c)P(c)}{P(\\vec{x})}.\n",
    "\\end{equation*}\n",
    "\n",
    "Here, we have the following terminology and concepts:\n",
    "- **Prior** $P(c)$:\n",
    "    \n",
    "    This is the probability $P(Y=c)$ that the target is $y$ before we have observed the features. We either make a reasonable assumption about this probability, or we estimate it from the data (e.g. by taking it to be the fraction of the training data that has target $y$ in the discrete case, or by computing an approximation to the density of the target at $y$ in the continuous case).\n",
    "- **Likelihood** $P(\\vec{x}|c)$:\n",
    "\n",
    "    This is the condition probability $P(\\vec{X} = \\vec{x}|Y=c)$ of observing the features $\\vec{x}$ given that the target is $y$.\n",
    "\n",
    "- **Evidence** $P(\\vec{x})$:\n",
    "\n",
    "    This is the probability $P(\\vec{X} = \\vec{x})$ of observing the features $\\vec{x}$, regardless of the target. \n",
    "\n",
    "- **Posterior** $P(c|\\vec{x})$:\n",
    "\n",
    "    This is the probability that the target is $y$ given that we have observed the features $\\vec{x}$. \n",
    "\n",
    "Note that the denominator $P(\\vec{x})$ can be computed as a sum over the possible classes of $Y$:\n",
    "\\begin{equation*}\n",
    "    P(\\vec{x}) = \\sum_{c \\in \\mathcal{C}} P(\\vec{x}|c)P(c).\n",
    "\\end{equation*}\n",
    "From this it becomes quite clear that the posteriors $P(c|\\vec{x})$ define a probability distribution over the possible class $c \\in \\mathcal{C}$. Indeed, each value is non-negative, and when we sum over all classes, we get $1$:\n",
    "\\begin{align*}\n",
    "    \\sum_{c \\in \\mathcal{C}} P(c| \\vec{x}) & = \\sum_{c \\in \\mathcal{C}} \\frac{P(\\vec{x}|c)P(c)}{ P(\\vec{x}) } = 1.\n",
    "\\end{align*}\n",
    "Quite logically, given a row of features $\\vec{x}$, our best bet for which class to predict for $Y$ would be the class $c \\in \\mathcal{C}$ with the largest posterior probability (because this is (in principle) the most likely class that was observed in the dataset). This is the basic idea behind Bayesian Classifiers. \n",
    "\n",
    "Now, let's go over the various classifiers mentioned in the outline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bayes Classifier\n",
    "This is the most direct classifier in the sense that it makes basically no assumptions about how the data are distributed; in practice, that means it is essentially impossible to implement. Nevertheless, it is still useful as a theoretical construct, and will help give us some intuition for Bayesian classifiers in general. \n",
    "\n",
    "First off, let's be completely clear that a classifier is a function\n",
    "\\begin{equation*}\n",
    "    F : \\mathbb{R}^n \\to \\mathcal{C},\n",
    "\\end{equation*}\n",
    "which assigns to any instance $\\vec{x} = (x_1,\\dotsc,x_n)$ of features a class $F(\\vec{x}) \\in \\mathcal{C}$. \n",
    "\n",
    "Now, the Baye's Classifier, which we denote by $F_{\\textup{Bayes}}$, does the natural probabilistic thing: it classifies a given $\\vec{x}$ into the class which bears the maximum posterior probability. Notationally, we write this as follows:\n",
    "\\begin{align*}\n",
    "    F_{\\textup{Bayes}}(\\vec{x}) & = \\argmax_{c \\in \\mathcal{C}} P(c \\mid \\vec{x})\\\\\n",
    "    & = \\argmax_{c \\in \\mathcal{C}} \\dfrac{ P(\\vec{x}| c) P(c) }{ P(\\vec{x}) }.\n",
    "\\end{align*}\n",
    "Note that the denominator is non-negative and fixed as we vary the classes, so to find the class which maximimizes the LHS, we may as well drop the denominator from the right-hand side. This yields the following simplified form of the Bayes Classifier:\n",
    "\\begin{align*}\n",
    "    F_{\\textup{Bayes}}(\\vec{x}) & = \\argmax_{c \\in \\mathcal{C}} P(\\vec{x}| c) P(c)\\\\\n",
    "    & = \\argmax_{c \\in \\mathcal{C}} P(\\vec{x},c),\n",
    "\\end{align*}\n",
    "where $P(\\vec{x},c)$ denotes the joint probability that $\\vec{X} = \\vec{x}$ and $Y = c$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
