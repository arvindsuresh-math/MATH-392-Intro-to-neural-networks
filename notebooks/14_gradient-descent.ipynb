{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db61c91",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "### Outline\n",
    "1. The idea behind gradient descent\n",
    "    - Introduction\n",
    "    - The three main spaces\n",
    "    - Models as functions\n",
    "    - Loss functions\n",
    "    - Model fitting\n",
    "    - Updating parameters\n",
    "    - Gradient descent algorithm\n",
    "2. Gradient descent\n",
    "3. Implementation for linear regression\n",
    "4. Implementation for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f935b6",
   "metadata": {},
   "source": [
    "## The idea behind gradient descent\n",
    "\n",
    "### Introduction\n",
    "Recall that in our general framework for ML, we have some features $X_1,\\dotsc,X_n$ and a target $Y$ (or multiple targets $Y_1,\\dotsc,Y_k$). The goal is to \"predict\" the values of $Y$ given the values of $X_i$ using a suitable function. We are equipped with a labelled dataset\n",
    "\\begin{equation*}\n",
    "    \\mathcal{D} = \\{(x_i, y_i) \\mid i=1,\\dotsc,m\\},\n",
    "\\end{equation*}\n",
    "where each $x_i \\in \\mathbb{R}^n$ is a sample of features and $y_i \\in \\mathbb{R}$ is the corresponding target. The goal is to find a(n approximation of the) function $F: \\mathbb{R}^n \\to \\mathbb{R}$ such that $F(x_i) = y_i$ for all $i=1,\\dotsc,m$. Then, given an unseen sample $x \\in \\mathbb{R}^n$, we can predict the value of $Y$ by computing $\\hat{y} = F(x)$. Recall that building such an approximation takes two steps:\n",
    "1. **Model selection**: Based on our exploratory data analysis, we choose a *parametric model*, i.e. a family of functions that are defined by a common formula involving some parameters $w \\in \\mathbb{R}^p$. \n",
    "2. **Model fitting**: we find the \"best\" parameters $\\hat{w} \\in \\mathbb{R}^p$ for the model given $\\mathcal{D}$, i.e. the parameters that minimize some measure of error between the predictions of the model and the true values of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9033f",
   "metadata": {},
   "source": [
    "### The three main spaces\n",
    "There are three spaces at play:\n",
    "1. **Feature space**: the space of all possible inputs $x \\in \\mathbb{R}^n$, corresponding to features $X_1,\\dotsc,X_n$.\n",
    "2. **Parameter space**: the space of all possible parameters $w \\in \\mathbb{R}^p$, corresponding to the weights and biases of the model.\n",
    "3. **Target space**: the space of all possible outputs $y \\in \\mathbb{R}$, corresponding to the target $Y$. (Of course, this would be $\\mathbb{R}^k$ if we are predicting multiple targets $Y_1,\\dotsc, Y_k$.)\n",
    "\n",
    "Conceptually, we can think of the parametric model as a function:\n",
    "\\begin{equation*}\n",
    "    F: (\\textup{feature space}) \\times (\\textup{parameter space}) \\to (\\textup{target space}).\n",
    "\\end{equation*}\n",
    "For a given input $x$ and parameters $w$, the output of the model is denoted $$\\hat{y} = F(x,w).$$ Depending on the context or need, we may:\n",
    "- Fix the input $x$ and vary the parameters $w$; this is done during training in order to find the optimum parameters given the data.\n",
    "- Fix the parameters $w$ and vary the input $x$; this is done during prediction in order to make predictions for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d6d69",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "Recall that the \"learning\" in \"machine learning\" refers to the process of learning the best parameters $w$ for the model, *given the training data*. For this, we use a loss function \n",
    "\\begin{equation*}\n",
    "    \\ell : (\\textup{target space}) \\times (\\textup{target space}) \\to \\mathbb{R},\n",
    "\\end{equation*}\n",
    "which takes a pair of values $(y, \\hat{y})$, where $y$ is the true value and $\\hat{y}$ is the predicted value, and returns a non-negative number $L(y,\\hat{y})$ that measures how well the prediction $\\hat{y}$ matches the true value $y$. \n",
    "\n",
    "**Example: Squared-error Loss.** For regression tasks with a single target, we use:\n",
    "$\\ell(y,\\hat{y}) = (y - \\hat{y})^2.$\n",
    "\n",
    "**Example: Binary cross-entropy loss.** For binary classification tasks, we use: $$\\ell(y,\\hat{y}) = -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y}),$$ where $y \\in \\{0,1\\}$ is the true value of the binary classification target, and $\\hat{y} \\in (0,1)$ is the predicted probability of the positive class.\n",
    "\n",
    "**Example: Categorical cross-entropy loss.** For multi-class classification tasks, we use the categorical cross-entropy loss function: $$\\ell(y,\\hat{y}) = y \\cdot (-\\log \\hat{y}) =  -\\sum_{i=1}^k y_i \\log(\\hat{y}_i),$$ where  $y = (y_1,\\dotsc,y_k) \\in \\Delta_k$ is the vertex of the probability simplex corresponding to the true class (i.e. $\\hat{y}_c = 1$ for the true class $i=c$ and $y_i = 0$ otherwise), and $\\hat{y} = (\\hat{y}_1,\\dotsc,\\hat{y}_k) \\in \\Delta_k$ is the vector of predicted probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349b4c6",
   "metadata": {},
   "source": [
    "### Sample and total loss \n",
    "Suppose our model is of the form $\\hat{y} = F(x;w)$. \n",
    "For a given sample (row) $(x,y) \\in \\mathcal{D}$, the *sample loss* is defined as\n",
    "\\begin{equation*}\n",
    "    L(w; x,y) = \\ell(y,\\hat{y}) = \\ell(y, F(x;w)).\n",
    "\\end{equation*}\n",
    "The semi-colon is used to indicate that, although the sample loss depends on both $w$ and $(x,y)$, we want to leave $(x,y)$ fixed (where $x$ is in the feature space and $y$ is in the target space) and think of sample loss as a function of $w$ alone:\n",
    "\\begin{equation*}\n",
    "    L : (\\textup{parameter space}) \\to \\mathbb{R}.\n",
    "\\end{equation*}\n",
    "Then, averaging over all samples $(x,y) \\in \\mathcal{D}$, we can define the *total loss* as\n",
    "\\begin{equation*}\n",
    "    \\mathcal{L}(w; \\mathcal{D}) = \\frac{1}{|\\mathcal{D}|} \\sum_{(x,y) \\in \\mathcal{D}} L(w; x,y).\n",
    "\\end{equation*}\n",
    "The total loss is again considered a function of the parameters $w$ alone, i.e.\n",
    "\\begin{equation*}\n",
    "    \\mathcal{L} : (\\textup{parameter space}) \\to \\mathbb{R}.\n",
    "\\end{equation*}\n",
    "The total loss is a measure of how well the model with parameters $w$ fits the data $\\mathcal{D}$. \n",
    "Typically, $\\mathcal{L}$ takes only non-negative values, and the goal of model training is to find the parameters $w$ that minimize the loss function. That is, denoting the fitted parameters by $\\hat{w}$, we want to solve the optimization problem:\n",
    "\\begin{equation*}\n",
    "    \\hat{w} = \\argmin_{w \\in \\textup{parameter space}} \\mathcal{L}(w; \\mathcal{D})\n",
    "\\end{equation*}\n",
    "For example, in linear regression, we use the mean squared error (MSE) as the loss function, and we've seen previously that the optimal parameters can be found in closed form by solving the normal equations. However, such a closed form solution does not in general exist for other models, so we instead resort to iterative optimization algorithms. We already saw an example of this, namely, the perceptron algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101bb74",
   "metadata": {},
   "source": [
    "### Updating parameters\n",
    "In order to optimize our model parameters, our strategy is to iteratively update the parameters in such a way that the loss function decreases with respect to the new parameters. That is, if the current value of the parameters is $w$, we want to find a vector of updates $\\Delta w$ such that:\n",
    "\\begin{equation*}\n",
    "    \\mathcal{L}(w - \\Delta w; \\mathcal{D}) < \\mathcal{L}(w; \\mathcal{D}).\n",
    "\\end{equation*}\n",
    "This begs the question: how should we obtain suitable updates $\\Delta w$? \n",
    "\n",
    "To accomplish this, we will make use of the **gradient of $\\mathcal{L}$ relative to $w$**, denoted $\\nabla_w \\mathcal{L}$:\n",
    "\\begin{equation*}\n",
    "    \\nabla_w \\mathcal{L} = \\begin{bmatrix} \\, \\frac{\\partial \\mathcal{L}}{\\partial w_1} & \\dotsb & \\frac{\\partial \\mathcal{L}}{\\partial w_p} \\, \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "where $w_1,\\dotsc,w_p$ are the components of the vector $w$. Note: here, we view the dataset $\\mathcal{D}$ as fixed and we view the loss $\\mathcal{L}$ as a function of the parameters $w$.  Now, to determine the updates $\\Delta w$, we take advantage of the following:\n",
    "\n",
    "**Fundamental property of gradients.** \n",
    "The gradient $\\nabla_w \\mathcal{L}$ is a vector that points in the direction of the steepest ascent of the loss function. Equivalently, the negative of the gradient $-\\nabla_w \\mathcal{L}$ points in the direction of the steepest descent of the loss function.\n",
    "\n",
    "Therefore, to minimize the loss, we should move in the opposite direction of the gradient, i.e. we should take our updates to be proportional to the negative of the gradient:\n",
    "\\begin{equation*}\n",
    "    \\Delta w = -\\alpha \\nabla_w \\mathcal{L}\n",
    "\\end{equation*}\n",
    "where $\\alpha$ is a positive scalar called the **learning rate**.\n",
    "\n",
    "**Remark.**\n",
    "Of course, for the above definition to make sense, we require that $\\mathcal{L}$ be differentiable with respect to $w$. Since $\\mathcal{L}$ is the average of the sample loss $L(w; x,y)$ over all samples $(x,y) \\in \\mathcal{D}$, this is equivalent to requiring that the sample loss $L(w; x,y)$ be differentiable with respect to $w$. This holds in all the examples we have seen so far, including linear regression with MSE, logistic regression with BCE, and softmax regression with CCE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f947b",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm\n",
    "Putting it all together, the gradient descent algorithm is (broadly speaking) as follows:\n",
    "1. Initialize the parameters $w$ to some random values and choose a learning rate $\\alpha$ (some small real number).\n",
    "2. Compute predictions $\\hat{y}$ for the dataset $\\mathcal{D}$ using the current parameters $w$.\n",
    "3. Compute the loss $\\mathcal{L}$ for the dataset $\\mathcal{D}$ using the current parameters $w$.\n",
    "4. Compute the gradient $\\nabla_w \\mathcal{L}$ of the loss with respect to the parameters.\n",
    "5. Update the parameters using the formula $w \\leftarrow w - \\alpha \\nabla_w \\mathcal{L}$.\n",
    "6. Repeat steps 2-5 until the loss converges to a minimum.\n",
    "\n",
    "In practice, there are variations of the gradient descent algorithm that are used to improve convergence speed and stability:\n",
    "1. **Batch gradient descent**: In this version, we compute the gradient $\\nabla_w \\mathcal{L}$ using the entire dataset, update the parameters, and iterate in this way. This is the most straightforward implementation of gradient descent, but it can be slow for large datasets.\n",
    "2. **Stochastic gradient descent (SGD)**: In this version, we compute the gradient using only a single instance from the dataset at each iteration, i.e. we compute each sample gradient $\\nabla_w L(w;x,y)$, update the weights, then move to the next sample and iterate in this way (once we finish one loop over the whole dataset, we repeat until we are satisfied with our loss). This can lead to faster convergence, but it can also be noisy and lead to oscillations in the loss function.\n",
    "3. **Mini-batch gradient descent**: This is a compromise between batch and stochastic gradient descent. In this version, we compute the gradient using a small random subset (mini-batch) of the dataset at each iteration (i.e. we average the sample gradients in the mini-batch). This can lead to faster convergence and more stable updates.\n",
    "4. **Momentum**: This is a technique that helps accelerate gradient descent in the relevant direction and dampens oscillations. It does this by adding a fraction of the previous update to the current update.\n",
    "5. **Adaptive learning rates**: This is a technique that adjusts the learning rate during training based on the progress of the optimization. This can help improve convergence speed and stability.\n",
    "\n",
    "**Remark.**\n",
    "Note that if we define the **sample gradient** as the gradient of the sample loss $L(w; x,y)$ with respect to $w$, i.e.\n",
    "\\begin{equation*}\n",
    "    \\nabla_w L = \\nabla_w L(w; x,y) = \\begin{bmatrix} \\, \\frac{\\partial L}{\\partial w_1} & \\dotsb & \\frac{\\partial L}{\\partial w_p} \\, \\end{bmatrix},\n",
    "\\end{equation*}\n",
    "then we can express the total gradient as the average of the sample gradients over all samples $(x,y) \\in \\mathcal{D}$:\n",
    "\\begin{equation*}\n",
    "    \\nabla_w \\mathcal{L}(w; \\mathcal{D}) = \\frac{1}{|\\mathcal{D}|} \\sum_{(x,y) \\in \\mathcal{D}} \\nabla_w L(w; x,y).\n",
    "\\end{equation*}\n",
    "This is an equality of row vectors in the parameter space $\\mathbb{R}^p$. Later, we will need to work with more general \"shapes\" of parameters, namely, we will have matrices of parameters. In that case, the gradients will be matrices as well, and we will need to be careful about the shapes of the matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926072a",
   "metadata": {},
   "source": [
    "## Computing $\\nabla_w \\mathcal{L}$\n",
    "\n",
    "### Theory and practice\n",
    "The gradient descent algorithm is powerful, but computing the gradient for many iterations can become very expensive (in terms of both time as well as memory). This is especially true when training on massive datasets, or when the model is complex (with many parameters), both of which hold in the case of deep learning.\n",
    "\n",
    "Thus, an *efficient* implementation of gradient descent is needed. To understand what this entails, there are two components:\n",
    "1. (Theoretical) We use the **chain rule** to break down our gradient computation into a number of intermediate steps. That is, we break up the function $\\mathcal{L}$ into a number of smaller functions, each of which is \"easy\" to differentiate, and then we apply the chain rule to appropriately multiply and add the intermediate derivatives to obtain the final product $\\nabla_w \\mathcal{L}$.\n",
    "2. (Practical) We need to minimize the computational and memory cost of the intermediate derivatives; this is accomplished by using a smart algorithm called **backpropagation**, which basically ensures that any intermediate derivative is computed only once and reused as needed.\n",
    "\n",
    "**Remark.** \n",
    "If you look up backpropagation on the internet, you will often find comments saying that \"it is just the chain rule\". This is flat out wrong:\n",
    "- The chain rule is a mathematical theorem that tells you how to compute the derivative of a composition of functions. It is \"simply\" an equation relating certain derivatives with certain sums and products of (intermediate) derivatives. \n",
    "- Backpropagation, on the other hand, is an algorithm that computes all the ingredients appearing in the chain rule in a particular order, stores them for re-use, and then combines them according to the chain rule to culminate in the final quantity $\\nabla_w \\mathcal{L}$. Thus, it can be understood as a particular \"clever\" implementation of the chain rule.\n",
    "\n",
    "This notebook is devoted to understanding the chain rule and where it comes from. The practical implementation of backpropagation is postponed until later. \n",
    "\n",
    "The main issue with the chain rule is not so much the conceptual understanding of it, but rather the notation, which can quickly pile up and turn into a confusing mess. We will sidestep this issue by systematically using the formalism of differentials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ec581",
   "metadata": {},
   "source": [
    "### Differentials\n",
    "Recall that a **variable** is simply a quantity (denoted $x,y,z$ etc.) that can vary. You should think that each variable has its own \"axis\" along which it varies. For example, the variable $x$ can be thought of as a point on the $x$-axis, and the variable $y$ can be thought of as a point on the $y$-axis. The **differential** of a variable $x$, denoted $dx$, is an abstract concept that represents an \"infinitesimal change\" in the variable $x$. \n",
    "\n",
    "A **variable vector** is simply a vector whose components are variables. If the vector is denoted (say) $z \\in \\mathbb{R}^h$, then the notation $z_i$ is used to denote the $i$-th component variable of $z$. The **differential** of a variable vector $z$ is defined as the vector whose components are the differentials of the components of $z$. That is:\n",
    "\\begin{equation*}\n",
    "    \\textup{If } z = \\begin{bmatrix} z_1 \\\\  \\vdots \\\\  z_h \\end{bmatrix}, \\textup{ then } dz = \\begin{bmatrix} dz_1 \\\\  \\vdots \\\\  dz_h \\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "NOTE: the shape of $dz$ is the same as the shape of $z$.\n",
    "\n",
    "For us, it is not so important to try to understand exactly what this means, rather, it is important to lay down certain rules for how to manipulate differentials. As we'll see, we will basically only use differentials as place-holders to track various partial derivatives and their products.\n",
    "\n",
    "**Fundamental rule of differentials.**\n",
    "If the variable $y$ is related to the variable vector $z \\in \\mathbb{R}^h$ by a differentiable function \n",
    "\\begin{equation*}\n",
    "    y = f(z) = f(z_1,\\dotsc,z_h),\n",
    "\\end{equation*}\n",
    "then the differentials are related by\n",
    "\\begin{align*}\n",
    "    dy & = \\nabla_z f \\cdot dz \\\\\n",
    "    & = \\begin{bmatrix} \\frac{\\partial f}{\\partial z_1} & \\dotsb & \\frac{\\partial f}{\\partial z_h} \\end{bmatrix} \\cdot \\begin{bmatrix} dz_1 \\\\  \\vdots \\\\  dz_h \\end{bmatrix} \\\\\n",
    "    & = \\frac{\\partial f}{\\partial z_1} dz_1 + \\dotsb + \\frac{\\partial f}{\\partial z_h} dz_h.\n",
    "\\end{align*}\n",
    "In particular, if the loss function $\\mathcal{L}$ is differentiable w.r.t. the weights $w \\in \\mathbb{R}^p$, then we have\n",
    "\\begin{equation*}\n",
    "    d\\mathcal{L} = \\nabla_w \\mathcal{L} \\cdot dw = \\frac{\\partial \\mathcal{L}}{\\partial w_1} dw_1 + \\dotsb + \\frac{\\partial \\mathcal{L}}{\\partial w_p} dw_p.\n",
    "\\end{equation*}\n",
    "Thus, we see that: *computing the gradient $\\nabla_w \\mathcal{L}$ is equivalent to writing $d\\mathcal{L}$ as a linear combination of the differentials $dw_1,\\dotsc,dw_p$.* \n",
    "\n",
    "\n",
    "**Examples.**\n",
    "The simplest case is the single-variable case $y = f(x)$. For example, if $u = \\sigma(z) = 1/({1 + \\exp(-z)})$, then\n",
    "\\begin{equation*}\n",
    "    du = \\sigma'(z) dz = u(1-u) dz.\n",
    "\\end{equation*}\n",
    "Similarly, if $u = \\textup{ReLU}(z) = \\max(0,z)$, then\n",
    "\\begin{equation*}\n",
    "    du = \\textup{ReLU}'(z) \\, dz = \\begin{cases}\n",
    "        0 & \\textup{if } z < 0 \\\\\n",
    "        1 & \\textup{if } z > 0\n",
    "    \\end{cases} dz.\n",
    "\\end{equation*}\n",
    "Note that the derivative of $\\textup{ReLU}(z)$ is almost the Heaviside function $H(z)$, except that it is undefined at $z=0$.\n",
    "\n",
    "**Example.**\n",
    "If a variable $z$ is the dot product $z = w^T x$, where $w,x \\in \\mathbb{R}^n$, then the differential is given by\n",
    "\\begin{equation*}\n",
    "    dz = (dw)^T x + w^T (dx) = \\sum_{i=1}^n w_i dx_i + \\sum_{i=1}^n x_i dw_i.\n",
    "\\end{equation*}\n",
    "If moreover $x$ is a constant vector, then $dx = 0$ (zero vector), so we have\n",
    "\\begin{equation*}\n",
    "    dz = (dw)^T x = \\sum_{i=1}^n x_i dw_i, \n",
    "\\end{equation*}\n",
    "which implies (by comparing with the fundamental rule of differentials) that\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial z}{\\partial w_i} = x_i, \\quad \\textup{for } i=1,\\dotsc,n.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6d463",
   "metadata": {},
   "source": [
    "## Example:  Linear regression with squared-error loss\n",
    "Suppose we have a regression task with a single target $Y$ and features $X_1,\\dotsc,X_n$. \n",
    "Suppose we have a linear regression model with parameters $w = (w_0,w_1,\\dotsc,w_n) \\in \\mathbb{R}^{n+1}$, which inputs a feature vector $x = (1,x_1,\\dotsc,x_n) \\in \\mathbb{R}^{n+1}$ and outputs a predicted target\n",
    "\\begin{equation*}\n",
    "    \\hat{y} = w^T x = w_0 + w_1 x_1 + \\dotsb + w_n x_n.\n",
    "\\end{equation*}\n",
    "The sample loss for a sample $(x,y) \\in \\mathcal{D}$ is then given by\n",
    "\\begin{equation*}\n",
    "    L(w; x,y) = (y - \\hat{y})^2 = (y - w^T x)^2,\n",
    "\\end{equation*}\n",
    "and the total loss is the mean squared error:\n",
    "\\begin{equation*}\n",
    "    \\mathcal{L}(w; \\mathcal{D}) = \\frac{1}{|\\mathcal{D}|} \\sum_{(x,y) \\in \\mathcal{D}} (y - w^T x)^2.\n",
    "\\end{equation*}\n",
    "\n",
    "To compute the sample gradient $\\nabla_w L(w;x,y)$, let's simply compute the differential $dL$. First, viewing $L$ as a function of $\\hat{y}$, we have $L = (\\hat{y} - y)^2$, and hence,\n",
    "\\begin{equation*}\n",
    "    dL = 2(\\hat{y} - y) d\\hat{y}.\n",
    "\\end{equation*}\n",
    "In a previous example, we saw that\n",
    "\\begin{equation*}\n",
    "    d \\hat{y} = x^T(dw) = x_0 dw_0 + x_1 dw_1 + \\dotsb + x_n d w_n,\n",
    "\\end{equation*}\n",
    "where $x_0 = 1$ is the bias term. Then, we can combine the two differentials to obtain\n",
    "\\begin{align*}\n",
    "    dL &= 2(\\hat{y} - y) d\\hat{y} \\\\\n",
    "    &= 2(\\hat{y} - y) x^T dw.\n",
    "\\end{align*}\n",
    "By the fundamental rule of differentials, we see that the sample gradient is given by\n",
    "\\begin{equation*}\n",
    "    \\nabla_w L(w;x,y) = 2(\\hat{y} - y) x^T \\in \\mathbb{R}^{n+1}.\n",
    "\\end{equation*}\n",
    "In particular, the $i$-th component of the sample gradient is given by\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial L}{\\partial w_i} = 2(\\hat{y} - y) x_i, \\quad \\textup{for } i=0,\\dotsc,n.\n",
    "\\end{equation*}\n",
    "Now, if there are $m$ samples in $\\mathcal{D}$, then denoting the vector of true values by ${\\mathbf{y}} \\in \\mathbb{R}^m$, the predicted values by $\\hat{\\mathbf{y}} \\in \\mathbb{R}^m$, and the $i$-th column by $\\mathbf{x}_j \\in \\mathbb{R}^m$, we have\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_i} &= \\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial L}{\\partial w_i} \\\\\n",
    "    &= \\frac{2}{m} (\\hat{\\mathbf{y}} - \\mathbf{y}) \\cdot \\mathbf{x}_i, \\quad \\textup{for } i=0,\\dotsc,n.\n",
    "\\end{align*}\n",
    "Thus, the total gradient is given by\n",
    "\\begin{align*}\n",
    "    \\nabla_w \\mathcal{L}(w;\\mathcal{D}) &= \\frac{2}{m} \\begin{bmatrix} \\, (\\hat{\\mathbf{y}} - \\mathbf{y}) \\cdot \\mathbf{x}_0 & \\dotsb & (\\hat{\\mathbf{y}} - \\mathbf{y}) \\cdot \\mathbf{x}_n \\, \\end{bmatrix} \\\\\n",
    "    &= \\frac{2}{m} (\\hat{\\mathbf{y}} - \\mathbf{y}) \\mathbf{X},\n",
    "\\end{align*}\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{m \\times (n+1)}$ is the design matrix, i.e. the matrix whose $i$-th column is the $i$-th feature vector $\\mathbf{x}_i$.\n",
    "\n",
    "Below, we implement gradient descent for linear regression with squared-error loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "600472ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, \n",
    "                 in_features, # Number of input features\n",
    "                 lr=0.01 # Learning rate, default is 0.01\n",
    "                 ):\n",
    "        # Initialize weights and bias randomly, similar to PyTorch's nn.Linear\n",
    "        self.weight = np.random.randn(in_features, 1) \n",
    "        self.bias = np.random.randn(1) \n",
    "\n",
    "        # Learning rate\n",
    "        self.lr = lr \n",
    "\n",
    "        # Placeholders for gradients\n",
    "        self.grad_weight = np.zeros_like(self.weight) # Gradient for weights\n",
    "        self.grad_bias = np.zeros_like(self.bias) # Gradient for bias\n",
    "\n",
    "        # Make a dataframe to store epoch, params, gradients, and losses\n",
    "        self.history = pd.DataFrame(columns=['epoch', 'weight', 'bias', 'grad_weight', 'grad_bias', 'loss'])\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute the linear transformation.\n",
    "        X should be a numpy array of shape (n_samples, in_features)\n",
    "        Returns a numpy array of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        return X.dot(self.weight) + self.bias\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error (MSE) loss.\n",
    "        y_true should be of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        return np.mean((y_pred - y_true) ** 2) \n",
    "\n",
    "    def backward(self, X, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute gradients for weights and bias using gradient descent update rules.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        error = y_pred - y_true  # shape (n_samples, 1)\n",
    "        self.grad_weight = (2 / m) * X.T.dot(error)\n",
    "        self.grad_bias = (2 / m) * np.sum(error)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update weights and bias using the computed gradients.\n",
    "        \"\"\"\n",
    "        self.weight -= self.lr * self.grad_weight\n",
    "        self.bias -= self.lr * self.grad_bias\n",
    "\n",
    "    def fit(self, X, y, \n",
    "            epochs=100, # Number of epochs to train\n",
    "            early_stopping_patience=10, # Number of epochs to wait for improvement before stopping\n",
    "            tol=1e-4 # Minimum change in loss to qualify as an improvement\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent.\n",
    "        X should be a numpy array (n_samples, in_features)\n",
    "        y should be a numpy array (n_samples, 1)\n",
    "        \n",
    "        Parameters:\n",
    "            epochs: Number of epochs to train.\n",
    "            early_stopping_patience: Number of epochs to wait for improvement before stopping.\n",
    "            tol: Minimum change in loss to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        best_loss = float('inf')\n",
    "        epochs_without_improve = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass to compute predictions\n",
    "            y_pred = self.forward(X)\n",
    "            # Compute loss\n",
    "            loss_val = self.loss(y_pred, y)\n",
    "            # Backward pass to compute gradients\n",
    "            self.backward(X, y_pred, y)\n",
    "\n",
    "            # Early stopping check\n",
    "            if loss_val < best_loss - tol: # If loss improved\n",
    "                best_loss = loss_val # Update best loss\n",
    "                epochs_without_improve = 0 # Reset counter\n",
    "            else:\n",
    "                epochs_without_improve += 1 # else, increment counter\n",
    "\n",
    "            # Print loss and log history every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss_val:.4f}\")\n",
    "\n",
    "                new_row = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'weight': [self.weight.flatten()],\n",
    "                    'bias': [self.bias.flatten()],\n",
    "                    'grad_weight': [self.grad_weight.flatten()],\n",
    "                    'grad_bias': [self.grad_bias.flatten()],\n",
    "                    'loss': [loss_val]\n",
    "                })\n",
    "                self.history = pd.concat([self.history, new_row], ignore_index=True)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.step()\n",
    "\n",
    "            # Early stopping condition\n",
    "            if epochs_without_improve >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "        print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec089ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "df_train = pd.read_csv('../data/regression/concrete_compressive_strength/train.csv')\n",
    "df_test = pd.read_csv('../data/regression/concrete_compressive_strength/test.csv')\n",
    "\n",
    "# Extract features and target variable, convert to numpy arrays\n",
    "X_train = df_train.drop(columns=['Y']).values\n",
    "y_train = df_train['Y'].values.reshape(-1, 1)\n",
    "X_test = df_test.drop(columns=['Y']).values\n",
    "y_test = df_test['Y'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3a3f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1510.7125\n",
      "Epoch 10, Loss: 132.6210\n",
      "Epoch 20, Loss: 113.1086\n",
      "Epoch 30, Loss: 110.8602\n",
      "Epoch 40, Loss: 109.7049\n",
      "Epoch 50, Loss: 109.0146\n",
      "Epoch 60, Loss: 108.5699\n",
      "Epoch 70, Loss: 108.2603\n",
      "Epoch 80, Loss: 108.0284\n",
      "Epoch 90, Loss: 107.8445\n",
      "Epoch 100, Loss: 107.6922\n",
      "Epoch 110, Loss: 107.5627\n",
      "Epoch 120, Loss: 107.4506\n",
      "Epoch 130, Loss: 107.3526\n",
      "Epoch 140, Loss: 107.2664\n",
      "Epoch 150, Loss: 107.1903\n",
      "Epoch 160, Loss: 107.1229\n",
      "Epoch 170, Loss: 107.0633\n",
      "Epoch 180, Loss: 107.0104\n",
      "Epoch 190, Loss: 106.9635\n",
      "Epoch 200, Loss: 106.9220\n",
      "Epoch 210, Loss: 106.8851\n",
      "Epoch 220, Loss: 106.8524\n",
      "Epoch 230, Loss: 106.8234\n",
      "Epoch 240, Loss: 106.7976\n",
      "Epoch 250, Loss: 106.7748\n",
      "Epoch 260, Loss: 106.7545\n",
      "Epoch 270, Loss: 106.7366\n",
      "Epoch 280, Loss: 106.7206\n",
      "Epoch 290, Loss: 106.7065\n",
      "Epoch 300, Loss: 106.6940\n",
      "Epoch 310, Loss: 106.6828\n",
      "Epoch 320, Loss: 106.6730\n",
      "Epoch 330, Loss: 106.6642\n",
      "Epoch 340, Loss: 106.6564\n",
      "Epoch 350, Loss: 106.6495\n",
      "Epoch 360, Loss: 106.6434\n",
      "Epoch 370, Loss: 106.6380\n",
      "Epoch 380, Loss: 106.6332\n",
      "Epoch 390, Loss: 106.6289\n",
      "Epoch 400, Loss: 106.6251\n",
      "Epoch 410, Loss: 106.6218\n",
      "Epoch 420, Loss: 106.6188\n",
      "Epoch 430, Loss: 106.6162\n",
      "Epoch 440, Loss: 106.6138\n",
      "Epoch 450, Loss: 106.6117\n",
      "Epoch 460, Loss: 106.6099\n",
      "Epoch 470, Loss: 106.6083\n",
      "Epoch 480, Loss: 106.6068\n",
      "Epoch 490, Loss: 106.6055\n",
      "Epoch 500, Loss: 106.6044\n",
      "Epoch 510, Loss: 106.6034\n",
      "Epoch 520, Loss: 106.6025\n",
      "Epoch 530, Loss: 106.6017\n",
      "Epoch 540, Loss: 106.6010\n",
      "Epoch 550, Loss: 106.6003\n",
      "Epoch 560, Loss: 106.5998\n",
      "Epoch 570, Loss: 106.5993\n",
      "Epoch 580, Loss: 106.5988\n",
      "Epoch 590, Loss: 106.5985\n",
      "Epoch 600, Loss: 106.5981\n",
      "Epoch 610, Loss: 106.5978\n",
      "Epoch 620, Loss: 106.5975\n",
      "Epoch 630, Loss: 106.5973\n",
      "Epoch 640, Loss: 106.5971\n",
      "Epoch 650, Loss: 106.5969\n",
      "Epoch 660, Loss: 106.5967\n",
      "Epoch 670, Loss: 106.5966\n",
      "Epoch 680, Loss: 106.5964\n",
      "Epoch 690, Loss: 106.5963\n",
      "Epoch 700, Loss: 106.5962\n",
      "Epoch 710, Loss: 106.5961\n",
      "Early stopping triggered at epoch 712.\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/143tjw8148nftd61z3w9qzv40000gp/T/ipykernel_4850/205095347.py:100: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.history = pd.concat([self.history, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LinearRegression(in_features=X_train.shape[1], # Number of features\n",
    "                         lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a8a7f",
   "metadata": {},
   "source": [
    "## Example: Logistic regression with binary cross-entropy loss\n",
    "\n",
    "Suppose we have a binary classification problem with target in $\\{0,1\\}$. Suppose we use a \n",
    "logistic regression model with parameters $w = (w_0,w_1,\\dotsc,w_n) \\in \\mathbb{R}^{n+1}$, which inputs a feature vector $x = (1,x_1,\\dotsc,x_n) \\in \\mathbb{R}^{n+1}$ and outputs a predicted probability\n",
    "\\begin{equation*}\n",
    "    \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}},\n",
    "\\end{equation*}\n",
    "where $z = w^T x \\in \\mathbb{R}$. The sample loss for a sample $(x,y)$ is then given by\n",
    "\\begin{align*}\n",
    "    L(w; x,y) &= -y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y}).\n",
    "\\end{align*}\n",
    "To compute $\\nabla_w L(w;x,y)$, we want to compute a formula for $dL$ in terms of $dw$. To do this, let's first compute $dL$ in terms of $d\\hat{y}$:\n",
    "\\begin{align*}\n",
    "    dL &= \\frac{\\partial L}{\\partial \\hat{y}} d\\hat{y} \\\\\n",
    "    &= \\left( -\\frac{y}{\\hat{y}} + \\frac{(1-y)}{1-\\hat{y}} \\right) d\\hat{y}.\n",
    "\\end{align*}\n",
    "Now, we can compute $d\\hat{y}$ in terms of $dz$:\n",
    "\\begin{align*}\n",
    "    d\\hat{y} &= \\sigma'(z) dz \\\\\n",
    "    &= \\sigma(z)(1-\\sigma(z)) dz\\\\\n",
    "    &= \\hat{y}(1-\\hat{y}) dz.\n",
    "\\end{align*}\n",
    "Combining these two, we compute $dL$ in terms of $dz$:\n",
    "\\begin{align*}\n",
    "    dL &= \\left( -\\frac{y}{\\hat{y}} + \\frac{(1-y)}{1-\\hat{y}} \\right) \\hat{y}(1-\\hat{y}) dz \\\\\n",
    "    &= \\left( -y(1-\\hat{y}) + (1-y)\\hat{y} \\right) dz \\\\\n",
    "    &= \\left( \\hat{y} - y \\right) dz.\n",
    "\\end{align*}\n",
    "We then compute $dz$ in terms of $dw$:\n",
    "\\begin{align*}\n",
    "    dz &= x^T (dw).\n",
    "\\end{align*}\n",
    "Putting it all together, we have\n",
    "\\begin{align*}\n",
    "    dL &= \\left( \\hat{y} - y \\right) dz \\\\\n",
    "    &= \\left( \\hat{y} - y \\right) x^T (dw).\n",
    "\\end{align*}\n",
    "By the fundamental rule of differentials, we see that the sample gradient is given by\n",
    "\\begin{align*}\n",
    "    \\nabla_w L(w;x,y) &= (\\hat{y} - y) x^T \\in \\mathbb{R}^{n+1}\\\\\n",
    "    &= \\begin{bmatrix} \\, (\\hat{y} - y) x_0 & \\dotsb & (\\hat{y} - y) x_n \\, \\end{bmatrix} \\in \\mathbb{R}^{n+1}.\n",
    "\\end{align*}\n",
    "Similar to the case of linear regression, we find that \n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\frac{1}{m} (\\hat{\\mathbf{y}} - \\mathbf{y})^T \\mathbf{x}_i, \\quad \\textup{for } i=0,\\dotsc,n.\n",
    "\\end{equation*}\n",
    "Thus, the total gradient is given by\n",
    "\\begin{align*}\n",
    "    \\nabla_w \\mathcal{L}(w;\\mathcal{D}) &= \\frac{1}{m} \\begin{bmatrix} \\, (\\hat{\\mathbf{y}} - \\mathbf{y})^T \\mathbf{x}_0 & \\dotsb & (\\hat{\\mathbf{y}} - \\mathbf{y})^T \\mathbf{x}_n \\, \\end{bmatrix} \\\\\n",
    "    &= \\frac{1}{m} (\\hat{\\mathbf{y}} - \\mathbf{y})^T \\mathbf{X},\n",
    "\\end{align*}\n",
    "\n",
    "Let's implement gradient descent for logistic regression with binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d95d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, \n",
    "                 in_features,   # Number of input features\n",
    "                 lr=0.01        # Learning rate\n",
    "                 ):\n",
    "        # Initialize weights and bias (similar to nn.Linear)\n",
    "        self.weight = np.random.randn(in_features, 1)\n",
    "        self.bias = np.random.randn(1)\n",
    "        \n",
    "        # Learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Placeholders for gradients\n",
    "        self.grad_weight = np.zeros_like(self.weight)\n",
    "        self.grad_bias = np.zeros_like(self.bias)\n",
    "        \n",
    "        # DataFrame for history tracking\n",
    "        self.history = pd.DataFrame(columns=['epoch', 'weight', 'bias', 'grad_weight', 'grad_bias', 'loss'])\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute the logistic regression output.\n",
    "        X: numpy array of shape (n_samples, in_features)\n",
    "        Returns: numpy array of shape (n_samples, 1) containing predicted probabilities.\n",
    "        \"\"\"\n",
    "        z = X.dot(self.weight) + self.bias\n",
    "        return sigmoid(z)\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the binary cross-entropy loss.\n",
    "        y_true: numpy array of shape (n_samples, 1) with binary labels {0,1}.\n",
    "        \"\"\"\n",
    "        # Add a small epsilon for numerical stability.\n",
    "        eps = 1e-15\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        return -np.mean( y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred) )\n",
    "\n",
    "    def backward(self, X, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute gradients for weight and bias.\n",
    "        Using the derived result: dL/dw = (y_pred - y_true) * x.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        error = y_pred - y_true  # (n_samples, 1)\n",
    "        self.grad_weight = (1 / m) * X.T.dot(error)   # (in_features, 1)\n",
    "        self.grad_bias = (1 / m) * np.sum(error)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update the parameters using the computed gradients.\n",
    "        \"\"\"\n",
    "        self.weight -= self.lr * self.grad_weight\n",
    "        self.bias -= self.lr * self.grad_bias\n",
    "\n",
    "    def fit(self, X, y, \n",
    "            epochs=100, \n",
    "            early_stopping_patience=10, \n",
    "            tol=1e-4):\n",
    "        \"\"\"\n",
    "        Train the logistic regression model using gradient descent.\n",
    "        \n",
    "        X: numpy array (n_samples, in_features)\n",
    "        y: numpy array (n_samples, 1) - expected binary labels {0,1}\n",
    "        epochs: maximum number of epochs for training.\n",
    "        early_stopping_patience: number of epochs with no significant improvement before stopping.\n",
    "        tol: the minimum decrease in loss required to count as an improvement.\n",
    "        \"\"\"\n",
    "        best_loss = float('inf')\n",
    "        epochs_without_improve = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            # Compute loss\n",
    "            loss_val = self.loss(y_pred, y)\n",
    "            # Backward pass (gradient computation)\n",
    "            self.backward(X, y_pred, y)\n",
    "\n",
    "            # Early stopping check\n",
    "            if loss_val < best_loss - tol:\n",
    "                best_loss = loss_val\n",
    "                epochs_without_improve = 0\n",
    "            else:\n",
    "                epochs_without_improve += 1\n",
    "\n",
    "            # Log history every 10 epochs\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss_val:.4f}\")\n",
    "                new_row = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'weight': [self.weight.flatten()],\n",
    "                    'bias': [self.bias.flatten()],\n",
    "                    'grad_weight': [self.grad_weight.flatten()],\n",
    "                    'grad_bias': [self.grad_bias.flatten()],\n",
    "                    'loss': [loss_val]\n",
    "                })\n",
    "                self.history = pd.concat([self.history, new_row], ignore_index=True)\n",
    "\n",
    "            # Update parameters\n",
    "            self.step()\n",
    "\n",
    "            # Early stopping condition\n",
    "            if epochs_without_improve >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "        print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f35b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in presidential election data\n",
    "df_train = pd.read_csv('../data/classification/presidential_election_binary/train.csv')\n",
    "df_test = pd.read_csv('../data/classification/presidential_election_binary/test.csv')\n",
    "\n",
    "features = ['edu_low_age_18', 'edu_low_age_45', 'edu_low_age_65', 'edu_mid_age_18',\n",
    "       'edu_mid_age_45', 'edu_mid_age_65', 'edu_high_age_18',\n",
    "       'edu_high_age_45', 'edu_high_age_65', 'race_wht_male',\n",
    "       'race_wht_female', 'race_blk_male', 'race_blk_female', 'marital_single', 'marital_married', 'marital_sepdiv','pop_density','income_percapita','income_10', 'income_10-15',\n",
    "       'income_15-25', 'income_25']\n",
    "\n",
    "X_train = df_train[features].values\n",
    "y_train = df_train['target'].values.reshape(-1, 1)\n",
    "X_test = df_test[features].values\n",
    "y_test = df_test['target'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27bc9545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7365\n",
      "Epoch 10, Loss: 1.6625\n",
      "Epoch 20, Loss: 1.1005\n",
      "Epoch 30, Loss: 0.8291\n",
      "Epoch 40, Loss: 0.6872\n",
      "Epoch 50, Loss: 0.6023\n",
      "Epoch 60, Loss: 0.5452\n",
      "Epoch 70, Loss: 0.5040\n",
      "Epoch 80, Loss: 0.4730\n",
      "Epoch 90, Loss: 0.4492\n",
      "Epoch 100, Loss: 0.4304\n",
      "Epoch 110, Loss: 0.4152\n",
      "Epoch 120, Loss: 0.4028\n",
      "Epoch 130, Loss: 0.3924\n",
      "Epoch 140, Loss: 0.3837\n",
      "Epoch 150, Loss: 0.3762\n",
      "Epoch 160, Loss: 0.3697\n",
      "Epoch 170, Loss: 0.3640\n",
      "Epoch 180, Loss: 0.3590\n",
      "Epoch 190, Loss: 0.3545\n",
      "Epoch 200, Loss: 0.3505\n",
      "Epoch 210, Loss: 0.3468\n",
      "Epoch 220, Loss: 0.3435\n",
      "Epoch 230, Loss: 0.3404\n",
      "Epoch 240, Loss: 0.3377\n",
      "Epoch 250, Loss: 0.3351\n",
      "Epoch 260, Loss: 0.3327\n",
      "Epoch 270, Loss: 0.3305\n",
      "Epoch 280, Loss: 0.3284\n",
      "Epoch 290, Loss: 0.3265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/143tjw8148nftd61z3w9qzv40000gp/T/ipykernel_56507/2566507609.py:104: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.history = pd.concat([self.history, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300, Loss: 0.3247\n",
      "Epoch 310, Loss: 0.3230\n",
      "Epoch 320, Loss: 0.3214\n",
      "Epoch 330, Loss: 0.3199\n",
      "Epoch 340, Loss: 0.3185\n",
      "Epoch 350, Loss: 0.3172\n",
      "Epoch 360, Loss: 0.3160\n",
      "Epoch 370, Loss: 0.3148\n",
      "Epoch 380, Loss: 0.3137\n",
      "Epoch 390, Loss: 0.3127\n",
      "Epoch 400, Loss: 0.3117\n",
      "Epoch 410, Loss: 0.3108\n",
      "Epoch 420, Loss: 0.3099\n",
      "Epoch 430, Loss: 0.3091\n",
      "Epoch 440, Loss: 0.3083\n",
      "Epoch 450, Loss: 0.3076\n",
      "Epoch 460, Loss: 0.3068\n",
      "Epoch 470, Loss: 0.3062\n",
      "Epoch 480, Loss: 0.3056\n",
      "Epoch 490, Loss: 0.3050\n",
      "Epoch 500, Loss: 0.3044\n",
      "Epoch 510, Loss: 0.3039\n",
      "Epoch 520, Loss: 0.3033\n",
      "Epoch 530, Loss: 0.3029\n",
      "Epoch 540, Loss: 0.3024\n",
      "Epoch 550, Loss: 0.3020\n",
      "Epoch 560, Loss: 0.3015\n",
      "Epoch 570, Loss: 0.3012\n",
      "Epoch 580, Loss: 0.3008\n",
      "Epoch 590, Loss: 0.3004\n",
      "Epoch 600, Loss: 0.3001\n",
      "Epoch 610, Loss: 0.2998\n",
      "Epoch 620, Loss: 0.2994\n",
      "Epoch 630, Loss: 0.2992\n",
      "Epoch 640, Loss: 0.2989\n",
      "Epoch 650, Loss: 0.2986\n",
      "Epoch 660, Loss: 0.2984\n",
      "Epoch 670, Loss: 0.2981\n",
      "Epoch 680, Loss: 0.2979\n",
      "Epoch 690, Loss: 0.2977\n",
      "Epoch 700, Loss: 0.2974\n",
      "Epoch 710, Loss: 0.2972\n",
      "Epoch 720, Loss: 0.2970\n",
      "Epoch 730, Loss: 0.2969\n",
      "Epoch 740, Loss: 0.2967\n",
      "Epoch 750, Loss: 0.2965\n",
      "Epoch 760, Loss: 0.2964\n",
      "Epoch 770, Loss: 0.2962\n",
      "Epoch 780, Loss: 0.2961\n",
      "Epoch 790, Loss: 0.2959\n",
      "Epoch 800, Loss: 0.2958\n",
      "Epoch 810, Loss: 0.2956\n",
      "Epoch 820, Loss: 0.2955\n",
      "Epoch 830, Loss: 0.2954\n",
      "Epoch 840, Loss: 0.2953\n",
      "Epoch 850, Loss: 0.2952\n",
      "Epoch 860, Loss: 0.2951\n",
      "Epoch 870, Loss: 0.2950\n",
      "Epoch 880, Loss: 0.2949\n",
      "Epoch 890, Loss: 0.2948\n",
      "Epoch 900, Loss: 0.2947\n",
      "Epoch 910, Loss: 0.2946\n",
      "Epoch 920, Loss: 0.2945\n",
      "Epoch 930, Loss: 0.2944\n",
      "Epoch 940, Loss: 0.2943\n",
      "Epoch 950, Loss: 0.2942\n",
      "Epoch 960, Loss: 0.2942\n",
      "Epoch 970, Loss: 0.2941\n",
      "Epoch 980, Loss: 0.2940\n",
      "Epoch 990, Loss: 0.2940\n",
      "Epoch 1000, Loss: 0.2939\n",
      "Epoch 1010, Loss: 0.2938\n",
      "Epoch 1020, Loss: 0.2938\n",
      "Epoch 1030, Loss: 0.2937\n",
      "Epoch 1040, Loss: 0.2937\n",
      "Epoch 1050, Loss: 0.2936\n",
      "Epoch 1060, Loss: 0.2936\n",
      "Epoch 1070, Loss: 0.2935\n",
      "Epoch 1080, Loss: 0.2935\n",
      "Epoch 1090, Loss: 0.2934\n",
      "Epoch 1100, Loss: 0.2934\n",
      "Epoch 1110, Loss: 0.2933\n",
      "Epoch 1120, Loss: 0.2933\n",
      "Epoch 1130, Loss: 0.2932\n",
      "Epoch 1140, Loss: 0.2932\n",
      "Epoch 1150, Loss: 0.2931\n",
      "Epoch 1160, Loss: 0.2931\n",
      "Epoch 1170, Loss: 0.2931\n",
      "Epoch 1180, Loss: 0.2930\n",
      "Epoch 1190, Loss: 0.2930\n",
      "Epoch 1200, Loss: 0.2929\n",
      "Epoch 1210, Loss: 0.2929\n",
      "Epoch 1220, Loss: 0.2929\n",
      "Epoch 1230, Loss: 0.2928\n",
      "Epoch 1240, Loss: 0.2928\n",
      "Epoch 1250, Loss: 0.2928\n",
      "Epoch 1260, Loss: 0.2928\n",
      "Epoch 1270, Loss: 0.2927\n",
      "Epoch 1280, Loss: 0.2927\n",
      "Epoch 1290, Loss: 0.2927\n",
      "Epoch 1300, Loss: 0.2926\n",
      "Epoch 1310, Loss: 0.2926\n",
      "Epoch 1320, Loss: 0.2926\n",
      "Epoch 1330, Loss: 0.2926\n",
      "Epoch 1340, Loss: 0.2925\n",
      "Epoch 1350, Loss: 0.2925\n",
      "Epoch 1360, Loss: 0.2925\n",
      "Epoch 1370, Loss: 0.2925\n",
      "Epoch 1380, Loss: 0.2924\n",
      "Epoch 1390, Loss: 0.2924\n",
      "Epoch 1400, Loss: 0.2924\n",
      "Epoch 1410, Loss: 0.2924\n",
      "Epoch 1420, Loss: 0.2924\n",
      "Epoch 1430, Loss: 0.2923\n",
      "Epoch 1440, Loss: 0.2923\n",
      "Epoch 1450, Loss: 0.2923\n",
      "Epoch 1460, Loss: 0.2923\n",
      "Epoch 1470, Loss: 0.2923\n",
      "Epoch 1480, Loss: 0.2922\n",
      "Epoch 1490, Loss: 0.2922\n",
      "Epoch 1500, Loss: 0.2922\n",
      "Epoch 1510, Loss: 0.2922\n",
      "Epoch 1520, Loss: 0.2922\n",
      "Epoch 1530, Loss: 0.2922\n",
      "Epoch 1540, Loss: 0.2921\n",
      "Epoch 1550, Loss: 0.2921\n",
      "Epoch 1560, Loss: 0.2921\n",
      "Epoch 1570, Loss: 0.2921\n",
      "Epoch 1580, Loss: 0.2921\n",
      "Epoch 1590, Loss: 0.2921\n",
      "Epoch 1600, Loss: 0.2920\n",
      "Epoch 1610, Loss: 0.2920\n",
      "Epoch 1620, Loss: 0.2920\n",
      "Epoch 1630, Loss: 0.2920\n",
      "Epoch 1640, Loss: 0.2920\n",
      "Epoch 1650, Loss: 0.2920\n",
      "Epoch 1660, Loss: 0.2920\n",
      "Epoch 1670, Loss: 0.2920\n",
      "Epoch 1680, Loss: 0.2919\n",
      "Epoch 1690, Loss: 0.2919\n",
      "Epoch 1700, Loss: 0.2919\n",
      "Epoch 1710, Loss: 0.2919\n",
      "Epoch 1720, Loss: 0.2919\n",
      "Epoch 1730, Loss: 0.2919\n",
      "Epoch 1740, Loss: 0.2919\n",
      "Epoch 1750, Loss: 0.2919\n",
      "Epoch 1760, Loss: 0.2918\n",
      "Epoch 1770, Loss: 0.2918\n",
      "Epoch 1780, Loss: 0.2918\n",
      "Epoch 1790, Loss: 0.2918\n",
      "Epoch 1800, Loss: 0.2918\n",
      "Epoch 1810, Loss: 0.2918\n",
      "Epoch 1820, Loss: 0.2918\n",
      "Epoch 1830, Loss: 0.2918\n",
      "Epoch 1840, Loss: 0.2918\n",
      "Epoch 1850, Loss: 0.2918\n",
      "Epoch 1860, Loss: 0.2917\n",
      "Epoch 1870, Loss: 0.2917\n",
      "Epoch 1880, Loss: 0.2917\n",
      "Epoch 1890, Loss: 0.2917\n",
      "Epoch 1900, Loss: 0.2917\n",
      "Epoch 1910, Loss: 0.2917\n",
      "Epoch 1920, Loss: 0.2917\n",
      "Epoch 1930, Loss: 0.2917\n",
      "Early stopping triggered at epoch 1937.\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LogisticRegression(in_features=X_train.shape[1], # Number of features\n",
    "                            lr=0.1)\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=5000,\n",
    "          early_stopping_patience=100,\n",
    "          tol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f80d98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8757\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = model.forward(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "accuracy = np.mean(y_pred_binary == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb463bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_binary, normalize='true')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=['Repub', 'Dem'], yticklabels=['Repub', 'Dem'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
