{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fdbf8b",
   "metadata": {},
   "source": [
    "# Backpropagation Algorithm\n",
    "\n",
    "### Recall computation graphs\n",
    "Recall that a computation graph is a directed acyclic graph (DAG) where:\n",
    "1.  Nodes in the graph represent variables (inputs, parameters, intermediate values, the final output $L$).\n",
    "3.  Edges represent functions/operations that produce an output node from input node(s). For an operation `y = f(u, v)`, there are incoming edges from nodes `u` and `u` to node `y`.\n",
    "4.  Each variable/node `u` in the graph will have two associated values:\n",
    "    *   `u.data`: The numerical value computed during the forward pass.\n",
    "    *   `u.grad`: Stores the computed partial derivative $\\frac{\\partial L}{\\partial u}$. Initialized appropriately before the backward pass.\n",
    "\n",
    "### The  Algorithm (Reverse-Mode Automatic Differentiation)\n",
    "\n",
    "**Goal:** Given a computation graph representing a scalar function `L` (e.g., the loss) that depends on some input parameters (e.g., `w1, w2, \\dots, wn`), efficiently compute the partial derivatives (gradients) of `L` with respect to each parameter.\n",
    "\n",
    "**Algorithm Steps:** (Note: below, `dy/dx` denotes the partial derivative of `y` with respect to `x`.)\n",
    "\n",
    "**1. Forward Pass:**\n",
    "\n",
    "*   Evaluate the graph from inputs to the final output `L`.\n",
    "*   For each node `u`, compute and store its value `u.data` based on the values of its predecessors and the operation performed.\n",
    "    *   Example: If `u = v + w`, then `u.data = v.data + w.data`.\n",
    "*   This pass computes the final value `L.data`. We need the intermediate `data` values stored at each node for the backward pass.\n",
    "\n",
    "**2. Backward Pass Initialization:**\n",
    "\n",
    "*   Initialize the `.grad` attribute for all nodes in the graph to zero.\n",
    "    ```python\n",
    "    # Conceptual initialization\n",
    "    for node u in graph:\n",
    "        u.grad = 0.0\n",
    "    ```\n",
    "*   Set the gradient of the final output node `L` with respect to itself to one.\n",
    "    ```python\n",
    "    L.grad = 1.0  # Represents dL/dL = 1\n",
    "    ```\n",
    "\n",
    "**3. Backward Pass Iteration:**\n",
    "\n",
    "*   Process the nodes in **reverse topological order** (starting from the final node `L` and moving backward towards the inputs/parameters). This ensures that when we process a node `u`, the gradient `u.grad` (representing `dL/du`) has already been fully computed by accumulating contributions from all paths downstream from `u`.\n",
    "*   For each node `u` being processed:\n",
    "    *   We have the accumulated gradient `dL/du` stored in `u.grad`.\n",
    "    *   Consider all nodes `v` that were **inputs** to the operation that produced `u`. Let the operation be `u = f(v_1, v_2, ..., v_k)`.\n",
    "    *   For each input node `v_i` to the operation `f`:\n",
    "        *   Compute the **local derivative** of the operation's output (`u`) with respect to this specific input (`v_i`), evaluated at the `.data` values computed during the forward pass. Let's call this `du/dv_i`.\n",
    "            *   Example: If `u = v_1 * v_2`, then `du/dv_i = v_2.data`.\n",
    "            *   Example: If `u = sin(v_1)`, then `du/dv_i = cos(v_1.data)`.\n",
    "        *   Apply the chain rule to find the contribution of `u.grad` to the gradient of `v_i` flowing *through* `u`:\n",
    "            ```python\n",
    "            contribution = u.grad * `du/dv_i = \n",
    "            ```\n",
    "        *   **Accumulate** this contribution into the gradient of the input node `v_i`:\n",
    "            ```python\n",
    "            v_i.grad += contribution\n",
    "            # This performs: dL/dv_i = (dL/dv_i)_old + (dL/du) * (du/dv_i)\n",
    "            ```\n",
    "            The `+=` operation is crucial. It ensures that if `v_i` is an input to multiple operations (i.e., it has multiple outgoing edges in the forward graph), its total gradient `dL/dv_i` correctly sums the contributions from all paths flowing back to it.\n",
    "\n",
    "**4. Final Result:**\n",
    "\n",
    "*   After processing all nodes in reverse topological order, the `.grad` attribute of each input parameter node `w_i` will contain the desired total partial derivative `dL/dw_i`.\n",
    "\n",
    "**Why it Works (Connection to Differentials):**\n",
    "\n",
    "This algorithm systematically computes the coefficients in the total differential `dL`. At each step, when computing `v_i.grad += u.grad * (du / dv_i)`, we are essentially substituting the expression for `du` into `dL`. The final `w_i.grad` values are the coefficients of `dw_i` in the full expression for $dL$ solely in terms of the input parameter differentials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119dbda",
   "metadata": {},
   "source": [
    "### Numerical Example: `L = log(x1*x2) + sin(x1*x3) + x2*x3`\n",
    "\n",
    "Let's compute the gradients `dL/dx1`, `dL/dx2`, and `dL/dx3` using backpropagation.\n",
    "\n",
    "**Inputs:** Let \n",
    "- `x1 = 2.0` \n",
    "- `x2 = 3.0` \n",
    "- `x3 = 4.0`.\n",
    "\n",
    "We assume `log` is the natural logarithm and `sin`/`cos` use radians.\n",
    "\n",
    "**1. Define Computation Graph Nodes & Operations:**\n",
    "\n",
    "*   `x1`, `x2`, `x3` (Inputs/Parameters)\n",
    "*   `a = x1 * x2`\n",
    "*   `b = log(a)`\n",
    "*   `c = x1 * x3`\n",
    "*   `d = sin(c)`\n",
    "*   `e = x2 * x3`\n",
    "*   `f = b + d`\n",
    "*   `L = f + e` (Final Output)\n",
    "\n",
    "**2. Forward Pass:** Calculate `.data` for each node.\n",
    "\n",
    "*   `x1.data = 2.0`\n",
    "*   `x2.data = 3.0`\n",
    "*   `x3.data = 4.0`\n",
    "*   `a.data = 2.0 * 3.0 = 6.0`\n",
    "*   `b.data = log(6.0) ≈ 1.7918`\n",
    "*   `c.data = 2.0 * 4.0 = 8.0`\n",
    "*   `d.data = sin(8.0) ≈ 0.9894`\n",
    "*   `e.data = 3.0 * 4.0 = 12.0`\n",
    "*   `f.data = 1.7918 + 0.9894 = 2.7812`\n",
    "*   `L.data = 2.7812 + 12.0 = 14.7812`\n",
    "\n",
    "**3. Backward Pass Initialization:**\n",
    "\n",
    "*   Initialize `.grad = 0.0` for nodes `x1, x2, x3, a, b, c, d, e, f`.\n",
    "*   Set `L.grad = 1.0`.\n",
    "\n",
    "**4. Backward Pass Iteration (Reverse Topological Order):**\n",
    "\n",
    "Processing order: `L, f, e, d, b, c, a, x3, x2, x1`\n",
    "\n",
    "*   **Node L:** (`L = f + e`)\n",
    "    *   Gradient `dL/dL` is `L.grad = 1.0`.\n",
    "    *   Inputs: `f`, `e`. \n",
    "    *   Local derivatives: `dL/df = 1`, `dL/de = 1`.\n",
    "    *   Propagate:\n",
    "        *   `f.grad += L.grad * (dL/df) = 0.0 + 1.0 * 1 = 1.0`\n",
    "        *   `e.grad += L.grad * (dL/de) = 0.0 + 1.0 * 1 = 1.0`\n",
    "    *   *Current Gradients*: \n",
    "        - `f.grad=1.0` \n",
    "        - `e.grad=1.0`\n",
    "\n",
    "*   **Node f:** (`f = b + d`)\n",
    "    *   Gradient `dL/df` is `f.grad = 1.0`.\n",
    "    *   Inputs: `b`, `d`. \n",
    "    *   Local derivatives: `df/db = 1`, `df/dd = 1`.\n",
    "    *   Propagate:\n",
    "        *   `b.grad += f.grad * (df/db) = 0.0 + 1.0 * 1 = 1.0`\n",
    "        *   `d.grad += f.grad * (df/dd) = 0.0 + 1.0 * 1 = 1.0`\n",
    "    *   *Current Gradients*: \n",
    "        - `b.grad=1.0` \n",
    "        - `d.grad=1.0`\n",
    "        - `e.grad=1.0`\n",
    "\n",
    "*   **Node e:** (`e = x2 * x3`)\n",
    "    *   Gradient `dL/de` is `e.grad = 1.0`.\n",
    "    *   Inputs: `x2`, `x3`. \n",
    "    *   Local derivatives: `de/dx2 = x3.data = 4.0`, `de/dx3 = x2.data = 3.0`.\n",
    "    *   Propagate:\n",
    "        *   `x2.grad += e.grad * (de/dx2) = 0.0 + 1.0 * 4.0 = 4.0`\n",
    "        *   `x3.grad += e.grad * (de/dx3) = 0.0 + 1.0 * 3.0 = 3.0`\n",
    "    *   *Current Gradients*: \n",
    "        - `b.grad=1.0` \n",
    "        - `d.grad=1.0` \n",
    "        - `x2.grad=4.0` \n",
    "        - `x3.grad=3.0`\n",
    "\n",
    "*   **Node d:** (`d = sin(c)`)\n",
    "    *   Gradient `dL/dd` is `d.grad = 1.0`.\n",
    "    *   Input: `c`. \n",
    "    *   Local derivative: `dd/dc = cos(c.data) = cos(8.0) ≈ -0.1455`.\n",
    "    *   Propagate:\n",
    "        *   `c.grad += d.grad * (dd/dc) = 0.0 + 1.0 * (-0.1455) = -0.1455`\n",
    "    *   *Current Gradients*: \n",
    "        - `b.grad=1.0` \n",
    "        - `c.grad=-0.1455` \n",
    "        - `x2.grad=4.0` \n",
    "        - `x3.grad=3.0`\n",
    "\n",
    "*   **Node b:** (`b = log(a)`)\n",
    "    *   Gradient `dL/db` is `b.grad = 1.0`.\n",
    "    *   Input: `a`. Local derivative: `db/da = 1 / a.data = 1 / 6.0 ≈ 0.1667`.\n",
    "    *   Propagate:\n",
    "        *   `a.grad += b.grad * (db/da) = 0.0 + 1.0 * (0.1667) = 0.1667`\n",
    "    *   *Current Gradients*: \n",
    "        - `a.grad=0.1667`\n",
    "        - `c.grad=-0.1455` \n",
    "        - `x2.grad=4.0` \n",
    "        - `x3.grad=3.0`\n",
    "\n",
    "*   **Node c:** (`c = x1 * x3`)\n",
    "    *   Gradient `dL/dc` is `c.grad ≈ -0.1455`.\n",
    "    *   Inputs: `x1`, `x3`. \n",
    "    *   Local derivatives: `dc/dx1 = x3.data = 4.0`, `dc/dx3 = x1.data = 2.0`.\n",
    "    *   Propagate:\n",
    "        *   `x1.grad += c.grad * (dc/dx1) = 0.0 + (-0.1455) * 4.0 ≈ -0.5820`\n",
    "        *   `x3.grad += c.grad * (dc/dx3) = 3.0 + (-0.1455) * 2.0 = 2.7090` (Accumulation!)\n",
    "    *   *Current Gradients*: \n",
    "        - `a.grad=0.1667` \n",
    "        - `x1.grad=-0.5820` \n",
    "        - `x2.grad=4.0` \n",
    "        - `x3.grad=2.7090`\n",
    "\n",
    "*   **Node a:** (`a = x1 * x2`)\n",
    "    *   Gradient `dL/da` is `a.grad ≈ 0.1667`.\n",
    "    *   Inputs: `x1`, `x2`. \n",
    "    *   Local derivatives: `da/dx1 = x2.data = 3.0`, `da/dx2 = x1.data = 2.0`.\n",
    "    *   Propagate:\n",
    "        *   `x1.grad += a.grad * (da/dx1) = -0.5820 + (0.1667) * 3.0 ≈ -0.0819` (Accumulation!)\n",
    "        *   `x2.grad += a.grad * (da/dx2) = 4.0 + (0.1667) * 2.0 ≈ 4.3334` (Accumulation!)\n",
    "    *   *Current Gradients*: \n",
    "        - `x1.grad=-0.0819` \n",
    "        - `x2.grad=4.3334` \n",
    "        - `x3.grad=2.7090`\n",
    "\n",
    "*   **Nodes x3, x2, x1:** These are input nodes. All incoming gradient paths have been processed.\n",
    "\n",
    "**5. Final Result:**\n",
    "\n",
    "The computed gradients are stored in the `.grad` attributes of the input nodes:\n",
    "\n",
    "*   `dL/dx1 = x1.grad ≈ -0.0819`\n",
    "*   `dL/dx2 = x2.grad ≈ 4.3334`\n",
    "*   `dL/dx3 = x3.grad ≈ 2.7090`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45752f83",
   "metadata": {},
   "source": [
    "### Implementing Backpropagation in Python\n",
    "\n",
    "There is a natural recursive structure to the backward pass that makes for a fun implementation problem. To illustrate the concept without getting bogged down in the details (especially, the complications introduced by using tensors), let's implmement Karpathy's `micrograd` library.\n",
    "\n",
    "#### High-Level Structure of micrograd\n",
    "\n",
    "The `micrograd` library is designed for automatic differentiation, specifically using the reverse-mode algorithm (backpropagation). Its core structure revolves around a single primary class:\n",
    "\n",
    "1.  **The `Value` Class:**\n",
    "    *   This is the central data structure in `micrograd`.\n",
    "    *   Each `Value` object represents a single **scalar node** within a computation graph. It could be an input variable, a parameter, an intermediate result, or the final output (like a loss function).\n",
    "    *   **Key Attributes:**\n",
    "        *   `data`: Stores the actual numerical floating-point value computed during the forward pass.\n",
    "        *   `grad`: Stores the gradient of the *final* scalar output of the computation graph with respect to this `Value` object's `data`. It's initialized to `0.0` and gets accumulated during the backward pass.\n",
    "        *   `_prev`: A set containing the parent `Value` objects (the inputs) that were used in the operation that created *this* `Value` object. This links nodes together to form the graph structure, pointing backward from children to parents.\n",
    "        *   `_op`: A string indicating the mathematical operation (e.g., `'+'`, `'*'`, `'tanh'`) that produced this `Value` object from its parents in `_prev`. Useful for visualization and debugging.\n",
    "        *   `_backward`: An internal *function* specific to this `Value` object. This function encapsulates the *local* chain rule logic for the operation (`_op`) that created this node. It knows how to take the gradient accumulated in *this* node (`self.grad`) and distribute it back to its parent nodes (those in `_prev`).\n",
    "        *   `label` (optional): A string name for the node, helpful for readability.\n",
    "\n",
    "2.  **Operations (Methods on `Value`):**\n",
    "    *   Mathematical operations like addition (`+`, `__add__`), multiplication (`*`, `__mul__`), exponentiation (`**`, `__pow__`), `tanh()`, `relu()`, `exp()`, `log()`, `sin()`, etc., are implemented as methods or by overloading standard Python operators for the `Value` class.\n",
    "    *   When an operation is performed (e.g., `c = a + b`), it:\n",
    "        *   Creates a *new* `Value` object (`c`).\n",
    "        *   Calculates `c.data`.\n",
    "        *   Sets `c._prev = {a, b}` and `c._op = '+'`.\n",
    "        *   Crucially, it defines and assigns the appropriate `_backward` function to `c`.\n",
    "\n",
    "3.  **Backpropagation Execution (`backward()` method):**\n",
    "    *   This is a *public* method called on the *final* `Value` object of the graph (typically the scalar loss, `L`).\n",
    "    *   Its role is to orchestrate the entire backward pass over the *whole graph* leading to that node.\n",
    "    *   **Steps:**\n",
    "        1.  **Topological Sort:** It first performs a topological sort of the computation graph, starting from the final node and traversing backward through the `_prev` links to identify all preceding nodes in an order that respects dependencies (parents before children when viewed backward).\n",
    "        2.  **Initialize Gradient:** It sets the gradient of the final node (`self.grad`) to `1.0` (since `dL/dL = 1`).\n",
    "        3.  **Iterate and Propagate:** It iterates through the topologically sorted nodes in *reverse* order. For each node in this sequence, it calls that node's internal `_backward()` function.\n",
    "\n",
    "4.  **Contrast: `backward()` vs. `_backward()`**\n",
    "    *   `backward()`: The **orchestrator**. Called *once* on the final node. Manages the *global* process of backpropagation across the *entire* graph (performs topological sort, initializes `L.grad=1`, iterates).\n",
    "    *   `_backward()`: The **local worker**. An *internal* function, *specific* to each node, defined by the operation that created it. It implements the *local* chain rule for *one specific operation*. It takes the gradient already computed for the *output* node (`self.grad`) and uses the local derivatives of the operation to calculate and *add* the gradient contributions to the `grad` attributes of the *input* nodes (`parent.grad += ...`). It is *called* repeatedly by the main `backward()` method during its iteration.\n",
    "\n",
    "**Remark: Implicit Backward Pass for Composed Operations**\n",
    "\n",
    "You might notice below that operations like division (`__truediv__`), subtraction (`__sub__`), or negation (`__neg__`) don't explicitly define their own `_backward` function within their implementation.\n",
    "\n",
    "This is because these operations are implemented by **composing** more primitive operations that *do* have `_backward` defined.\n",
    "\n",
    "For example, division `a / b` is implemented as `a * (b**-1)`:\n",
    "\n",
    "1.  `b**-1`: Creates an intermediate `Value` node using the `__pow__` operation. This node gets the `_backward` logic associated with exponentiation.\n",
    "2.  `a * (intermediate_node)`: Creates the final `Value` node using the `__mul__` operation. This node gets the `_backward` logic associated with multiplication.\n",
    "\n",
    "When `backward()` is called on the result of the division, the topological sort includes these intermediate nodes. The main `backward()` loop then calls the respective `_backward` functions for `__mul__` and `__pow__` on the relevant nodes in the correct order. The chain rule is thus applied correctly through these constituent steps, automatically handling the gradient propagation for the composite division operation without needing an explicit `_backward` function defined within `__truediv__` itself. This composition simplifies the implementation, as complex derivatives are built up from simpler ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Value:\n",
    "    \"\"\"\n",
    "    Stores a single scalar value and its gradient. Enables automatic differentiation\n",
    "    by tracking the operations that created it (building a computation graph)\n",
    "    and implementing the chain rule for gradient propagation during the backward pass.\n",
    "\n",
    "    Attributes:\n",
    "        data (float): The numerical value stored in this node.\n",
    "        grad (float): The gradient of the final scalar output (often the loss L)\n",
    "                      with respect to this node's value. Initialized to 0.\n",
    "        _backward (callable): A function that computes the gradient contribution\n",
    "                              of this node to its children (inputs). This is\n",
    "                              defined by the operation that created this node.\n",
    "        _prev (set): A set containing the 'Value' objects that were inputs to\n",
    "                     the operation that created this 'Value' object (its parents\n",
    "                     in the computation graph).\n",
    "        _op (str): A string representation of the operation that created this\n",
    "                   node (e.g., '+', '*', 'tanh'). Useful for debugging/visualization.\n",
    "        label (str): An optional label for the node, helpful for diagrams.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        \"\"\"\n",
    "        Initializes a Value object.\n",
    "\n",
    "        Args:\n",
    "            data (float or int): The numerical data for this value.\n",
    "            _children (tuple): Internal argument. Tuple of parent Value objects\n",
    "                               that produced this Value.\n",
    "            _op (str): Internal argument. The operation that produced this Value.\n",
    "            label (str): Optional descriptive label for this value.\n",
    "        \"\"\"\n",
    "        self.data = float(data)\n",
    "        self.grad = 0.0  # Initialize gradient to zero\n",
    "\n",
    "        # Internal variables used for building the computation graph & backprop\n",
    "        self._backward = lambda: None  # Default: leaf nodes have no backward function\n",
    "        self._prev = set(_children)    # Set of input Value objects (parents)\n",
    "        self._op = _op                 # Operation that produced this node\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Provides a readable representation of the Value object.\"\"\"\n",
    "        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Overloads the '+' operator for Value objects.\n",
    "\n",
    "        Handles addition with another Value object or a constant (int/float).\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other) # Wrap constants\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient of '+' is 1 for both inputs. Apply chain rule:\n",
    "            # dL/dself = dL/dout * dout/dself = dL/dout * 1\n",
    "            # dL/dother = dL/dout * dout/dother = dL/dout * 1\n",
    "            self.grad += out.grad * 1.0\n",
    "            other.grad += out.grad * 1.0\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        Overloads the '*' operator for Value objects.\n",
    "\n",
    "        Handles multiplication with another Value object or a constant (int/float).\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Value) else Value(other) # Wrap constants\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient of '*' uses the product rule (applied via chain rule):\n",
    "            # dL/dself = dL/dout * dout/dself = dL/dout * other.data\n",
    "            # dL/dother = dL/dout * dout/dother = dL/dout * self.data\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        \"\"\"\n",
    "        Overloads the '**' operator for Value objects (raising self to a power).\n",
    "\n",
    "        Args:\n",
    "            other (int or float): The exponent (must be a constant).\n",
    "        \"\"\"\n",
    "        assert isinstance(other, (int, float)), \"Exponent must be int/float for this simple version\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient rule: d(x^n)/dx = n * x^(n-1)\n",
    "            # dL/dself = dL/dout * dout/dself = dL/dout * (other * self.data**(other - 1))\n",
    "            self.grad += out.grad * (other * self.data**(other - 1))\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        \"\"\"\n",
    "        Overloads the '/' operator (true division). Implemented as self * (other**-1).\n",
    "        \"\"\"\n",
    "        return self * (other**-1)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Overloads the unary '-' operator (negation). Implemented as self * -1.\"\"\"\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \"\"\"Overloads the binary '-' operator (subtraction). Implemented as self + (-other).\"\"\"\n",
    "        return self + (-other)\n",
    "\n",
    "    # --- Reflected operators for handling constants on the left ---\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        \"\"\"Handles addition when a constant is on the left (e.g., 2 + Value(3)).\"\"\"\n",
    "        return self + other\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        \"\"\"Handles multiplication when a constant is on the left (e.g., 2 * Value(3)).\"\"\"\n",
    "        return self * other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        \"\"\"Handles subtraction when a constant is on the left (e.g., 2 - Value(3)).\"\"\"\n",
    "        return Value(other) + (-self) # Convert other to Value first\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        \"\"\"Handles division when a constant is on the left (e.g., 2 / Value(3)).\"\"\"\n",
    "        return Value(other) * (self**-1) # Convert other to Value first\n",
    "\n",
    "    # --- Activation and other mathematical functions ---\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"Applies the exponential function (e^x).\"\"\"\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient rule: d(e^x)/dx = e^x\n",
    "            # dL/dself = dL/dout * dout/dself = dL/dout * exp(self.data) = dL/dout * out.data\n",
    "            self.grad += out.grad * out.data\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "        \"\"\"Applies the natural logarithm (ln(x)).\"\"\"\n",
    "        x = self.data\n",
    "        if x <= 0:\n",
    "            # Avoid math domain error and gradient issues with log(0) or log(negative)\n",
    "            # In a real library, might add a small epsilon or handle differently.\n",
    "            raise ValueError(\"Logarithm undefined for non-positive values.\")\n",
    "        out = Value(math.log(x), (self,), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient rule: d(ln(x))/dx = 1/x\n",
    "            # dL/dself = dL/dout * dout/dself = dL/dout * (1 / self.data)\n",
    "            self.grad += out.grad * (1.0 / self.data)\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def sin(self):\n",
    "      \"\"\"Applies the sine function (sin(x), assumes radians).\"\"\"\n",
    "      x = self.data\n",
    "      out = Value(math.sin(x), (self,), 'sin')\n",
    "\n",
    "      def _backward():\n",
    "          # Gradient rule: d(sin(x))/dx = cos(x)\n",
    "          # dL/dself = dL/dout * dout/dself = dL/dout * cos(self.data)\n",
    "          self.grad += out.grad * math.cos(self.data)\n",
    "      out._backward = _backward\n",
    "\n",
    "      return out\n",
    "\n",
    "    def tanh(self):\n",
    "        \"\"\"Applies the hyperbolic tangent activation function.\"\"\"\n",
    "        x = self.data\n",
    "        t = math.tanh(x)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient rule: d(tanh(x))/dx = 1 - tanh(x)^2\n",
    "            # dL/dself = dL/dout * dout/dself = dL/dout * (1 - out.data**2)\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        \"\"\"Applies the Rectified Linear Unit (ReLU) activation function.\"\"\"\n",
    "        out = Value(max(0, self.data), (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            # Gradient rule: d(ReLU(x))/dx = 1 if x > 0, 0 otherwise\n",
    "            # dL/dself = dL/dout * dout/dself = dL/dout * (1 if self.data > 0 else 0)\n",
    "            # Note: Using out.data > 0 is also common and equivalent here.\n",
    "            self.grad += out.grad * (1.0 if self.data > 0 else 0.0)\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "      \"\"\"Applies the sigmoid activation function.\"\"\"\n",
    "      x = self.data\n",
    "      # Stable sigmoid implementation:\n",
    "      if x >= 0:\n",
    "          z = math.exp(-x)\n",
    "          s = 1 / (1 + z)\n",
    "      else:\n",
    "          z = math.exp(x)\n",
    "          s = z / (1 + z)\n",
    "      out = Value(s, (self,), 'sigmoid')\n",
    "\n",
    "      def _backward():\n",
    "          # Gradient rule: d(sigmoid(x))/dx = sigmoid(x) * (1 - sigmoid(x))\n",
    "          # dL/dself = dL/dout * dout/dself = dL/dout * (out.data * (1 - out.data))\n",
    "          self.grad += out.grad * (out.data * (1 - out.data))\n",
    "      out._backward = _backward\n",
    "\n",
    "      return out\n",
    "\n",
    "\n",
    "    # --- Backpropagation ---\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Performs the backward pass (reverse-mode automatic differentiation)\n",
    "        starting from this Value object. It computes the gradients of all\n",
    "        nodes in the computation graph that led to this node.\n",
    "\n",
    "        Assumes this node is the final scalar output (e.g., loss L).\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Build a topologically sorted list of nodes\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v) # Add node *after* its children are processed\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        # Step 2: Initialize the gradient of the final node (self) to 1.0\n",
    "        #         (dL/dL = 1). Gradients of all other nodes are already 0.\n",
    "        self.grad = 1.0\n",
    "\n",
    "        # Step 3: Iterate through the nodes in reverse topological order\n",
    "        #         and apply the chain rule using the _backward functions.\n",
    "        for node in reversed(topo):\n",
    "            node._backward() # This calls the specific backward function defined\n",
    "                             # by the operation that created 'node'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage (Matches the markdown example) ---\n",
    "\n",
    "print(\"--- Running Example from Markdown ---\")\n",
    "\n",
    "# Input values\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(3.0, label='x2')\n",
    "x3 = Value(4.0, label='x3')\n",
    "\n",
    "# Build the computation graph: L = log(x1*x2) + sin(x1*x3) + x2*x3\n",
    "# Let's break it down step-by-step like in the markdown\n",
    "\n",
    "# a = x1 * x2\n",
    "a = x1 * x2; a.label = 'a'\n",
    "# b = log(a)\n",
    "b = a.log(); b.label = 'b'\n",
    "\n",
    "# c = x1 * x3\n",
    "c = x1 * x3; c.label = 'c'\n",
    "# d = sin(c)\n",
    "d = c.sin(); d.label = 'd'\n",
    "\n",
    "# e = x2 * x3\n",
    "e = x2 * x3; e.label = 'e'\n",
    "\n",
    "# f = b + d\n",
    "f = b + d; f.label = 'f'\n",
    "# L = f + e\n",
    "L = f + e; L.label = 'L'\n",
    "\n",
    "# --- Forward Pass Verification ---\n",
    "print(f\"Forward Pass Result L.data: {L.data:.4f}\")\n",
    "# Expected: log(2*3) + sin(2*4) + 3*4 = log(6) + sin(8) + 12\n",
    "# Expected: ~1.7918 + 0.9894 + 12 = 14.7812\n",
    "print(\"---\")\n",
    "\n",
    "# --- Backward Pass ---\n",
    "print(\"Running Backward Pass (L.backward())...\")\n",
    "L.backward()\n",
    "print(\"---\")\n",
    "\n",
    "# --- Check Gradients ---\n",
    "print(\"Gradients after backpropagation:\")\n",
    "print(f\"{x1.label}: {x1}\") # Expected: ~ -0.0819\n",
    "print(f\"{x2.label}: {x2}\") # Expected: ~ 4.3334\n",
    "print(f\"{x3.label}: {x3}\") # Expected: ~ 2.7090\n",
    "print(\"---\")\n",
    "\n",
    "# --- You can also inspect intermediate gradients if needed ---\n",
    "print(\"Intermediate node gradients:\")\n",
    "print(f\"{a.label}: {a}\") # dL/da = dL/db * db/da = b.grad * (1/a.data) = 1.0 * (1/6) = 0.1667\n",
    "print(f\"{b.label}: {b}\") # dL/db = dL/df * df/db = f.grad * 1 = 1.0 * 1 = 1.0\n",
    "print(f\"{c.label}: {c}\") # dL/dc = dL/dd * dd/dc = d.grad * cos(c.data) = 1.0 * cos(8) ~ -0.1455\n",
    "print(f\"{d.label}: {d}\") # dL/dd = dL/df * df/dd = f.grad * 1 = 1.0 * 1 = 1.0\n",
    "print(f\"{e.label}: {e}\") # dL/de = dL/dL * dL/de = L.grad * 1 = 1.0 * 1 = 1.0\n",
    "print(f\"{f.label}: {f}\") # dL/df = dL/dL * dL/df = L.grad * 1 = 1.0 * 1 = 1.0\n",
    "print(f\"{L.label}: {L}\") # dL/dL = 1.0 (by definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing other operations ---\")\n",
    "v1 = Value(2.0, label='v1')\n",
    "v2 = Value(5.0, label='v2')\n",
    "\n",
    "# Test division: y = v1 / v2 = 2.0 / 5.0 = 0.4\n",
    "y_div = v1 / v2; y_div.label = 'y_div'\n",
    "y_div.backward()\n",
    "print(f\"Division: {y_div}\")\n",
    "# dy/dv1 = 1/v2 = 1/5 = 0.2\n",
    "# dy/dv2 = -v1 / v2^2 = -2 / 25 = -0.08\n",
    "print(f\" Gradient {v1.label}: {v1.grad:.4f} (Expected: 0.2)\")\n",
    "print(f\" Gradient {v2.label}: {v2.grad:.4f} (Expected: -0.08)\")\n",
    "v1.grad = 0; v2.grad = 0 # Reset grads for next test\n",
    "\n",
    "# Test exponentiation: y = v1**3 = 2.0**3 = 8.0\n",
    "y_pow = v1**3; y_pow.label = 'y_pow'\n",
    "y_pow.backward()\n",
    "print(f\"Power: {y_pow}\")\n",
    "# dy/dv1 = 3 * v1^2 = 3 * 2^2 = 12\n",
    "print(f\" Gradient {v1.label}: {v1.grad:.4f} (Expected: 12.0)\")\n",
    "v1.grad = 0 # Reset grad\n",
    "\n",
    "# Test ReLU: y = relu(-3.0) = 0\n",
    "v_neg = Value(-3.0, label='v_neg')\n",
    "y_relu1 = v_neg.relu(); y_relu1.label = 'y_relu1'\n",
    "y_relu1.backward()\n",
    "print(f\"ReLU (negative input): {y_relu1}\")\n",
    "print(f\" Gradient {v_neg.label}: {v_neg.grad:.4f} (Expected: 0.0)\")\n",
    "v_neg.grad = 0\n",
    "\n",
    "# Test ReLU: y = relu(4.0) = 4.0\n",
    "v_pos = Value(4.0, label='v_pos')\n",
    "y_relu2 = v_pos.relu(); y_relu2.label = 'y_relu2'\n",
    "y_relu2.backward()\n",
    "print(f\"ReLU (positive input): {y_relu2}\")\n",
    "print(f\" Gradient {v_pos.label}: {v_pos.grad:.4f} (Expected: 1.0)\")\n",
    "v_pos.grad = 0\n",
    "\n",
    "# Test Sigmoid: y = sigmoid(0.0) = 0.5\n",
    "v_zero = Value(0.0, label='v_zero')\n",
    "y_sig = v_zero.sigmoid(); y_sig.label = 'y_sig'\n",
    "y_sig.backward()\n",
    "print(f\"Sigmoid (zero input): {y_sig}\")\n",
    "# dy/dv_zero = sig(0)*(1-sig(0)) = 0.5*(1-0.5) = 0.25\n",
    "print(f\" Gradient {v_zero.label}: {v_zero.grad:.4f} (Expected: 0.25)\")\n",
    "v_zero.grad = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a7701",
   "metadata": {},
   "source": [
    "### Visualization with Graphviz\n",
    "Following Karpathy's example, we can visualize the computation graph using Graphviz. The `graphviz` library allows us to create directed graphs in a simple and intuitive way.\n",
    "\n",
    "To install it, try `conda install graphviz` or `pip install graphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87764ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz # Make sure this is installed\n",
    "\n",
    "def trace(root):\n",
    "    \"\"\"\n",
    "    Builds a set of all nodes and edges in a computation graph starting\n",
    "    from a root Value object.\n",
    "\n",
    "    Args:\n",
    "        root (Value): The final node of the graph to trace back from.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two sets:\n",
    "               - nodes (set): All Value objects in the graph.\n",
    "               - edges (set): Tuples representing connections (parent_value, child_value).\n",
    "    \"\"\"\n",
    "    nodes, edges = set(), set()\n",
    "    visited = set()\n",
    "    def build(v):\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            nodes.add(v)\n",
    "            for parent in v._prev:\n",
    "                edges.add((parent, v)) # Edge points from parent to child\n",
    "                build(parent)          # Recurse on parents\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    Generates a Graphviz visualization of the computation graph ending at 'root'.\n",
    "\n",
    "    Args:\n",
    "        root (Value): The root node of the graph (e.g., the final loss L).\n",
    "        format (str): The output format for Graphviz ('svg', 'png', 'pdf', etc.).\n",
    "        rankdir (str): The direction of the graph layout ('LR' for left-to-right,\n",
    "                       'TB' for top-to-bottom).\n",
    "\n",
    "    Returns:\n",
    "        graphviz.Digraph: The Graphviz object representing the graph.\n",
    "                          You can render this in Jupyter or save it to a file.\n",
    "\n",
    "    Example Usage (in Jupyter):\n",
    "        # Assume L is the final Value object from your calculation\n",
    "        dot_graph = draw_dot(L)\n",
    "        dot_graph # This will display the graph in the notebook output cell\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root) # Get all unique nodes and parent->child connections\n",
    "    dot = graphviz.Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'shape': 'record'})\n",
    "\n",
    "    for n in nodes:\n",
    "        # Use object's id as a unique identifier for the node in Graphviz\n",
    "        uid = str(id(n))\n",
    "        # Create a node for the Value object itself\n",
    "        # Use 'record' shape for multi-line labels (label | data | grad)\n",
    "        node_label = f\"{{ {n.label+' | ' if n.label else ''}data {n.data:.4f} | grad {n.grad:.4f} }}\"\n",
    "        dot.node(name=uid, label=node_label, shape='record')\n",
    "\n",
    "        if n._op: # If this Value node was created by an operation\n",
    "            # Create a small, distinct node representing the operation\n",
    "            op_uid = uid + n._op # Unique ID for the operation node\n",
    "            dot.node(name=op_uid, label=n._op, shape='ellipse') # Use ellipse or circle for ops\n",
    "            # Add an edge from the operation node to the Value node it created\n",
    "            dot.edge(op_uid, uid)\n",
    "            # Add edges from the parent Value nodes to this operation node\n",
    "            for parent in n._prev:\n",
    "                parent_uid = str(id(parent))\n",
    "                dot.edge(parent_uid, op_uid)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage with the previous calculation ---\n",
    "# Make sure the Value class definition and the example calculation\n",
    "# from the previous steps have been executed first.\n",
    "\n",
    "# Assuming 'L' is the final node from: L = log(x1*x2) + sin(x1*x3) + x2*x3\n",
    "print(\"\\n--- Generating Computation Graph Visualization ---\")\n",
    "# Calculate L again if necessary\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(3.0, label='x2')\n",
    "x3 = Value(4.0, label='x3')\n",
    "a = x1 * x2; a.label = 'a'\n",
    "b = a.log(); b.label = 'b'\n",
    "c = x1 * x3; c.label = 'c'\n",
    "d = c.sin(); d.label = 'd'\n",
    "e = x2 * x3; e.label = 'e'\n",
    "f = b + d; f.label = 'f'\n",
    "L = f + e; L.label = 'L'\n",
    "L.backward() # Run backward pass to populate gradients for display\n",
    "\n",
    "# Generate the graph object\n",
    "dot_graph = draw_dot(L)\n",
    "\n",
    "# To save to a file (e.g., SVG):\n",
    "# try:\n",
    "#     dot_graph.render('computation_graph', view=False) # Saves computation_graph.gv and computation_graph.gv.svg\n",
    "#     print(\"Graph saved to computation_graph.gv.svg\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not save graph. Ensure Graphviz executable is installed and in PATH. Error: {e}\")\n",
    "\n",
    "dot_graph # Display the graph in the notebook output cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
